from pymongo import MongoClient, UpdateOne
from pymongo.errors import BulkWriteError, ServerSelectionTimeoutError, ConnectionFailure, NetworkTimeout
import os
from dotenv import load_dotenv
import pandas as pd
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any
import pymongo
from pymongo import timeout
import logging
import time
from functools import wraps

load_dotenv()

# æ•°æ®åº“é…ç½®
CLOUD_MONGO_URI = "mongodb://root:example@cd-1.frp.one:48714/quant_analysis?authSource=admin"
LOCAL_MONGO_URI = "mongodb://root:example@127.0.0.1:27017/quant_analysis?authSource=admin"
DB_NAME = os.getenv("DB_NAME", "quant_analysis")

def retry_on_connection_error(max_retries=3, delay=2):
    """é‡è¯•è£…é¥°å™¨ï¼Œç”¨äºå¤„ç†æ•°æ®åº“è¿æ¥é”™è¯¯"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except (ConnectionFailure, NetworkTimeout, ServerSelectionTimeoutError, Exception) as e:
                    if attempt == max_retries - 1:
                        raise e
                    logging.warning(f"è¿æ¥å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    time.sleep(delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
            return None
        return wrapper
    return decorator

class DBHandler:
    """åŒæ•°æ®åº“å¤„ç†å™¨ï¼Œæ”¯æŒæœ¬åœ°+äº‘ç«¯åŒæ—¶å†™å…¥"""
    
    def __init__(self, local_priority=True):
        """
        åˆå§‹åŒ–åŒæ•°æ®åº“è¿æ¥
        
        Args:
            local_priority: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ•°æ®åº“ï¼ˆé»˜è®¤Trueï¼‰
        """
        self.local_priority = local_priority
        self.logger = self._setup_logger()
        
        # åˆå§‹åŒ–è¿æ¥çŠ¶æ€
        self.cloud_client = None
        self.local_client = None
        self.cloud_db = None
        self.local_db = None
        self.cloud_available = False
        self.local_available = False
        
        # å»ºç«‹æ•°æ®åº“è¿æ¥
        self._connect_databases()
        
        # å‘åå…¼å®¹ - ä¿ç•™åŸæœ‰çš„clientå’Œdbå±æ€§ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ•°æ®åº“
        self.client = self.local_client if self.local_available else self.cloud_client
        self.db = self.local_db if self.local_available else self.cloud_db
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger('DBHandler')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    def _connect_databases(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥ - ä»…è¿æ¥æœ¬åœ°æ•°æ®åº“ï¼Œä¸è¿æ¥äº‘ç«¯æ•°æ®åº“"""
        # è¿æ¥æœ¬åœ°æ•°æ®åº“ï¼ˆä¼˜å…ˆï¼‰
        try:
            self.logger.info("ğŸ  è¿æ¥æœ¬åœ°æ•°æ®åº“...")
            self.local_client = MongoClient(
                LOCAL_MONGO_URI,
                serverSelectionTimeoutMS=30000,  # ä¿æŒ30ç§’
                connectTimeoutMS=30000,           # ä¿æŒ30ç§’
                socketTimeoutMS=120000,           # å¢åŠ åˆ°2åˆ†é’Ÿ
                maxPoolSize=50,                   # å¢åŠ è¿æ¥æ± å¤§å°
                minPoolSize=10,                   # è®¾ç½®æœ€å°è¿æ¥æ± 
                maxIdleTimeMS=60000,              # å¢åŠ ç©ºé—²è¶…æ—¶
                retryWrites=True,
                w=1,
                heartbeatFrequencyMS=30000        # å‡å°‘å¿ƒè·³é¢‘ç‡
            )
            # æµ‹è¯•è¿æ¥
            self.local_client.admin.command('ismaster')
            self.local_db = self.local_client[DB_NAME]
            self.local_available = True
            
            local_info = self.local_client.server_info()
            print("âœ… æœ¬åœ°MongoDBè¿æ¥æˆåŠŸ")
            print(f"ğŸ“ æœ¬åœ°åœ°å€: 127.0.0.1:27017")
            print(f"ğŸ”§ æœ¬åœ°ç‰ˆæœ¬: {local_info['version']}")
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿æœ¬åœ°MongoDBå®¹å™¨æ­£åœ¨è¿è¡Œ:")
            print("   cd database && docker-compose -f docker-compose.single.yml up -d")
            self.local_available = False
        
        # è·³è¿‡äº‘ç«¯æ•°æ®åº“è¿æ¥ï¼ˆä»…ç”¨äºAPIæ¥å£åˆ†æï¼‰
        print("ğŸ’¡ APIæ¥å£åˆ†ææ¨¡å¼ï¼šè·³è¿‡äº‘ç«¯æ•°æ®åº“è¿æ¥ï¼Œä»…ä½¿ç”¨æœ¬åœ°æ•°æ®åº“")
        self.cloud_available = False
        self.cloud_client = None
        self.cloud_db = None
        
        # è¿æ¥çŠ¶æ€æ€»ç»“
        if self.local_available:
            print("ğŸ¯ æœ¬åœ°æ•°æ®åº“è¿æ¥æˆåŠŸï¼Œæ•°æ®å°†ä»…å†™å…¥æœ¬åœ°æ•°æ®åº“")
            print("ğŸ’¡ äº‘ç«¯æ•°æ®åº“è¿æ¥å·²è·³è¿‡ï¼Œå¦‚éœ€å¤‡ä»½è¯·ç¨åæ‰‹åŠ¨åŒæ­¥")
        else:
            print("âŒ æœ¬åœ°æ•°æ®åº“è¿æ¥å¤±è´¥")
            raise Exception("æ— æ³•è¿æ¥åˆ°æœ¬åœ°æ•°æ®åº“ï¼Œè¯·æ£€æŸ¥æœ¬åœ°MongoDBæœåŠ¡")

    def get_collection(self, collection_name):
        """è·å–é›†åˆï¼Œä¼˜å…ˆè¿”å›æœ¬åœ°æ•°æ®åº“é›†åˆ"""
        if self.local_available:
            return self.local_db[collection_name]
        elif self.cloud_available:
            return self.cloud_db[collection_name]
        else:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„æ•°æ®åº“è¿æ¥")
    
    def get_cloud_collection(self, collection_name):
        """è·å–äº‘ç«¯æ•°æ®åº“é›†åˆ"""
        if self.cloud_available:
            return self.cloud_db[collection_name]
        return None
    
    def get_local_collection(self, collection_name):
        """è·å–æœ¬åœ°æ•°æ®åº“é›†åˆ"""
        if self.local_available:
            return self.local_db[collection_name]
        return None

    def find_data(self, collection_name: str, query: Dict, sort: List[tuple] = None, limit: int = None) -> List[Dict]:
        """æŸ¥è¯¢æ•°æ®åº“æ•°æ®
        
        Args:
            collection_name: é›†åˆåç§°
            query: æŸ¥è¯¢æ¡ä»¶
            sort: æ’åºæ¡ä»¶ï¼Œæ ¼å¼ä¸º[('field', 1/-1)]ï¼Œ1ä¸ºå‡åºï¼Œ-1ä¸ºé™åº
            limit: é™åˆ¶è¿”å›æ•°é‡
            
        Returns:
            List[Dict]: æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        try:
            collection = self.get_collection(collection_name)
            cursor = collection.find(query)
            
            # åº”ç”¨æ’åº
            if sort:
                cursor = cursor.sort(sort)
            
            # åº”ç”¨é™åˆ¶
            if limit:
                cursor = cursor.limit(limit)
                
            return list(cursor)
        except Exception as e:
            logging.error(f"æŸ¥è¯¢æ•°æ®å¤±è´¥: {e}")
            return []

    def _check_cloud_connection(self):
        """å¿«é€Ÿæ£€æŸ¥äº‘ç«¯è¿æ¥çŠ¶æ€ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾… - APIæ¥å£åˆ†ææ¨¡å¼ä¸‹ç¦ç”¨"""
        return False
    
    def _reconnect_cloud(self):
        """é‡æ–°è¿æ¥äº‘ç«¯æ•°æ®åº“ - APIæ¥å£åˆ†ææ¨¡å¼ä¸‹ç¦ç”¨"""
        print("ğŸ’¡ APIæ¥å£åˆ†ææ¨¡å¼ï¼šè·³è¿‡äº‘ç«¯æ•°æ®åº“é‡è¿")
        self.cloud_available = False
        return False

    def _write_to_cloud_with_retry(self, collection_name, updates, batch_size):
        """å†™å…¥äº‘ç«¯æ•°æ®åº“çš„å†…éƒ¨æ–¹æ³•ï¼Œå¸¦æ™ºèƒ½é‡è¯•æœºåˆ¶ - APIæ¥å£åˆ†ææ¨¡å¼ä¸‹ç¦ç”¨"""
        print("ğŸ’¡ APIæ¥å£åˆ†ææ¨¡å¼ï¼šè·³è¿‡äº‘ç«¯æ•°æ®åº“å†™å…¥")
        return False, 0, 0

    @retry_on_connection_error(max_retries=3, delay=2)
    def _write_to_cloud(self, collection_name, updates, batch_size):
        """å†™å…¥äº‘ç«¯æ•°æ®åº“çš„å†…éƒ¨æ–¹æ³•ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰"""
        return self._write_to_cloud_with_retry(collection_name, updates, batch_size)
    
    def _write_to_local(self, collection_name, updates, batch_size):
        """å†™å…¥æœ¬åœ°æ•°æ®åº“çš„å†…éƒ¨æ–¹æ³•"""
        if not self.local_available:
            return False, 0, 0
        
        try:
            print(f"ğŸ  å†™å…¥æœ¬åœ°æ•°æ®åº“: {collection_name}")
            local_collection = self.local_db[collection_name]
            
            total_upserted = 0
            total_modified = 0
            batch_count = 0
            total_batches = (len(updates) + batch_size - 1) // batch_size
            
            for i in range(0, len(updates), batch_size):
                batch = updates[i:i + batch_size]
                batch_count += 1
                
                try:
                    result = local_collection.bulk_write(batch, ordered=False)
                    total_upserted += result.upserted_count
                    total_modified += result.modified_count
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if batch_count % 10 == 0 or batch_count == total_batches:
                        print(f"   ğŸ“ è¿›åº¦: {batch_count}/{total_batches} æ‰¹æ¬¡")
                        
                except BulkWriteError as bwe:
                    total_upserted += bwe.details.get('nUpserted', 0)
                    total_modified += bwe.details.get('nModified', 0)
                    error_count = len(bwe.details.get('writeErrors', []))
                    if error_count > 0:
                        logging.warning(f"æ‰¹æ¬¡ {batch_count} éƒ¨åˆ†å¤±è´¥: {error_count} ä¸ªé”™è¯¯")
            
            print(f"âœ… æœ¬åœ°å†™å…¥æˆåŠŸ: æ–°å¢{total_upserted} æ›´æ–°{total_modified}")
            return True, total_upserted, total_modified
            
        except Exception as e:
            logging.error(f"æœ¬åœ°å†™å…¥å¤±è´¥: {e}")
            return False, 0, 0
    
    def _get_optimal_batch_size(self, data_size, collection_name):
        """æ ¹æ®æ•°æ®é‡å’Œé›†åˆç±»å‹åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°"""
        # åŸºç¡€æ‰¹æ¬¡å¤§å°
        if data_size > 100000:  # è¶…è¿‡10ä¸‡æ¡è®°å½•
            base_size = 200
        elif data_size > 10000:  # è¶…è¿‡1ä¸‡æ¡è®°å½•
            base_size = 500
        else:
            base_size = 1000
        
        # æ ¹æ®é›†åˆç±»å‹è°ƒæ•´
        if 'daily' in collection_name.lower():
            return min(base_size, 800)  # æ—¥çº¿æ•°æ®é€šå¸¸è¾ƒå¤§
        elif 'basic' in collection_name.lower():
            return min(base_size, 1500)  # åŸºç¡€ä¿¡æ¯æ•°æ®è¾ƒå°
        else:
            return base_size

    def bulk_upsert(self, collection_name, data, unique_keys):
        """æ‰¹é‡æ›´æ–°æ’å…¥æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“ï¼ˆäº‘ç«¯æ•°æ®åº“ä½œä¸ºå¤‡ä»½ï¼‰"""
        if not data:
            print(f"æ²¡æœ‰æ•°æ®éœ€è¦æ’å…¥æˆ–æ›´æ–°åˆ°é›†åˆ {collection_name}")
            return

        # å‡†å¤‡æ‰¹é‡æ“ä½œ
        updates = []
        for item in data:
            filter_query = {key: item[key] for key in unique_keys if key in item}
            updates.append(UpdateOne(filter_query, {'$set': item}, upsert=True))

        # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        batch_size = self._get_optimal_batch_size(len(data), collection_name)
        print(f"ğŸ“Š æ•°æ®é‡: {len(data):,} æ¡ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        local_success = False

        # åªå†™å…¥æœ¬åœ°æ•°æ®åº“
        if self.local_available:
            try:
                local_success, local_upserted, local_modified = self._write_to_local(collection_name, updates, batch_size)
            except Exception as e:
                logging.error(f"âŒ æœ¬åœ°æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")
                local_success = False
        else:
            # å¦‚æœæœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨ï¼ŒæŠ›å‡ºå¼‚å¸¸
            logging.error("âŒ æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ•°æ®é‡‡é›†")
            raise Exception("æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æœ¬åœ°MongoDBæœåŠ¡")

        # æ€»ç»“
        if local_success:
            print(f"ğŸ¯ æœ¬åœ°æ•°æ®åº“å†™å…¥å®Œæˆ: {collection_name}")
            if self.cloud_available:
                print("ğŸ’¡ äº‘ç«¯æ•°æ®åº“å¯ç”¨ï¼Œå¯é€šè¿‡æ‰‹åŠ¨åŒæ­¥è¿›è¡Œå¤‡ä»½")
            else:
                print("âš ï¸  äº‘ç«¯æ•°æ®åº“ä¸å¯ç”¨ï¼Œè¯·ç¨åæ‰‹åŠ¨åŒæ­¥å¤‡ä»½")
        else:
            logging.error(f"âŒ æœ¬åœ°æ•°æ®åº“å†™å…¥å¤±è´¥: {collection_name}")
            raise Exception("æœ¬åœ°æ•°æ®åº“å†™å…¥å¤±è´¥")

    def manual_sync_to_cloud(self, collection_names=None, start_date=None, end_date=None):
        """
        æ‰‹åŠ¨åŒæ­¥æœ¬åœ°æ•°æ®åº“åˆ°äº‘ç«¯æ•°æ®åº“
        
        Args:
            collection_names: è¦åŒæ­¥çš„é›†åˆåç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åŒæ­¥æ‰€æœ‰é›†åˆ
            start_date: èµ·å§‹æ—¥æœŸï¼Œæ ¼å¼ä¸º'YYYYMMDD'ï¼Œç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„å¢é‡åŒæ­¥
            end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼ä¸º'YYYYMMDD'ï¼Œç”¨äºæ—¶é—´åºåˆ—æ•°æ®çš„å¢é‡åŒæ­¥
        
        Returns:
            bool: åŒæ­¥æ˜¯å¦æˆåŠŸ
        """
        if not self.local_available:
            print("âŒ æœ¬åœ°æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒåŒæ­¥")
            return False
        
        if not self.cloud_available:
            print("âŒ äº‘ç«¯æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒåŒæ­¥")
            return False
        
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šé›†åˆï¼Œè·å–æ‰€æœ‰é›†åˆ
            if collection_names is None:
                collection_names = self.local_db.list_collection_names()
                print(f"ğŸ“‹ å‘ç° {len(collection_names)} ä¸ªé›†åˆéœ€è¦åŒæ­¥")
            
            sync_success_count = 0
            sync_total_count = len(collection_names)
            
            for collection_name in collection_names:
                print(f"\nğŸ”„ å¼€å§‹åŒæ­¥é›†åˆ: {collection_name}")
                
                try:
                    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
                    query = {}
                    if start_date or end_date:
                        date_filter = {}
                        if start_date:
                            date_filter['$gte'] = start_date
                        if end_date:
                            date_filter['$lte'] = end_date
                        
                        # å°è¯•ä¸åŒçš„æ—¥æœŸå­—æ®µå
                        date_fields = ['trade_date', 'cal_date', 'ann_date', 'pub_date']
                        for date_field in date_fields:
                            # æ£€æŸ¥é›†åˆä¸­æ˜¯å¦å­˜åœ¨è¯¥æ—¥æœŸå­—æ®µ
                            sample = self.local_db[collection_name].find_one({date_field: {'$exists': True}})
                            if sample:
                                query[date_field] = date_filter
                                print(f"   ğŸ“… ä½¿ç”¨æ—¥æœŸå­—æ®µ: {date_field}")
                                break
                    
                    # ä»æœ¬åœ°æ•°æ®åº“è¯»å–æ•°æ®
                    local_collection = self.local_db[collection_name]
                    cloud_collection = self.cloud_db[collection_name]
                    
                    # è·å–æ•°æ®æ€»æ•°
                    total_docs = local_collection.count_documents(query)
                    if total_docs == 0:
                        print(f"   âš ï¸  é›†åˆ {collection_name} æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
                        continue
                    
                    print(f"   ğŸ“Š æ‰¾åˆ° {total_docs:,} æ¡æ•°æ®éœ€è¦åŒæ­¥")
                    
                    # æ‰¹é‡åŒæ­¥æ•°æ®
                    batch_size = 1000
                    synced_count = 0
                    
                    cursor = local_collection.find(query)
                    batch = []
                    
                    for doc in cursor:
                        # ç§»é™¤MongoDBçš„_idå­—æ®µï¼Œè®©äº‘ç«¯æ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆ
                        if '_id' in doc:
                            del doc['_id']
                        batch.append(doc)
                        
                        if len(batch) >= batch_size:
                            # æ‰§è¡Œæ‰¹é‡æ’å…¥/æ›´æ–°
                            self._sync_batch_to_cloud(cloud_collection, batch, collection_name)
                            synced_count += len(batch)
                            batch = []
                            
                            # æ˜¾ç¤ºè¿›åº¦
                            progress = (synced_count / total_docs) * 100
                            print(f"   ğŸ“ è¿›åº¦: {synced_count:,}/{total_docs:,} ({progress:.1f}%)")
                    
                    # å¤„ç†å‰©ä½™çš„æ•°æ®
                    if batch:
                        self._sync_batch_to_cloud(cloud_collection, batch, collection_name)
                        synced_count += len(batch)
                    
                    print(f"   âœ… é›†åˆ {collection_name} åŒæ­¥å®Œæˆ: {synced_count:,} æ¡æ•°æ®")
                    sync_success_count += 1
                    
                except Exception as e:
                    print(f"   âŒ é›†åˆ {collection_name} åŒæ­¥å¤±è´¥: {e}")
                    logging.error(f"åŒæ­¥é›†åˆ {collection_name} å¤±è´¥: {e}")
            
            # åŒæ­¥æ€»ç»“
            print(f"\nğŸ“Š åŒæ­¥å®Œæˆ: {sync_success_count}/{sync_total_count} ä¸ªé›†åˆåŒæ­¥æˆåŠŸ")
            return sync_success_count == sync_total_count
            
        except Exception as e:
            print(f"âŒ æ‰‹åŠ¨åŒæ­¥è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            logging.error(f"æ‰‹åŠ¨åŒæ­¥å¤±è´¥: {e}")
            return False
    
    def _sync_batch_to_cloud(self, cloud_collection, batch, collection_name):
        """
        å°†æ‰¹é‡æ•°æ®åŒæ­¥åˆ°äº‘ç«¯æ•°æ®åº“
        
        Args:
            cloud_collection: äº‘ç«¯é›†åˆå¯¹è±¡
            batch: è¦åŒæ­¥çš„æ•°æ®æ‰¹æ¬¡
            collection_name: é›†åˆåç§°
        """
        try:
            # ä½¿ç”¨insert_manyè¿›è¡Œæ‰¹é‡æ’å…¥ï¼Œå¿½ç•¥é‡å¤æ•°æ®
            cloud_collection.insert_many(batch, ordered=False)
        except BulkWriteError as bwe:
            # å¤„ç†éƒ¨åˆ†æ’å…¥æˆåŠŸçš„æƒ…å†µ
            inserted_count = bwe.details.get('nInserted', 0)
            error_count = len(bwe.details.get('writeErrors', []))
            if error_count > 0:
                # å¤§éƒ¨åˆ†é”™è¯¯å¯èƒ½æ˜¯é‡å¤æ•°æ®ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                logging.debug(f"æ‰¹æ¬¡åŒæ­¥éƒ¨åˆ†æˆåŠŸ: æ’å…¥{inserted_count}æ¡ï¼Œè·³è¿‡{error_count}æ¡é‡å¤æ•°æ®")
        except Exception as e:
            logging.error(f"æ‰¹æ¬¡åŒæ­¥åˆ°äº‘ç«¯å¤±è´¥: {e}")
            raise e
    
    def get_sync_status(self):
        """
        è·å–æœ¬åœ°å’Œäº‘ç«¯æ•°æ®åº“çš„åŒæ­¥çŠ¶æ€
        
        Returns:
            dict: åŒ…å«å„é›†åˆæ•°æ®é‡å¯¹æ¯”çš„å­—å…¸
        """
        if not self.local_available or not self.cloud_available:
            print("âŒ æœ¬åœ°æˆ–äº‘ç«¯æ•°æ®åº“ä¸å¯ç”¨ï¼Œæ— æ³•è·å–åŒæ­¥çŠ¶æ€")
            return {}
        
        try:
            local_collections = self.local_db.list_collection_names()
            cloud_collections = self.cloud_db.list_collection_names()
            
            status = {}
            
            print("ğŸ“Š æ•°æ®åº“åŒæ­¥çŠ¶æ€å¯¹æ¯”:")
            print(f"{'é›†åˆåç§°':<30} {'æœ¬åœ°æ•°æ®é‡':<15} {'äº‘ç«¯æ•°æ®é‡':<15} {'çŠ¶æ€':<10}")
            print("-" * 70)
            
            for collection_name in sorted(set(local_collections + cloud_collections)):
                local_count = 0
                cloud_count = 0
                
                if collection_name in local_collections:
                    local_count = self.local_db[collection_name].count_documents({})
                
                if collection_name in cloud_collections:
                    cloud_count = self.cloud_db[collection_name].count_documents({})
                
                # åˆ¤æ–­åŒæ­¥çŠ¶æ€
                if local_count == cloud_count:
                    sync_status = "âœ… åŒæ­¥"
                elif local_count > cloud_count:
                    sync_status = "âš ï¸  å¾…åŒæ­¥"
                else:
                    sync_status = "â“ å¼‚å¸¸"
                
                status[collection_name] = {
                    'local_count': local_count,
                    'cloud_count': cloud_count,
                    'status': sync_status
                }
                
                print(f"{collection_name:<30} {local_count:<15,} {cloud_count:<15,} {sync_status:<10}")
            
            return status
            
        except Exception as e:
            print(f"âŒ è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
            logging.error(f"è·å–åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
            return {}
    
    def __del__(self):
        """å®‰å…¨å…³é—­MongoDBè¿æ¥"""
        try:
            if hasattr(self, 'cloud_client') and self.cloud_client is not None:
                self.cloud_client.close()
            if hasattr(self, 'local_client') and self.local_client is not None:
                self.local_client.close()
        except (ImportError, AttributeError, TypeError):
            # Pythonå…³é—­æ—¶æ¨¡å—å¯èƒ½å·²è¢«æ¸…ç†ï¼Œå¿½ç•¥è¿™äº›é”™è¯¯
            pass

    def get_kline_data(self, stock_code: str, days: int) -> Optional[pd.DataFrame]:
        """
        è·å–æŒ‡å®šè‚¡ç¥¨çš„Kçº¿æ•°æ®
        
        Args:
            stock_code (str): è‚¡ç¥¨ä»£ç 
            days (int): è·å–æœ€è¿‘çš„å¤©æ•°

        Returns:
            Optional[pd.DataFrame]: åŒ…å«Kçº¿æ•°æ®çš„DataFrameï¼Œå¦‚æœæ— æ•°æ®åˆ™è¿”å›None
        """
        try:
            collection = self.db.get_collection('stock_kline_daily')
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days)
            
            query = {
                'ts_code': stock_code,
                'trade_date': {
                    '$gte': start_date,
                    '$lte': end_date
                }
            }
            
            cursor = collection.find(query).sort('trade_date', pymongo.ASCENDING)
            data = list(cursor)
            
            if not data:
                return None
            
            df = pd.DataFrame(data)
            # ç¡®ä¿å…³é”®åˆ—å­˜åœ¨
            required_cols = ['trade_date', 'open', 'high', 'low', 'close', 'vol']
            if not all(col in df.columns for col in required_cols):
                print("âŒ Kçº¿æ•°æ®ç¼ºå°‘å…³é”®åˆ—")
                return None

            # ç±»å‹è½¬æ¢
            for col in ['open', 'high', 'low', 'close', 'vol']:
                df[col] = pd.to_numeric(df[col])

            return df

        except Exception as e:
            print(f"âŒ ä»æ•°æ®åº“è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            return None

    def get_latest_date_for_code(self, collection_name: str, ts_code: str) -> Optional[datetime]:
        """è·å–æŒ‡å®šä»£ç åœ¨æŒ‡å®šé›†åˆä¸­çš„æœ€æ–°äº¤æ˜“æ—¥æœŸ"""
        try:
            collection = self.db.get_collection(collection_name)
            latest_record = collection.find_one(
                {'ts_code': ts_code},
                sort=[('trade_date', -1)]
            )
            if latest_record and 'trade_date' in latest_record:
                # ç¡®ä¿è¿”å›çš„æ˜¯datetimeå¯¹è±¡
                trade_date = latest_record['trade_date']
                if isinstance(trade_date, str):
                    return datetime.strptime(trade_date, '%Y%m%d')
                return trade_date
            return None
        except Exception as e:
            logging.error(f"åœ¨ {collection_name} ä¸­æŸ¥è¯¢ {ts_code} çš„æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
            return None

    def get_all_stock_codes(self) -> List[str]:
        """ä»æ•°æ®åº“è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç """
        try:
            # Implementation of get_all_stock_codes method
            # This method needs to be implemented based on your specific requirements
            # For now, it's left empty as the implementation is not provided in the original file or the new code block
            return []
        except Exception as e:
            logging.error(f"ä»æ•°æ®åº“è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç å¤±è´¥: {e}")
            return []

# å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¯¼å…¥æ—¶å°±åˆ›å»ºè¿æ¥
_db_handler = None

def get_db_handler():
    """è·å–æ•°æ®åº“å¤„ç†å™¨å•ä¾‹"""
    global _db_handler
    if _db_handler is None:
        _db_handler = DBHandler()
    return _db_handler

def reset_db_handler():
    """é‡ç½®æ•°æ®åº“å¤„ç†å™¨å•ä¾‹ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–"""
    global _db_handler
    if _db_handler is not None:
        try:
            _db_handler.__del__()  # å…³é—­ç°æœ‰è¿æ¥
        except:
            pass
    _db_handler = None
    print("ğŸ”„ æ•°æ®åº“å¤„ç†å™¨å·²é‡ç½®")

def check_database_connection():
    """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
    try:
        db_handler = get_db_handler()
        client = db_handler.client # ç›´æ¥è®¿é—® client å±æ€§
        client.server_info()  # æ£€æŸ¥è¿æ¥
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False