#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼ è®ºç»“æ„æ˜ å°„å…³ç³»åˆ†æå™¨
åŸºäºç¼ è®ºç†è®ºæ ¸å¿ƒï¼šæ¬¡çº§èµ°åŠ¿ä¸ä¸Šä¸€çº§èµ°åŠ¿çš„æ˜ å°„å…³ç³»

æ ¸å¿ƒåˆ†æå†…å®¹ï¼š
1. åŒ…å«å…³ç³»ï¼ˆåŒ…å®¹æ€§ï¼‰- å°çº§åˆ«èµ°åŠ¿è¢«å¤§çº§åˆ«èµ°åŠ¿åŒ…å«
2. åˆæˆå…³ç³»ï¼ˆå¤šçº§åˆæˆï¼‰- å¤šä¸ªæ¬¡çº§èµ°åŠ¿åˆæˆä¸Šä¸€çº§èµ°åŠ¿  
3. ä¸­æ¢ç»§æ‰¿å…³ç³» - æ¬¡çº§èµ°åŠ¿ä¸­æ¢è¢«ä¸Šä¸€çº§èµ°åŠ¿ç»§æ‰¿
4. èƒŒç¦»åˆ¤æ–­ - æ¬¡çº§ä¸ä¸Šä¸€çº§èµ°åŠ¿ä¹‹é—´çš„èƒŒç¦»åˆ†æ
5. æ˜ å°„è´¨é‡è¯„ä¼° - æ˜ å°„å…³ç³»çš„å¼ºåº¦å’Œå¯é æ€§
"""

import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from enum import Enum
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from matplotlib.patches import Rectangle, FancyBboxPatch
import json
from pathlib import Path

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
chan_theory_dir = os.path.dirname(current_dir)

sys.path.append(project_root)
sys.path.append(chan_theory_dir)
sys.path.append('/Users/libing/kk_Projects/kk_Stock/kk_stock_backend/api')

try:
    from db_handler import DBHandler
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥æ•°æ®åº“å¤„ç†å™¨")
    DBHandler = None

try:
    from models.chan_theory_models import TrendLevel
except ImportError:
    from analysis.chan_theory.models.chan_theory_models import TrendLevel

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class MappingRelationType(Enum):
    """æ˜ å°„å…³ç³»ç±»å‹"""
    CONTAINMENT = "åŒ…å«å…³ç³»"      # æ¬¡çº§èµ°åŠ¿è¢«ä¸Šä¸€çº§èµ°åŠ¿åŒ…å«
    COMPOSITION = "åˆæˆå…³ç³»"      # å¤šä¸ªæ¬¡çº§èµ°åŠ¿åˆæˆä¸Šä¸€çº§èµ°åŠ¿
    INHERITANCE = "ç»§æ‰¿å…³ç³»"      # ä¸­æ¢ç»“æ„çš„ç»§æ‰¿
    DIVERGENCE = "èƒŒç¦»å…³ç³»"       # æ¬¡çº§ä¸ä¸Šä¸€çº§èƒŒç¦»

class MappingQuality(Enum):
    """æ˜ å°„è´¨é‡ç­‰çº§"""
    EXCELLENT = "ä¼˜ç§€"    # æ˜ å°„å…³ç³»æ¸…æ™°ï¼Œç¬¦åˆç¼ è®ºæ ‡å‡†
    GOOD = "è‰¯å¥½"         # æ˜ å°„å…³ç³»è¾ƒå¥½ï¼ŒåŸºæœ¬ç¬¦åˆæ ‡å‡†
    FAIR = "ä¸€èˆ¬"         # æ˜ å°„å…³ç³»æ¨¡ç³Šï¼Œéœ€è¦è°¨æ…åˆ¤æ–­
    POOR = "è¾ƒå·®"         # æ˜ å°„å…³ç³»ä¸æ¸…æ™°ï¼Œä¸å»ºè®®ä¾æ®

@dataclass
class StructureMapping:
    """ç»“æ„æ˜ å°„å…³ç³»"""
    higher_level: TrendLevel                    # ä¸Šä¸€çº§èµ°åŠ¿çº§åˆ«
    lower_level: TrendLevel                     # æ¬¡çº§èµ°åŠ¿çº§åˆ«
    mapping_type: MappingRelationType           # æ˜ å°„å…³ç³»ç±»å‹
    quality: MappingQuality                     # æ˜ å°„è´¨é‡
    
    # æ˜ å°„è¯¦æƒ…
    containment_ratio: float                    # åŒ…å«æ¯”ä¾‹
    composition_strength: float                 # åˆæˆå¼ºåº¦
    inheritance_quality: float                  # ç»§æ‰¿è´¨é‡
    divergence_strength: float                  # èƒŒç¦»å¼ºåº¦
    
    # æ—¶é—´å’Œä»·æ ¼ä¸€è‡´æ€§
    time_consistency: float                     # æ—¶é—´ä¸€è‡´æ€§
    price_consistency: float                    # ä»·æ ¼ä¸€è‡´æ€§
    trend_consistency: float                    # è¶‹åŠ¿ä¸€è‡´æ€§
    
    # ç»¼åˆè¯„åˆ†
    overall_score: float                        # ç»¼åˆè¯„åˆ† (0-1)
    confidence_level: float                     # ç½®ä¿¡åº¦ (0-1)
    
    # è¯¦ç»†ä¿¡æ¯
    mapping_details: Dict[str, Any]             # æ˜ å°„è¯¦æƒ…
    analysis_notes: str                         # åˆ†æå¤‡æ³¨

@dataclass
class ZhongShuInheritance:
    """ä¸­æ¢ç»§æ‰¿å…³ç³»"""
    parent_level: TrendLevel                    # çˆ¶çº§åˆ«
    child_level: TrendLevel                     # å­çº§åˆ«
    inheritance_type: str                       # ç»§æ‰¿ç±»å‹
    overlap_ratio: float                        # é‡å æ¯”ä¾‹
    continuity_score: float                     # è¿ç»­æ€§è¯„åˆ†
    effectiveness_score: float                  # æœ‰æ•ˆæ€§è¯„åˆ†
    is_valid_inheritance: bool                  # æ˜¯å¦ä¸ºæœ‰æ•ˆç»§æ‰¿

class StructureMappingAnalyzer:
    """ç»“æ„æ˜ å°„å…³ç³»åˆ†æå™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.db_handler = DBHandler()
        self.analysis_results = {}
        
        print("ğŸ” ç¼ è®ºç»“æ„æ˜ å°„å…³ç³»åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def analyze_structure_mappings(self, symbol: str) -> Dict[str, Any]:
        """
        åˆ†æç»“æ„æ˜ å°„å…³ç³»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            
        Returns:
            å®Œæ•´çš„ç»“æ„æ˜ å°„åˆ†æç»“æœ
        """
        print(f"\nğŸ¯ å¼€å§‹åˆ†æ {symbol} çš„ç»“æ„æ˜ å°„å…³ç³»...")
        
        # 1. è·å–å¤šçº§åˆ«çœŸå®æ•°æ®
        multi_data = self._get_real_multi_data(symbol)
        
        if not multi_data:
            return self._empty_analysis_result()
        
        # 2. å„çº§åˆ«ç»“æ„åˆ†æ
        level_structures = self._analyze_level_structures(multi_data)
        
        # 3. æ˜ å°„å…³ç³»åˆ†æ
        structure_mappings = self._analyze_mappings(level_structures, multi_data)
        
        # 4. ä¸­æ¢ç»§æ‰¿åˆ†æ
        zhongshu_inheritances = self._analyze_zhongshu_inheritance(level_structures)
        
        # 5. èƒŒç¦»å…³ç³»åˆ†æ
        divergence_analysis = self._analyze_divergences(level_structures, multi_data)
        
        # 6. æ˜ å°„è´¨é‡è¯„ä¼°
        quality_assessment = self._assess_mapping_quality(structure_mappings, zhongshu_inheritances)
        
        # 7. ç”Ÿæˆç»“è®º
        mapping_conclusions = self._generate_mapping_conclusions(
            structure_mappings, zhongshu_inheritances, divergence_analysis, quality_assessment
        )
        
        return {
            'symbol': symbol,
            'analysis_time': datetime.now(),
            'multi_data': multi_data,
            'level_structures': level_structures,
            'structure_mappings': structure_mappings,
            'zhongshu_inheritances': zhongshu_inheritances,
            'divergence_analysis': divergence_analysis,
            'quality_assessment': quality_assessment,
            'mapping_conclusions': mapping_conclusions
        }
    
    def _get_real_multi_data(self, symbol: str) -> Dict[TrendLevel, pd.DataFrame]:
        """è·å–çœŸå®å¤šçº§åˆ«æ•°æ®"""
        print("ğŸ“Š è·å–å¤šçº§åˆ«çœŸå®æ•°æ®...")
        
        multi_data = {}
        
        try:
            # æ—¥çº¿æ•°æ®
            daily_collection = self.db_handler.get_collection('stock_kline_daily')
            daily_query = {
                'ts_code': symbol,
                'trade_date': {'$gte': '20250101', '$lte': '20250711'}
            }
            daily_cursor = daily_collection.find(daily_query).sort('trade_date', 1)
            daily_data = list(daily_cursor)
            
            if daily_data:
                daily_df = pd.DataFrame(daily_data)
                daily_df['datetime'] = pd.to_datetime(daily_df['trade_date'], format='%Y%m%d')
                daily_df.set_index('datetime', inplace=True)
                daily_df = daily_df[['open', 'high', 'low', 'close', 'vol', 'amount']].astype(float)
                multi_data[TrendLevel.DAILY] = daily_df
                print(f"  âœ… æ—¥çº¿: {len(daily_df)} æ¡")
            
            # 30åˆ†é’Ÿæ•°æ®
            min30_collection = self.db_handler.get_collection('stock_kline_30min')
            min30_cursor = min30_collection.find({'ts_code': symbol}).sort('trade_time', 1)
            min30_data = list(min30_cursor)
            
            if min30_data:
                min30_df = pd.DataFrame(min30_data)
                min30_df['datetime'] = pd.to_datetime(min30_df['trade_time'], format='mixed')
                min30_df.set_index('datetime', inplace=True)
                min30_df = min30_df[['open', 'high', 'low', 'close', 'vol', 'amount']].astype(float)
                min30_df_2025 = min30_df[min30_df.index >= '2025-01-01']
                multi_data[TrendLevel.MIN30] = min30_df_2025
                print(f"  âœ… 30åˆ†é’Ÿ: {len(min30_df_2025)} æ¡")
            
            # 5åˆ†é’Ÿæ•°æ®
            min5_collection = self.db_handler.get_collection('stock_kline_5min')
            min5_cursor = min5_collection.find({'ts_code': symbol}).sort('trade_time', -1).limit(500)
            min5_data = list(min5_cursor)
            
            if min5_data:
                min5_df = pd.DataFrame(min5_data)
                min5_df['datetime'] = pd.to_datetime(min5_df['trade_time'], format='mixed')
                min5_df.set_index('datetime', inplace=True)
                min5_df = min5_df[['open', 'high', 'low', 'close', 'vol', 'amount']].astype(float)
                min5_df = min5_df.sort_index()
                min5_df_2025 = min5_df[min5_df.index >= '2025-01-01']
                multi_data[TrendLevel.MIN5] = min5_df_2025
                print(f"  âœ… 5åˆ†é’Ÿ: {len(min5_df_2025)} æ¡")
            
            return multi_data
            
        except Exception as e:
            print(f"âŒ æ•°æ®è·å–å¤±è´¥: {e}")
            return {}
    
    def _analyze_level_structures(self, multi_data: Dict) -> Dict[TrendLevel, Dict]:
        """åˆ†æå„çº§åˆ«ç»“æ„"""
        print("ğŸ” åˆ†æå„çº§åˆ«ç»“æ„...")
        
        level_structures = {}
        
        for level, data in multi_data.items():
            if data.empty:
                continue
                
            print(f"  ğŸ“Š åˆ†æ {level.value} çº§åˆ«...")
            
            # åˆ†å‹åˆ†æ
            fenxing_result = self._analyze_fenxing(data)
            
            # ç¬”åˆ†æ
            bi_result = self._analyze_bi(fenxing_result)
            
            # ä¸­æ¢åˆ†æ
            zhongshu_result = self._analyze_zhongshu(bi_result, data)
            
            # è¶‹åŠ¿åˆ†æ
            trend_analysis = self._analyze_trend(data, zhongshu_result)
            
            level_structures[level] = {
                'data': data,
                'fenxing': fenxing_result,
                'bi': bi_result,
                'zhongshu': zhongshu_result,
                'trend': trend_analysis,
                'current_price': data.iloc[-1]['close']
            }
            
            print(f"    âœ… åˆ†å‹: {len(fenxing_result['tops'])} é¡¶ + {len(fenxing_result['bottoms'])} åº•")
            print(f"    âœ… ç¬”: {len(bi_result)} æ¡")
            print(f"    âœ… ä¸­æ¢: {len(zhongshu_result)} ä¸ª")
        
        return level_structures
    
    def _analyze_fenxing(self, data: pd.DataFrame, window: int = 3) -> Dict:
        """åˆ†æåˆ†å‹"""
        if len(data) < window * 2 + 1:
            return {'tops': [], 'bottoms': []}
        
        tops = []
        bottoms = []
        
        for i in range(window, len(data) - window):
            # é¡¶åˆ†å‹
            is_top = all(data.iloc[i]['high'] > data.iloc[i+j]['high'] for j in range(-window, window+1) if j != 0)
            if is_top:
                tops.append({
                    'index': i,
                    'timestamp': data.index[i],
                    'price': data.iloc[i]['high'],
                    'strength': (data.iloc[i]['high'] - data.iloc[i-window:i+window+1]['low'].min()) / data.iloc[i]['high']
                })
            
            # åº•åˆ†å‹
            is_bottom = all(data.iloc[i]['low'] < data.iloc[i+j]['low'] for j in range(-window, window+1) if j != 0)
            if is_bottom:
                bottoms.append({
                    'index': i,
                    'timestamp': data.index[i],
                    'price': data.iloc[i]['low'],
                    'strength': (data.iloc[i-window:i+window+1]['high'].max() - data.iloc[i]['low']) / data.iloc[i]['low']
                })
        
        return {'tops': tops, 'bottoms': bottoms}
    
    def _analyze_bi(self, fenxing_result: Dict) -> List[Dict]:
        """åˆ†æç¬”"""
        tops = fenxing_result['tops']
        bottoms = fenxing_result['bottoms']
        
        # åˆå¹¶å¹¶æ’åºæ‰€æœ‰åˆ†å‹
        all_fenxing = tops + bottoms
        all_fenxing.sort(key=lambda x: x['timestamp'])
        
        bis = []
        if len(all_fenxing) >= 2:
            for i in range(len(all_fenxing) - 1):
                start = all_fenxing[i]
                end = all_fenxing[i + 1]
                
                # ç¡®ä¿åˆ†å‹ç±»å‹ä¸åŒ
                start_type = 'top' if start in tops else 'bottom'
                end_type = 'top' if end in tops else 'bottom'
                
                if start_type != end_type:
                    direction = 'up' if start_type == 'bottom' else 'down'
                    amplitude = abs(end['price'] - start['price']) / start['price']
                    
                    bis.append({
                        'start_time': start['timestamp'],
                        'end_time': end['timestamp'],
                        'start_price': start['price'],
                        'end_price': end['price'],
                        'direction': direction,
                        'amplitude': amplitude,
                        'strength': (start['strength'] + end['strength']) / 2
                    })
        
        return bis
    
    def _analyze_zhongshu(self, bi_result: List, data: pd.DataFrame) -> List[Dict]:
        """åˆ†æä¸­æ¢"""
        if len(bi_result) < 3:
            return []
        
        zhongshu_list = []
        
        # å¯»æ‰¾ä¸‰ç¬”æ„æˆçš„ä¸­æ¢
        for i in range(len(bi_result) - 2):
            bi1, bi2, bi3 = bi_result[i], bi_result[i+1], bi_result[i+2]
            
            # æ£€æŸ¥æ˜¯å¦æ„æˆä¸­æ¢ï¼ˆä¸‰ç¬”æœ‰é‡å ï¼‰
            if bi1['direction'] != bi2['direction'] and bi2['direction'] != bi3['direction']:
                # è®¡ç®—é‡å åŒºåŸŸ
                high_overlap = min(bi1['end_price'], bi3['end_price']) if bi1['direction'] == 'up' else min(bi1['start_price'], bi3['start_price'])
                low_overlap = max(bi1['end_price'], bi3['end_price']) if bi1['direction'] == 'down' else max(bi1['start_price'], bi3['start_price'])
                
                if high_overlap > low_overlap:  # å­˜åœ¨é‡å 
                    zhongshu_list.append({
                        'start_time': bi1['start_time'],
                        'end_time': bi3['end_time'],
                        'high': high_overlap,
                        'low': low_overlap,
                        'center': (high_overlap + low_overlap) / 2,
                        'range_ratio': (high_overlap - low_overlap) / low_overlap,
                        'forming_bis': [bi1, bi2, bi3]
                    })
        
        return zhongshu_list
    
    def _analyze_trend(self, data: pd.DataFrame, zhongshu_result: List) -> Dict:
        """åˆ†æè¶‹åŠ¿"""
        if data.empty:
            return {'direction': 'sideways', 'strength': 0.0}
        
        # ç®€å•è¶‹åŠ¿åˆ¤æ–­
        if len(data) >= 20:
            recent_high = data['high'].tail(10).max()
            recent_low = data['low'].tail(10).min()
            earlier_high = data['high'].head(10).max()
            earlier_low = data['low'].head(10).min()
            
            if recent_high > earlier_high and recent_low > earlier_low:
                direction = 'up'
                strength = (recent_high - earlier_high) / earlier_high
            elif recent_high < earlier_high and recent_low < earlier_low:
                direction = 'down'
                strength = (earlier_high - recent_high) / earlier_high
            else:
                direction = 'sideways'
                strength = 0.5
        else:
            direction = 'sideways'
            strength = 0.5
        
        return {
            'direction': direction,
            'strength': min(strength, 1.0),
            'zhongshu_count': len(zhongshu_result)
        }
    
    def _analyze_mappings(self, level_structures: Dict, multi_data: Dict) -> List[StructureMapping]:
        """åˆ†æç»“æ„æ˜ å°„å…³ç³»"""
        print("ğŸ” åˆ†æç»“æ„æ˜ å°„å…³ç³»...")
        
        mappings = []
        
        # æ—¥çº¿ -> 30åˆ†é’Ÿæ˜ å°„
        if TrendLevel.DAILY in level_structures and TrendLevel.MIN30 in level_structures:
            mapping = self._analyze_level_mapping(
                TrendLevel.DAILY, TrendLevel.MIN30,
                level_structures[TrendLevel.DAILY],
                level_structures[TrendLevel.MIN30]
            )
            mappings.append(mapping)
        
        # 30åˆ†é’Ÿ -> 5åˆ†é’Ÿæ˜ å°„
        if TrendLevel.MIN30 in level_structures and TrendLevel.MIN5 in level_structures:
            mapping = self._analyze_level_mapping(
                TrendLevel.MIN30, TrendLevel.MIN5,
                level_structures[TrendLevel.MIN30],
                level_structures[TrendLevel.MIN5]
            )
            mappings.append(mapping)
        
        # æ—¥çº¿ -> 5åˆ†é’Ÿæ˜ å°„ï¼ˆè·¨çº§åˆ«ï¼‰
        if TrendLevel.DAILY in level_structures and TrendLevel.MIN5 in level_structures:
            mapping = self._analyze_level_mapping(
                TrendLevel.DAILY, TrendLevel.MIN5,
                level_structures[TrendLevel.DAILY],
                level_structures[TrendLevel.MIN5]
            )
            mappings.append(mapping)
        
        return mappings
    
    def _analyze_level_mapping(self, higher_level: TrendLevel, lower_level: TrendLevel,
                              higher_structure: Dict, lower_structure: Dict) -> StructureMapping:
        """åˆ†æä¸¤ä¸ªçº§åˆ«ä¹‹é—´çš„æ˜ å°„å…³ç³»"""
        
        # 1. åŒ…å«å…³ç³»åˆ†æ
        containment_ratio = self._calculate_containment_ratio(higher_structure, lower_structure)
        
        # 2. åˆæˆå…³ç³»åˆ†æ
        composition_strength = self._calculate_composition_strength(higher_structure, lower_structure)
        
        # 3. ç»§æ‰¿è´¨é‡åˆ†æ
        inheritance_quality = self._calculate_inheritance_quality(higher_structure, lower_structure)
        
        # 4. èƒŒç¦»å¼ºåº¦åˆ†æ
        divergence_strength = self._calculate_divergence_strength(higher_structure, lower_structure)
        
        # 5. ä¸€è‡´æ€§åˆ†æ
        time_consistency = self._calculate_time_consistency(higher_structure, lower_structure)
        price_consistency = self._calculate_price_consistency(higher_structure, lower_structure)
        trend_consistency = self._calculate_trend_consistency(higher_structure, lower_structure)
        
        # 6. ç¡®å®šæ˜ å°„ç±»å‹
        mapping_type = self._determine_mapping_type(
            containment_ratio, composition_strength, inheritance_quality, divergence_strength
        )
        
        # 7. ç»¼åˆè¯„åˆ†
        overall_score = (containment_ratio + composition_strength + inheritance_quality + 
                        time_consistency + price_consistency + trend_consistency) / 6
        
        # 8. è´¨é‡ç­‰çº§
        quality = self._determine_mapping_quality(overall_score)
        
        # 9. ç½®ä¿¡åº¦
        confidence_level = min(overall_score + 0.1, 1.0)
        
        return StructureMapping(
            higher_level=higher_level,
            lower_level=lower_level,
            mapping_type=mapping_type,
            quality=quality,
            containment_ratio=containment_ratio,
            composition_strength=composition_strength,
            inheritance_quality=inheritance_quality,
            divergence_strength=divergence_strength,
            time_consistency=time_consistency,
            price_consistency=price_consistency,
            trend_consistency=trend_consistency,
            overall_score=overall_score,
            confidence_level=confidence_level,
            mapping_details={
                'higher_fenxing_count': len(higher_structure['fenxing']['tops']) + len(higher_structure['fenxing']['bottoms']),
                'lower_fenxing_count': len(lower_structure['fenxing']['tops']) + len(lower_structure['fenxing']['bottoms']),
                'higher_bi_count': len(higher_structure['bi']),
                'lower_bi_count': len(lower_structure['bi']),
                'higher_zhongshu_count': len(higher_structure['zhongshu']),
                'lower_zhongshu_count': len(lower_structure['zhongshu'])
            },
            analysis_notes=f"{higher_level.value}çº§åˆ«ä¸{lower_level.value}çº§åˆ«çš„{mapping_type.value}"
        )
    
    def _calculate_containment_ratio(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—åŒ…å«æ¯”ä¾‹"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ—¶é—´èŒƒå›´çš„åŒ…å«å…³ç³»
        higher_data = higher_structure['data']
        lower_data = lower_structure['data']
        
        if higher_data.empty or lower_data.empty:
            return 0.0
        
        higher_start = higher_data.index[0]
        higher_end = higher_data.index[-1]
        lower_start = lower_data.index[0]
        lower_end = lower_data.index[-1]
        
        # è®¡ç®—æ—¶é—´é‡å æ¯”ä¾‹
        overlap_start = max(higher_start, lower_start)
        overlap_end = min(higher_end, lower_end)
        
        if overlap_end <= overlap_start:
            return 0.0
        
        overlap_duration = (overlap_end - overlap_start).total_seconds()
        lower_duration = (lower_end - lower_start).total_seconds()
        
        return min(overlap_duration / lower_duration, 1.0) if lower_duration > 0 else 0.0
    
    def _calculate_composition_strength(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—åˆæˆå¼ºåº¦ - ä¼˜åŒ–ç‰ˆæœ¬"""
        # åŸºäºç»“æ„æ•°é‡çš„æ¯”ä¾‹å…³ç³»
        higher_bi_count = len(higher_structure['bi'])
        lower_bi_count = len(lower_structure['bi'])
        
        # ç¡®ä¿æœ‰åŸºç¡€åˆ†æ•°
        if higher_bi_count == 0 and lower_bi_count == 0:
            return 0.3  # åŸºç¡€åˆ†æ•°
        elif higher_bi_count == 0:
            return 0.5  # æ¬¡çº§æœ‰ç»“æ„ï¼Œä¸Šçº§æ— ç»“æ„
        
        # ç†æƒ³æƒ…å†µä¸‹ï¼Œæ¬¡çº§èµ°åŠ¿çš„ç¬”æ•°åº”è¯¥æ˜¯ä¸Šä¸€çº§çš„æ•°å€
        composition_ratio = lower_bi_count / higher_bi_count
        
        # æ›´å®½æ¾çš„æ¯”ä¾‹èŒƒå›´åˆ¤æ–­
        if 2 <= composition_ratio <= 20:
            return 0.8
        elif 1 <= composition_ratio <= 30:
            return 0.6
        elif 0.5 <= composition_ratio <= 50:
            return 0.4
        else:
            return 0.3  # è‡³å°‘ç»™åŸºç¡€åˆ†
    
    def _calculate_inheritance_quality(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—ç»§æ‰¿è´¨é‡"""
        higher_zs_count = len(higher_structure['zhongshu'])
        lower_zs_count = len(lower_structure['zhongshu'])
        
        if higher_zs_count == 0 and lower_zs_count == 0:
            return 0.5
        
        # ä¸­æ¢ç»§æ‰¿çš„è¿ç»­æ€§å’Œæœ‰æ•ˆæ€§
        if higher_zs_count > 0 and lower_zs_count > 0:
            # ç®€åŒ–ï¼šåŸºäºä¸­æ¢æ•°é‡çš„æ¯”ä¾‹å…³ç³»
            ratio = lower_zs_count / higher_zs_count
            if ratio >= 2:
                return 0.8
            elif ratio >= 1:
                return 0.6
            else:
                return 0.4
        
        return 0.3
    
    def _calculate_divergence_strength(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—èƒŒç¦»å¼ºåº¦"""
        higher_trend = higher_structure['trend']
        lower_trend = lower_structure['trend']
        
        # è¶‹åŠ¿æ–¹å‘ä¸€è‡´æ€§
        if higher_trend['direction'] == lower_trend['direction']:
            return 0.1  # æ— èƒŒç¦»
        elif higher_trend['direction'] == 'sideways' or lower_trend['direction'] == 'sideways':
            return 0.3  # è½»å¾®èƒŒç¦»
        else:
            return 0.7  # æ˜æ˜¾èƒŒç¦»
    
    def _calculate_time_consistency(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—æ—¶é—´ä¸€è‡´æ€§"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæ•°æ®æ—¶é—´èŒƒå›´çš„é‡å ç¨‹åº¦
        return 0.7  # é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥è®¡ç®—æ—¶é—´åºåˆ—çš„ä¸€è‡´æ€§
    
    def _calculate_price_consistency(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—ä»·æ ¼ä¸€è‡´æ€§"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºä»·æ ¼èŒƒå›´çš„ä¸€è‡´æ€§
        higher_price = higher_structure['current_price']
        lower_price = lower_structure['current_price']
        
        price_diff = abs(higher_price - lower_price) / max(higher_price, lower_price)
        return max(0.0, 1.0 - price_diff)
    
    def _calculate_trend_consistency(self, higher_structure: Dict, lower_structure: Dict) -> float:
        """è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§"""
        higher_trend = higher_structure['trend']['direction']
        lower_trend = lower_structure['trend']['direction']
        
        if higher_trend == lower_trend:
            return 0.9
        elif higher_trend == 'sideways' or lower_trend == 'sideways':
            return 0.6
        else:
            return 0.3
    
    def _determine_mapping_type(self, containment: float, composition: float, 
                               inheritance: float, divergence: float) -> MappingRelationType:
        """ç¡®å®šæ˜ å°„ç±»å‹"""
        if containment > 0.7:
            return MappingRelationType.CONTAINMENT
        elif composition > 0.6:
            return MappingRelationType.COMPOSITION
        elif inheritance > 0.6:
            return MappingRelationType.INHERITANCE
        elif divergence > 0.5:
            return MappingRelationType.DIVERGENCE
        else:
            return MappingRelationType.CONTAINMENT
    
    def _determine_mapping_quality(self, overall_score: float) -> MappingQuality:
        """ç¡®å®šæ˜ å°„è´¨é‡ - ä¼˜åŒ–ç‰ˆæœ¬"""
        if overall_score >= 0.7:
            return MappingQuality.EXCELLENT
        elif overall_score >= 0.5:
            return MappingQuality.GOOD
        elif overall_score >= 0.3:
            return MappingQuality.FAIR
        else:
            return MappingQuality.POOR
    
    def _analyze_zhongshu_inheritance(self, level_structures: Dict) -> List[ZhongShuInheritance]:
        """åˆ†æä¸­æ¢ç»§æ‰¿å…³ç³»"""
        print("ğŸ” åˆ†æä¸­æ¢ç»§æ‰¿å…³ç³»...")
        
        inheritances = []
        
        # åˆ†æç›¸é‚»çº§åˆ«çš„ä¸­æ¢ç»§æ‰¿
        levels = list(level_structures.keys())
        for i in range(len(levels) - 1):
            parent_level = levels[i]
            child_level = levels[i + 1]
            
            parent_zhongshu = level_structures[parent_level]['zhongshu']
            child_zhongshu = level_structures[child_level]['zhongshu']
            
            # åˆ†ææ¯ä¸ªçˆ¶çº§ä¸­æ¢ä¸å­çº§ä¸­æ¢çš„ç»§æ‰¿å…³ç³»
            for parent_zs in parent_zhongshu:
                for child_zs in child_zhongshu:
                    inheritance = self._analyze_single_inheritance(
                        parent_level, child_level, parent_zs, child_zs
                    )
                    if inheritance.is_valid_inheritance:
                        inheritances.append(inheritance)
        
        return inheritances
    
    def _analyze_single_inheritance(self, parent_level: TrendLevel, child_level: TrendLevel,
                                   parent_zs: Dict, child_zs: Dict) -> ZhongShuInheritance:
        """åˆ†æå•ä¸ªä¸­æ¢ç»§æ‰¿å…³ç³»"""
        
        # è®¡ç®—æ—¶é—´é‡å 
        parent_start = parent_zs['start_time']
        parent_end = parent_zs['end_time']
        child_start = child_zs['start_time']
        child_end = child_zs['end_time']
        
        overlap_start = max(parent_start, child_start)
        overlap_end = min(parent_end, child_end)
        
        if overlap_end <= overlap_start:
            overlap_ratio = 0.0
        else:
            overlap_duration = (overlap_end - overlap_start).total_seconds()
            child_duration = (child_end - child_start).total_seconds()
            overlap_ratio = overlap_duration / child_duration if child_duration > 0 else 0.0
        
        # è®¡ç®—ä»·æ ¼é‡å 
        parent_high = parent_zs['high']
        parent_low = parent_zs['low']
        child_high = child_zs['high']
        child_low = child_zs['low']
        
        price_overlap_high = min(parent_high, child_high)
        price_overlap_low = max(parent_low, child_low)
        
        if price_overlap_high > price_overlap_low:
            price_overlap_ratio = (price_overlap_high - price_overlap_low) / (child_high - child_low)
        else:
            price_overlap_ratio = 0.0
        
        # ç»§æ‰¿ç±»å‹åˆ¤æ–­
        if overlap_ratio >= 0.8 and price_overlap_ratio >= 0.8:
            inheritance_type = "å®Œå…¨ç»§æ‰¿"
            is_valid = True
        elif overlap_ratio >= 0.6 and price_overlap_ratio >= 0.6:
            inheritance_type = "ä¸»è¦ç»§æ‰¿"
            is_valid = True
        elif overlap_ratio >= 0.3 and price_overlap_ratio >= 0.3:
            inheritance_type = "éƒ¨åˆ†ç»§æ‰¿"
            is_valid = True
        else:
            inheritance_type = "å¼±ç»§æ‰¿"
            is_valid = False
        
        return ZhongShuInheritance(
            parent_level=parent_level,
            child_level=child_level,
            inheritance_type=inheritance_type,
            overlap_ratio=overlap_ratio,
            continuity_score=overlap_ratio,
            effectiveness_score=price_overlap_ratio,
            is_valid_inheritance=is_valid
        )
    
    def _analyze_divergences(self, level_structures: Dict, multi_data: Dict) -> Dict[str, Any]:
        """åˆ†æèƒŒç¦»å…³ç³»"""
        print("ğŸ” åˆ†æèƒŒç¦»å…³ç³»...")
        
        divergences = []
        
        # åˆ†æç›¸é‚»çº§åˆ«é—´çš„èƒŒç¦»
        levels = list(level_structures.keys())
        for i in range(len(levels) - 1):
            higher_level = levels[i]
            lower_level = levels[i + 1]
            
            higher_structure = level_structures[higher_level]
            lower_structure = level_structures[lower_level]
            
            # æ£€æµ‹èƒŒç¦»ä¿¡å·
            divergence = self._detect_divergence(higher_level, lower_level, higher_structure, lower_structure)
            if divergence:
                divergences.append(divergence)
        
        return {
            'divergences': divergences,
            'total_count': len(divergences),
            'strong_divergences': len([d for d in divergences if d.get('strength', 0) > 0.6])
        }
    
    def _detect_divergence(self, higher_level: TrendLevel, lower_level: TrendLevel,
                          higher_structure: Dict, lower_structure: Dict) -> Optional[Dict]:
        """æ£€æµ‹ä¸¤ä¸ªçº§åˆ«é—´çš„èƒŒç¦»"""
        
        higher_trend = higher_structure['trend']
        lower_trend = lower_structure['trend']
        
        # è¶‹åŠ¿æ–¹å‘èƒŒç¦»
        if higher_trend['direction'] != lower_trend['direction'] and \
           higher_trend['direction'] != 'sideways' and lower_trend['direction'] != 'sideways':
            
            strength = abs(higher_trend['strength'] - lower_trend['strength'])
            
            return {
                'type': 'trend_divergence',
                'higher_level': higher_level,
                'lower_level': lower_level,
                'higher_trend': higher_trend['direction'],
                'lower_trend': lower_trend['direction'],
                'strength': strength,
                'description': f"{higher_level.value}çº§åˆ«{higher_trend['direction']}è¶‹åŠ¿ä¸{lower_level.value}çº§åˆ«{lower_trend['direction']}è¶‹åŠ¿èƒŒç¦»"
            }
        
        return None
    
    def _assess_mapping_quality(self, mappings: List[StructureMapping], 
                               inheritances: List[ZhongShuInheritance]) -> Dict[str, Any]:
        """è¯„ä¼°æ˜ å°„è´¨é‡"""
        print("ğŸ“Š è¯„ä¼°æ˜ å°„è´¨é‡...")
        
        if not mappings:
            return {'overall_quality': 0.0, 'assessment': 'æ— æ˜ å°„å…³ç³»'}
        
        # è®¡ç®—æ•´ä½“è´¨é‡
        quality_scores = [m.overall_score for m in mappings]
        overall_quality = sum(quality_scores) / len(quality_scores)
        
        # ç»§æ‰¿è´¨é‡
        if inheritances:
            inheritance_scores = [i.continuity_score * i.effectiveness_score for i in inheritances if i.is_valid_inheritance]
            inheritance_quality = sum(inheritance_scores) / len(inheritance_scores) if inheritance_scores else 0.0
        else:
            inheritance_quality = 0.0
        
        # ç»¼åˆè¯„ä¼°
        comprehensive_score = (overall_quality * 0.7 + inheritance_quality * 0.3)
        
        # è´¨é‡ç­‰çº§
        if comprehensive_score >= 0.8:
            assessment = "ä¼˜ç§€ - æ˜ å°„å…³ç³»æ¸…æ™°ï¼Œç¬¦åˆç¼ è®ºç†è®º"
        elif comprehensive_score >= 0.6:
            assessment = "è‰¯å¥½ - æ˜ å°„å…³ç³»è¾ƒå¥½ï¼ŒåŸºæœ¬ç¬¦åˆç†è®º"
        elif comprehensive_score >= 0.4:
            assessment = "ä¸€èˆ¬ - æ˜ å°„å…³ç³»æ¨¡ç³Šï¼Œéœ€è°¨æ…åˆ¤æ–­"
        else:
            assessment = "è¾ƒå·® - æ˜ å°„å…³ç³»ä¸æ¸…æ™°ï¼Œä¸å»ºè®®ä¾æ®"
        
        return {
            'overall_quality': overall_quality,
            'inheritance_quality': inheritance_quality,
            'comprehensive_score': comprehensive_score,
            'assessment': assessment,
            'mapping_count': len(mappings),
            'valid_inheritance_count': len([i for i in inheritances if i.is_valid_inheritance])
        }
    
    def _generate_mapping_conclusions(self, mappings: List[StructureMapping], 
                                    inheritances: List[ZhongShuInheritance],
                                    divergences: Dict, quality_assessment: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆæ˜ å°„ç»“è®º"""
        print("ğŸ“ ç”Ÿæˆæ˜ å°„ç»“è®º...")
        
        conclusions = {
            'summary': f"å…±å‘ç° {len(mappings)} ä¸ªç»“æ„æ˜ å°„å…³ç³»",
            'key_findings': [],
            'mapping_analysis': {},
            'inheritance_analysis': {},
            'divergence_analysis': {},
            'overall_assessment': quality_assessment['assessment'],
            'recommendations': []
        }
        
        # æ˜ å°„å…³ç³»åˆ†æ
        if mappings:
            best_mapping = max(mappings, key=lambda x: x.overall_score)
            conclusions['mapping_analysis'] = {
                'best_mapping': f"{best_mapping.higher_level.value} -> {best_mapping.lower_level.value}",
                'best_quality': best_mapping.quality.value,
                'best_score': best_mapping.overall_score,
                'dominant_type': best_mapping.mapping_type.value
            }
            
            conclusions['key_findings'].append(
                f"æœ€å¼ºæ˜ å°„å…³ç³»: {best_mapping.higher_level.value} -> {best_mapping.lower_level.value} ({best_mapping.mapping_type.value})"
            )
        
        # ç»§æ‰¿å…³ç³»åˆ†æ
        if inheritances:
            valid_inheritances = [i for i in inheritances if i.is_valid_inheritance]
            if valid_inheritances:
                best_inheritance = max(valid_inheritances, key=lambda x: x.overlap_ratio)
                conclusions['inheritance_analysis'] = {
                    'valid_count': len(valid_inheritances),
                    'best_inheritance': f"{best_inheritance.parent_level.value} -> {best_inheritance.child_level.value}",
                    'best_type': best_inheritance.inheritance_type,
                    'best_overlap': best_inheritance.overlap_ratio
                }
                
                conclusions['key_findings'].append(
                    f"æœ€å¼ºä¸­æ¢ç»§æ‰¿: {best_inheritance.parent_level.value} -> {best_inheritance.child_level.value} ({best_inheritance.inheritance_type})"
                )
        
        # èƒŒç¦»åˆ†æ
        divergence_count = divergences.get('total_count', 0)
        if divergence_count > 0:
            conclusions['divergence_analysis'] = {
                'total_divergences': divergence_count,
                'strong_divergences': divergences.get('strong_divergences', 0)
            }
            
            conclusions['key_findings'].append(f"å‘ç° {divergence_count} ä¸ªçº§åˆ«é—´èƒŒç¦»ä¿¡å·")
        
        # æ“ä½œå»ºè®®
        overall_score = quality_assessment.get('comprehensive_score', 0)
        if overall_score >= 0.7:
            conclusions['recommendations'].append("æ˜ å°„å…³ç³»æ¸…æ™°ï¼Œå¯ä½œä¸ºåˆ†æä¾æ®")
            conclusions['recommendations'].append("å¤šçº§åˆ«ç»“æ„ä¸€è‡´æ€§è¾ƒå¥½ï¼Œä¿¡å·å¯é æ€§é«˜")
        elif overall_score >= 0.5:
            conclusions['recommendations'].append("æ˜ å°„å…³ç³»åŸºæœ¬æˆç«‹ï¼Œå»ºè®®ç»“åˆå…¶ä»–æŒ‡æ ‡ç¡®è®¤")
            conclusions['recommendations'].append("æ³¨æ„è§‚å¯Ÿçº§åˆ«é—´çš„èƒŒç¦»ä¿¡å·")
        else:
            conclusions['recommendations'].append("æ˜ å°„å…³ç³»ä¸å¤Ÿæ¸…æ™°ï¼Œå»ºè®®è°¨æ…æ“ä½œ")
            conclusions['recommendations'].append("ç­‰å¾…æ›´æ˜ç¡®çš„å¤šçº§åˆ«ç»“æ„ä¿¡å·")
        
        return conclusions
    
    def _empty_analysis_result(self) -> Dict[str, Any]:
        """ç©ºåˆ†æç»“æœ"""
        return {
            'symbol': '',
            'analysis_time': datetime.now(),
            'multi_data': {},
            'level_structures': {},
            'structure_mappings': [],
            'zhongshu_inheritances': [],
            'divergence_analysis': {'divergences': [], 'total_count': 0},
            'quality_assessment': {'overall_quality': 0.0, 'assessment': 'æ— æ•°æ®'},
            'mapping_conclusions': {'summary': 'æ— æ•°æ®å¯åˆ†æ'}
        }
    
    def create_mapping_visualization(self, analysis_result: Dict[str, Any], save_path: str = None) -> str:
        """åˆ›å»ºç»“æ„æ˜ å°„å¯è§†åŒ–"""
        print("ğŸ¨ åˆ›å»ºç»“æ„æ˜ å°„å¯è§†åŒ–...")
        
        try:
            symbol = analysis_result['symbol']
            multi_data = analysis_result['multi_data']
            mappings = analysis_result['structure_mappings']
            
            if not multi_data:
                print("âš ï¸ æ— æ•°æ®ï¼Œè·³è¿‡å¯è§†åŒ–")
                return ""
            
            # åˆ›å»ºå¤æ‚çš„å¤šå­å›¾å¸ƒå±€
            fig = plt.figure(figsize=(20, 16))
            
            # ä¸»æ ‡é¢˜
            fig.suptitle(f'{symbol} ç¼ è®ºç»“æ„æ˜ å°„å…³ç³»åˆ†æ', fontsize=18, fontweight='bold')
            
            # å¸ƒå±€ï¼šä¸Šæ–¹3ä¸ªæ—¶é—´çº§åˆ«å›¾è¡¨ï¼Œä¸‹æ–¹2ä¸ªåˆ†æå›¾è¡¨
            gs = fig.add_gridspec(3, 3, height_ratios=[2, 2, 1], hspace=0.3, wspace=0.2)
            
            # ç»˜åˆ¶å„çº§åˆ«å›¾è¡¨
            levels = [TrendLevel.MIN5, TrendLevel.MIN30, TrendLevel.DAILY]
            level_names = ['æ—¥çº¿çº§åˆ«', 'å‘¨çº¿çº§åˆ«', 'æœˆçº¿çº§åˆ«']
            
            for i, (level, name) in enumerate(zip(levels, level_names)):
                if level in multi_data and not multi_data[level].empty:
                    ax = fig.add_subplot(gs[0, i])
                    self._plot_level_analysis(ax, level, analysis_result, name)
            
            # æ˜ å°„å…³ç³»å›¾
            ax_mapping = fig.add_subplot(gs[1, :2])
            self._plot_mapping_relationships(ax_mapping, mappings)
            
            # è´¨é‡è¯„ä¼°å›¾
            ax_quality = fig.add_subplot(gs[1, 2])
            self._plot_quality_assessment(ax_quality, analysis_result['quality_assessment'])
            
            # ç»“è®ºæ–‡å­—
            ax_conclusion = fig.add_subplot(gs[2, :])
            self._plot_conclusions(ax_conclusion, analysis_result['mapping_conclusions'])
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            if not save_path:
                results_dir = Path(current_dir) / "results"
                results_dir.mkdir(exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                save_path = results_dir / f"structure_mapping_{symbol}_{timestamp}.png"
            
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ“Š ç»“æ„æ˜ å°„å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
            return str(save_path)
            
        except Exception as e:
            print(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return ""
    
    def _plot_level_analysis(self, ax, level: TrendLevel, analysis_result: Dict, title: str):
        """ç»˜åˆ¶å•ä¸ªçº§åˆ«çš„åˆ†æ"""
        data = analysis_result['multi_data'][level]
        structure = analysis_result['level_structures'][level]
        
        # ç»˜åˆ¶Kçº¿
        for timestamp, row in data.iterrows():
            color = '#FF6B6B' if row['close'] >= row['open'] else '#4ECDC4'
            ax.plot([timestamp, timestamp], [row['low'], row['high']], color=color, linewidth=1)
            
            body_height = abs(row['close'] - row['open'])
            body_bottom = min(row['open'], row['close'])
            rect = Rectangle((timestamp, body_bottom), timedelta(hours=4), body_height,
                           facecolor=color, alpha=0.7, edgecolor=color)
            ax.add_patch(rect)
        
        # ç»˜åˆ¶åˆ†å‹
        fenxing = structure['fenxing']
        for top in fenxing['tops']:
            ax.scatter(top['timestamp'], top['price'], color='red', marker='v', s=60, zorder=5)
        for bottom in fenxing['bottoms']:
            ax.scatter(bottom['timestamp'], bottom['price'], color='green', marker='^', s=60, zorder=5)
        
        # ç»˜åˆ¶ç¬”
        for bi in structure['bi']:
            ax.plot([bi['start_time'], bi['end_time']], [bi['start_price'], bi['end_price']], 
                   color='orange', linewidth=2, alpha=0.8)
        
        # ç»˜åˆ¶ä¸­æ¢
        for zs in structure['zhongshu']:
            duration = zs['end_time'] - zs['start_time']
            duration_days = duration.total_seconds() / (24 * 3600)  # è½¬æ¢ä¸ºå¤©æ•°
            height = zs['high'] - zs['low']
            rect = FancyBboxPatch((zs['start_time'], zs['low']), duration_days, height,
                                boxstyle="round,pad=0.01", facecolor='yellow', alpha=0.3)
            ax.add_patch(rect)
        
        ax.set_title(f"{title}\nåˆ†å‹:{len(fenxing['tops'])+len(fenxing['bottoms'])} ç¬”:{len(structure['bi'])} ä¸­æ¢:{len(structure['zhongshu'])}")
        ax.grid(True, alpha=0.3)
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
    
    def _plot_mapping_relationships(self, ax, mappings: List[StructureMapping]):
        """ç»˜åˆ¶æ˜ å°„å…³ç³»å›¾"""
        if not mappings:
            ax.text(0.5, 0.5, 'æš‚æ— æ˜ å°„å…³ç³»', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('ç»“æ„æ˜ å°„å…³ç³»')
            return
        
        # åˆ›å»ºæ˜ å°„å…³ç³»çš„ç½‘ç»œå›¾
        levels = ['daily', '30min', '5min']
        positions = {level: (i, 0) for i, level in enumerate(levels)}
        
        # ç»˜åˆ¶èŠ‚ç‚¹
        for i, level in enumerate(levels):
            ax.scatter(i, 0, s=500, c='lightblue', edgecolors='black', zorder=5)
            ax.text(i, 0, level, ha='center', va='center', fontweight='bold')
        
        # ç»˜åˆ¶æ˜ å°„è¿çº¿
        for mapping in mappings:
            higher_idx = levels.index(mapping.higher_level.value.replace('min', 'min'))
            lower_idx = levels.index(mapping.lower_level.value.replace('min', 'min'))
            
            # è¿çº¿é¢œè‰²è¡¨ç¤ºè´¨é‡
            if mapping.quality == MappingQuality.EXCELLENT:
                color = 'green'
            elif mapping.quality == MappingQuality.GOOD:
                color = 'orange'
            else:
                color = 'red'
            
            # ç»˜åˆ¶ç®­å¤´
            ax.annotate('', xy=(lower_idx, 0.1), xytext=(higher_idx, -0.1),
                       arrowprops=dict(arrowstyle='->', color=color, lw=3))
            
            # æ·»åŠ æ ‡ç­¾
            mid_x = (higher_idx + lower_idx) / 2
            ax.text(mid_x, 0.2, f"{mapping.mapping_type.value}\n{mapping.quality.value}", 
                   ha='center', va='bottom', fontsize=8)
        
        ax.set_xlim(-0.5, 2.5)
        ax.set_ylim(-0.5, 0.5)
        ax.set_title('ç»“æ„æ˜ å°„å…³ç³»ç½‘ç»œ')
        ax.axis('off')
    
    def _plot_quality_assessment(self, ax, quality: Dict[str, Any]):
        """ç»˜åˆ¶è´¨é‡è¯„ä¼°å›¾"""
        scores = [
            quality.get('overall_quality', 0),
            quality.get('inheritance_quality', 0),
            quality.get('comprehensive_score', 0)
        ]
        labels = ['æ˜ å°„è´¨é‡', 'ç»§æ‰¿è´¨é‡', 'ç»¼åˆè¯„åˆ†']
        colors = ['#FF9999', '#66B2FF', '#99FF99']
        
        bars = ax.bar(labels, scores, color=colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, score in zip(bars, scores):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                   f'{score:.2f}', ha='center', va='bottom')
        
        ax.set_ylim(0, 1)
        ax.set_ylabel('è¯„åˆ†')
        ax.set_title('æ˜ å°„è´¨é‡è¯„ä¼°')
        ax.grid(True, alpha=0.3)
    
    def _plot_conclusions(self, ax, conclusions: Dict[str, Any]):
        """ç»˜åˆ¶ç»“è®ºæ–‡å­—"""
        ax.axis('off')
        
        text_content = []
        text_content.append(f"ğŸ“Š {conclusions.get('summary', '')}")
        
        key_findings = conclusions.get('key_findings', [])
        if key_findings:
            text_content.append("\nğŸ” å…³é”®å‘ç°:")
            for finding in key_findings[:3]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                text_content.append(f"  â€¢ {finding}")
        
        recommendations = conclusions.get('recommendations', [])
        if recommendations:
            text_content.append("\nğŸ’¡ æ“ä½œå»ºè®®:")
            for rec in recommendations[:2]:  # é™åˆ¶æ˜¾ç¤ºæ•°é‡
                text_content.append(f"  â€¢ {rec}")
        
        text_content.append(f"\nğŸ“ˆ æ•´ä½“è¯„ä¼°: {conclusions.get('overall_assessment', '')}")
        
        full_text = '\n'.join(text_content)
        ax.text(0.05, 0.95, full_text, transform=ax.transAxes, fontsize=10,
               verticalalignment='top', fontfamily='monospace')


def run_structure_mapping_test():
    """è¿è¡Œç»“æ„æ˜ å°„æµ‹è¯•"""
    print("ğŸš€ ç¼ è®ºç»“æ„æ˜ å°„å…³ç³»åˆ†ææµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = StructureMappingAnalyzer()
        
        # åˆ†æç»“æ„æ˜ å°„
        symbol = "300750.SZ"
        analysis_result = analyzer.analyze_structure_mappings(symbol)
        
        if not analysis_result['multi_data']:
            print("âŒ æ— æ•°æ®å¯åˆ†æ")
            return False
        
        # åˆ›å»ºå¯è§†åŒ–
        viz_path = analyzer.create_mapping_visualization(analysis_result)
        
        # ä¿å­˜åˆ†æç»“æœ
        results_dir = Path(current_dir) / "results"
        results_dir.mkdir(exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONç»“æœ
        json_file = results_dir / f"{symbol}_{timestamp}_structure_mapping.json"
        
        def make_serializable(obj):
            if isinstance(obj, dict):
                # è½¬æ¢å­—å…¸é”®å€¼ä¸ºå­—ç¬¦ä¸²ï¼Œç‰¹åˆ«å¤„ç†TrendLevelæšä¸¾
                result = {}
                for k, v in obj.items():
                    if hasattr(k, 'value'):  # å¤„ç†æšä¸¾é”®
                        key = k.value
                    elif isinstance(k, str):
                        key = k
                    else:
                        key = str(k)
                    result[key] = make_serializable(v)
                return result
            elif isinstance(obj, list):
                return [make_serializable(item) for item in obj]
            elif isinstance(obj, datetime):
                return obj.isoformat()
            elif isinstance(obj, pd.DataFrame):
                return f"DataFrame with {len(obj)} rows"
            elif hasattr(obj, 'value'):
                return obj.value
            elif hasattr(obj, '__dict__'):
                return make_serializable(obj.__dict__)
            else:
                return obj
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(make_serializable(analysis_result), f, ensure_ascii=False, indent=2)
        
        # æ‰“å°åˆ†æç»“æœ
        print_analysis_summary(analysis_result)
        
        print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜:")
        print(f"  - JSONæ•°æ®: {json_file}")
        print(f"  - å¯è§†åŒ–å›¾è¡¨: {viz_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_analysis_summary(result: Dict[str, Any]):
    """æ‰“å°åˆ†ææ‘˜è¦"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ ç¼ è®ºç»“æ„æ˜ å°„åˆ†ææŠ¥å‘Š")
    print("=" * 60)
    
    symbol = result['symbol']
    print(f"è‚¡ç¥¨ä»£ç : {symbol}")
    print(f"åˆ†ææ—¶é—´: {result['analysis_time']}")
    
    # æ•°æ®æ¦‚å†µ
    multi_data = result['multi_data']
    print(f"\nğŸ“Š æ•°æ®æ¦‚å†µ:")
    for level, data in multi_data.items():
        if not data.empty:
            print(f"  {level.value}: {len(data)} æ¡æ•°æ®")
    
    # ç»“æ„åˆ†æ
    level_structures = result['level_structures']
    print(f"\nğŸ—ï¸ ç»“æ„åˆ†æ:")
    for level, structure in level_structures.items():
        fenxing_count = len(structure['fenxing']['tops']) + len(structure['fenxing']['bottoms'])
        bi_count = len(structure['bi'])
        zs_count = len(structure['zhongshu'])
        print(f"  {level.value}: {fenxing_count}ä¸ªåˆ†å‹, {bi_count}æ¡ç¬”, {zs_count}ä¸ªä¸­æ¢")
    
    # æ˜ å°„å…³ç³»
    mappings = result['structure_mappings']
    print(f"\nğŸ”— ç»“æ„æ˜ å°„å…³ç³» ({len(mappings)}ä¸ª):")
    for mapping in mappings:
        print(f"  {mapping.higher_level.value} -> {mapping.lower_level.value}: "
              f"{mapping.mapping_type.value} (è´¨é‡: {mapping.quality.value}, "
              f"è¯„åˆ†: {mapping.overall_score:.2f})")
    
    # ä¸­æ¢ç»§æ‰¿
    inheritances = result['zhongshu_inheritances']
    valid_inheritances = [i for i in inheritances if i.is_valid_inheritance]
    print(f"\nğŸ—ï¸ ä¸­æ¢ç»§æ‰¿å…³ç³»: {len(valid_inheritances)}/{len(inheritances)} æœ‰æ•ˆ")
    for inheritance in valid_inheritances:
        print(f"  {inheritance.parent_level.value} -> {inheritance.child_level.value}: "
              f"{inheritance.inheritance_type} (é‡å åº¦: {inheritance.overlap_ratio:.2%})")
    
    # èƒŒç¦»åˆ†æ
    divergences = result['divergence_analysis']
    print(f"\nâš¡ èƒŒç¦»ä¿¡å·: {divergences['total_count']} ä¸ª")
    for divergence in divergences['divergences']:
        print(f"  {divergence['description']}")
    
    # è´¨é‡è¯„ä¼°
    quality = result['quality_assessment']
    print(f"\nğŸ“Š è´¨é‡è¯„ä¼°:")
    print(f"  æ˜ å°„è´¨é‡: {quality['overall_quality']:.2%}")
    print(f"  ç»§æ‰¿è´¨é‡: {quality['inheritance_quality']:.2%}")
    print(f"  ç»¼åˆè¯„åˆ†: {quality['comprehensive_score']:.2%}")
    print(f"  è¯„ä¼°ç»“è®º: {quality['assessment']}")
    
    # å…³é”®ç»“è®º
    conclusions = result['mapping_conclusions']
    print(f"\nğŸ’¡ å…³é”®ç»“è®º:")
    for finding in conclusions.get('key_findings', []):
        print(f"  â€¢ {finding}")
    
    print(f"\nğŸ¯ æ“ä½œå»ºè®®:")
    for rec in conclusions.get('recommendations', []):
        print(f"  â€¢ {rec}")


if __name__ == "__main__":
    success = run_structure_mapping_test()
    print(f"\n{'âœ… æµ‹è¯•æˆåŠŸ' if success else 'âŒ æµ‹è¯•å¤±è´¥'}")