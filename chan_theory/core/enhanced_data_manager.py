#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºå‹æ•°æ®ç®¡ç†å™¨
åŸºäºæŠ€æœ¯æ–¹æ¡ˆæ–‡æ¡£çš„æ•°æ®è®¿é—®å±‚ä¼˜åŒ–å®ç°
"""

import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging
from dataclasses import dataclass
import redis
import pickle
import hashlib
import json
from concurrent.futures import ThreadPoolExecutor
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
sys.path.append(project_root)

from database.db_handler import DBHandler
from chan_theory.models.chan_theory_models import TrendLevel

logger = logging.getLogger(__name__)


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    default_ttl: int = 3600  # 1å°æ—¶
    analysis_result_ttl: int = 7200  # 2å°æ—¶
    kline_data_ttl: int = 1800  # 30åˆ†é’Ÿ


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, config: CacheConfig = None):
        self.config = config or CacheConfig()
        self.redis_client = None
        self._init_redis()
    
    def _init_redis(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.redis_host,
                port=self.config.redis_port,
                db=self.config.redis_db,
                decode_responses=False  # ä¿æŒäºŒè¿›åˆ¶æ¨¡å¼ä»¥æ”¯æŒpickle
            )
            # æµ‹è¯•è¿æ¥
            self.redis_client.ping()
            logger.info("âœ… Redisç¼“å­˜è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
            self.redis_client = None
    
    def _generate_cache_key(self, prefix: str, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_parts = [prefix]
        for k, v in sorted(kwargs.items()):
            if isinstance(v, datetime):
                v = v.strftime('%Y%m%d')
            key_parts.append(f"{k}:{v}")
        
        key_string = ":".join(key_parts)
        # ä½¿ç”¨MD5å“ˆå¸Œé¿å…é”®è¿‡é•¿
        key_hash = hashlib.md5(key_string.encode()).hexdigest()
        return f"chan:{prefix}:{key_hash}"
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if not self.redis_client:
            return None
        
        try:
            data = self.redis_client.get(key)
            if data:
                return pickle.loads(data)
        except Exception as e:
            logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        
        return None
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        if not self.redis_client:
            return False
        
        try:
            ttl = ttl or self.config.default_ttl
            serialized_data = pickle.dumps(value)
            self.redis_client.setex(key, ttl, serialized_data)
            return True
        except Exception as e:
            logger.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
            return False
    
    async def delete(self, pattern: str = None) -> int:
        """åˆ é™¤ç¼“å­˜æ•°æ®"""
        if not self.redis_client:
            return 0
        
        try:
            if pattern:
                keys = self.redis_client.keys(pattern)
                if keys:
                    return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"ç¼“å­˜åˆ é™¤å¤±è´¥: {e}")
            return 0


class LocalDataFetcher:
    """æœ¬åœ°æ•°æ®è·å–å™¨"""
    
    def __init__(self, db_handler: DBHandler):
        self.db_handler = db_handler
        self.executor = ThreadPoolExecutor(max_workers=4)
    
    async def fetch_kline_data(self, symbol: str, period: str, 
                              start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ®"""
        try:
            # æ ¹æ®å‘¨æœŸé€‰æ‹©åˆé€‚çš„é›†åˆ
            collection_map = {
                '1min': 'stock_1min_qfq',
                '5min': 'stock_5min_qfq', 
                '30min': 'stock_30min_qfq',
                'daily': 'stock_daily_qfq'
            }
            
            collection_name = collection_map.get(period, 'stock_daily_qfq')
            
            # æ„å»ºæŸ¥è¯¢æ¡ä»¶
            query = {
                'ts_code': symbol,
                'trade_date': {
                    '$gte': start_date,
                    '$lte': end_date
                }
            }
            
            # å¼‚æ­¥æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
            loop = asyncio.get_event_loop()
            data = await loop.run_in_executor(
                self.executor,
                self._fetch_from_db,
                collection_name, query
            )
            
            if data:
                df = pd.DataFrame(data)
                df = self._process_kline_data(df)
                logger.info(f"ğŸ“Š è·å–{symbol} {period}æ•°æ®: {len(df)}æ¡")
                return df
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°{symbol} {period}æ•°æ®")
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"âŒ è·å–Kçº¿æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _fetch_from_db(self, collection_name: str, query: Dict) -> List[Dict]:
        """ä»æ•°æ®åº“è·å–æ•°æ®"""
        try:
            collection = self.db_handler.get_collection(collection_name)
            cursor = collection.find(query).sort('trade_date', 1)
            return list(cursor)
        except Exception as e:
            logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def _process_kline_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¤„ç†Kçº¿æ•°æ®"""
        if df.empty:
            return df
        
        # æ ‡å‡†åŒ–åˆ—å
        column_mapping = {
            'trade_date': 'timestamp',
            'open_qfq': 'open',
            'high_qfq': 'high', 
            'low_qfq': 'low',
            'close_qfq': 'close',
            'vol': 'volume'
        }
        
        # é‡å‘½ååˆ—
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                df = df.rename(columns={old_col: new_col})
        
        # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
        required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logger.warning(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return pd.DataFrame()
        
        # æ•°æ®ç±»å‹è½¬æ¢
        numeric_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in numeric_cols:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # å¤„ç†æ—¶é—´æˆ³
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df.set_index('timestamp', inplace=True)
        
        # åˆ é™¤æ— æ•ˆæ•°æ®
        df = df.dropna(subset=['open', 'high', 'low', 'close'])
        
        # æ•°æ®éªŒè¯
        df = df[(df['high'] >= df['low']) & 
                (df['high'] >= df['open']) & 
                (df['high'] >= df['close']) &
                (df['low'] <= df['open']) & 
                (df['low'] <= df['close'])]
        
        return df.sort_index()
    
    async def fetch_financial_data(self, symbol: str) -> Dict:
        """è·å–è´¢åŠ¡æ•°æ®"""
        try:
            query = {'ts_code': symbol}
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            basic_info = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                self._fetch_from_db,
                'stock_basic', query
            )
            
            # è·å–è´¢åŠ¡æŒ‡æ ‡
            financial_data = await asyncio.get_event_loop().run_in_executor(
                self.executor,
                self._fetch_from_db,
                'fina_indicator', query
            )
            
            return {
                'basic_info': basic_info[0] if basic_info else {},
                'financial_indicators': financial_data
            }
            
        except Exception as e:
            logger.error(f"è·å–è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
            return {}
    
    async def fetch_technical_indicators(self, symbol: str, period: str,
                                       start_date: datetime, end_date: datetime) -> Dict:
        """è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        try:
            # æ ¹æ®å‘¨æœŸé€‰æ‹©æŠ€æœ¯æŒ‡æ ‡é›†åˆ
            indicator_collections = {
                'daily': ['stock_daily_qfq'],  # æ—¥çº¿æŠ€æœ¯æŒ‡æ ‡å·²åŒ…å«åœ¨Kçº¿æ•°æ®ä¸­
                '30min': ['stock_30min_qfq'],
                '5min': ['stock_5min_qfq']
            }
            
            collections = indicator_collections.get(period, ['stock_daily_qfq'])
            indicators = {}
            
            for collection_name in collections:
                query = {
                    'ts_code': symbol,
                    'trade_date': {
                        '$gte': start_date,
                        '$lte': end_date
                    }
                }
                
                data = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self._fetch_from_db,
                    collection_name, query
                )
                
                if data:
                    df = pd.DataFrame(data)
                    # æå–æŠ€æœ¯æŒ‡æ ‡åˆ—
                    indicator_cols = [col for col in df.columns 
                                    if any(indicator in col.lower() 
                                          for indicator in ['ma', 'ema', 'boll', 'macd', 'rsi', 'kdj'])]
                    
                    if indicator_cols:
                        indicators[collection_name] = df[['trade_date'] + indicator_cols]
            
            return indicators
            
        except Exception as e:
            logger.error(f"è·å–æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
            return {}


class EnhancedDataManager:
    """å¢å¼ºå‹æ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, cache_config: CacheConfig = None):
        self.db_handler = DBHandler(local_priority=True)
        self.cache_manager = CacheManager(cache_config)
        self.data_fetcher = LocalDataFetcher(self.db_handler)
        
        logger.info("ğŸš€ å¢å¼ºå‹æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def get_kline_data(self, symbol: str, period: str, 
                           start_date: datetime, end_date: datetime) -> pd.DataFrame:
        """è·å–Kçº¿æ•°æ® - ä¼˜å…ˆæœ¬åœ°æ•°æ®åº“ï¼Œæ”¯æŒç¼“å­˜"""
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = self.cache_manager._generate_cache_key(
            "kline",
            symbol=symbol,
            period=period,
            start_date=start_date,
            end_date=end_date
        )
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data is not None:
            logger.info(f"ğŸ“‹ ä»ç¼“å­˜è·å–{symbol} {period}æ•°æ®")
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        df = await self.data_fetcher.fetch_kline_data(symbol, period, start_date, end_date)
        
        # ç¼“å­˜ç»“æœ
        if not df.empty:
            await self.cache_manager.set(
                cache_key, df, 
                ttl=self.cache_manager.config.kline_data_ttl
            )
        
        return df
    
    async def get_financial_data(self, symbol: str) -> Dict:
        """è·å–è´¢åŠ¡æ•°æ®"""
        cache_key = self.cache_manager._generate_cache_key("financial", symbol=symbol)
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        data = await self.data_fetcher.fetch_financial_data(symbol)
        
        # ç¼“å­˜ç»“æœ
        if data:
            await self.cache_manager.set(cache_key, data, ttl=86400)  # 24å°æ—¶ç¼“å­˜
        
        return data
    
    async def get_technical_indicators(self, symbol: str, period: str,
                                     start_date: datetime, end_date: datetime) -> Dict:
        """è·å–æŠ€æœ¯æŒ‡æ ‡æ•°æ®"""
        cache_key = self.cache_manager._generate_cache_key(
            "indicators",
            symbol=symbol,
            period=period,
            start_date=start_date,
            end_date=end_date
        )
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_data = await self.cache_manager.get(cache_key)
        if cached_data is not None:
            return cached_data
        
        # ä»æ•°æ®åº“è·å–
        data = await self.data_fetcher.fetch_technical_indicators(symbol, period, start_date, end_date)
        
        # ç¼“å­˜ç»“æœ
        if data:
            await self.cache_manager.set(cache_key, data, ttl=3600)  # 1å°æ—¶ç¼“å­˜
        
        return data
    
    async def cache_analysis_result(self, symbol: str, timeframe: str, 
                                  analysis_date: datetime, result: Dict) -> bool:
        """ç¼“å­˜åˆ†æç»“æœ"""
        cache_key = self.cache_manager._generate_cache_key(
            "analysis",
            symbol=symbol,
            timeframe=timeframe,
            date=analysis_date
        )
        
        return await self.cache_manager.set(
            cache_key, result,
            ttl=self.cache_manager.config.analysis_result_ttl
        )
    
    async def get_cached_analysis_result(self, symbol: str, timeframe: str,
                                       analysis_date: datetime) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        cache_key = self.cache_manager._generate_cache_key(
            "analysis",
            symbol=symbol,
            timeframe=timeframe,
            date=analysis_date
        )
        
        return await self.cache_manager.get(cache_key)
    
    async def batch_get_kline_data(self, requests: List[Dict]) -> Dict[str, pd.DataFrame]:
        """æ‰¹é‡è·å–Kçº¿æ•°æ®"""
        results = {}
        
        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for req in requests:
            task = self.get_kline_data(
                req['symbol'], req['period'],
                req['start_date'], req['end_date']
            )
            tasks.append((req['symbol'], req['period'], task))
        
        # å¹¶è¡Œæ‰§è¡Œ
        for symbol, period, task in tasks:
            try:
                df = await task
                key = f"{symbol}_{period}"
                results[key] = df
            except Exception as e:
                logger.error(f"æ‰¹é‡è·å–{symbol} {period}æ•°æ®å¤±è´¥: {e}")
                results[f"{symbol}_{period}"] = pd.DataFrame()
        
        return results
    
    async def clear_cache(self, pattern: str = None) -> int:
        """æ¸…ç†ç¼“å­˜"""
        if pattern:
            cache_pattern = f"chan:{pattern}:*"
        else:
            cache_pattern = "chan:*"
        
        deleted_count = await self.cache_manager.delete(cache_pattern)
        logger.info(f"ğŸ—‘ï¸ æ¸…ç†ç¼“å­˜: {deleted_count}ä¸ªé”®")
        return deleted_count
    
    async def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.cache_manager.redis_client:
            return {"status": "Redisæœªè¿æ¥"}
        
        try:
            info = self.cache_manager.redis_client.info()
            return {
                "connected_clients": info.get("connected_clients", 0),
                "used_memory_human": info.get("used_memory_human", "0B"),
                "keyspace_hits": info.get("keyspace_hits", 0),
                "keyspace_misses": info.get("keyspace_misses", 0),
                "hit_rate": info.get("keyspace_hits", 0) / 
                          max(info.get("keyspace_hits", 0) + info.get("keyspace_misses", 0), 1)
            }
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self.data_fetcher, 'executor'):
            self.data_fetcher.executor.shutdown(wait=False)