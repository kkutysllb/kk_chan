#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼ è®ºåˆ†ææ•°æ®API v2
åŸºäºæœ€æ–°çš„ç¼ è®ºv2æ ¸å¿ƒæ¨¡å—ï¼ˆå½¢æ€å­¦+åŠ¨åŠ›å­¦ï¼‰ï¼Œæä¾›å®Œæ•´çš„ç¼ è®ºåˆ†ææœåŠ¡
æ”¯æŒå¤šçº§åˆ«åˆ†æã€åŠ¨åŠ›å­¦åˆ†æã€ä¹°å–ç‚¹è¯†åˆ«ç­‰é«˜çº§åŠŸèƒ½
"""

import sys
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# å¯¼å…¥ç¼ è®ºv2æ ¸å¿ƒç»„ä»¶
from chan_theory_v2.core.chan_engine import ChanEngine, ChanAnalysisResult, AnalysisLevel, quick_analyze, multi_level_analyze
from chan_theory_v2.models.enums import TimeLevel, BiDirection, SegDirection, ZhongShuType
from chan_theory_v2.models.dynamics import BuySellPointType, BackChi, DynamicsConfig
from chan_theory_v2.config.chan_config import ChanConfig
from chan_theory_v2.strategies.backchi_stock_selector import SimpleBackchiStockSelector
from database.db_handler import get_db_handler

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChanDataAPIv2:
    """ç¼ è®ºæ•°æ®API v2 - åŸºäºæœ€æ–°ç¼ è®ºå¼•æ“çš„å®Œæ•´åˆ†ææœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–API"""
        self.db_handler = get_db_handler()
        self.db = self.db_handler.db
        
        # åˆå§‹åŒ–ç¼ è®ºå¼•æ“
        self.chan_engine = ChanEngine()
        
        # åˆå§‹åŒ–é€‰è‚¡å™¨
        self.stock_selector = SimpleBackchiStockSelector()
        
        logger.info("ğŸš€ ç¼ è®ºæ•°æ®API v2åˆå§‹åŒ–å®Œæˆ")
    
    def get_symbols_list(self, query: str = "") -> List[Dict[str, str]]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            collection = self.db["infrastructure_stock_basic"]
            
            # æ„å»ºæœç´¢æ¡ä»¶
            search_filter = {"ts_code": {"$exists": True}, "name": {"$exists": True}}
            
            if query and query.strip():
                # æ”¯æŒæŒ‰è‚¡ç¥¨ä»£ç æˆ–åç§°æœç´¢
                query = query.strip().upper()
                search_filter["$or"] = [
                    {"ts_code": {"$regex": query, "$options": "i"}},
                    {"name": {"$regex": query, "$options": "i"}}
                ]
                logger.info(f"ğŸ” æœç´¢è‚¡ç¥¨: {query}")
            
            cursor = collection.find(
                search_filter,
                {"ts_code": 1, "name": 1, "_id": 0}
            ).limit(100)  # æœç´¢æ—¶é™åˆ¶100ä¸ªç»“æœ
            
            stocks = []
            for doc in cursor:
                stocks.append({
                    "value": doc.get("ts_code", ""),
                    "label": f"{doc.get('ts_code', '')} - {doc.get('name', '')}",
                    "name": doc.get("name", "")
                })
            
            logger.info(f"ğŸ“‹ è·å–åˆ° {len(stocks)} ä¸ªè‚¡ç¥¨{'ï¼ˆæœç´¢ç»“æœï¼‰' if query else ''}")
            return stocks
            
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def analyze_symbol_complete(self, 
                              symbol: str, 
                              timeframe: str = "daily", 
                              days: int = 90,
                              analysis_level: str = "complete") -> Dict[str, Any]:
        """
        å®Œæ•´çš„ç¼ è®ºåˆ†æ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´çº§åˆ« ("5min", "30min", "daily")
            days: åˆ†æå¤©æ•°
            analysis_level: åˆ†æçº§åˆ« ("basic", "standard", "advanced", "complete")
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹ç¼ è®ºv2åˆ†æ {symbol} ({timeframe}, {days}å¤©, {analysis_level}çº§åˆ«)")
            
            # è·å–æ•°æ®
            time_level = self._get_time_level(timeframe)
            data = self._fetch_stock_data(symbol, time_level, days)
            
            if not data:
                logger.warning(f"âš ï¸ æ— æ³•è·å– {symbol} çš„æ•°æ®")
                return self._generate_empty_result(symbol, timeframe)
            
            # è®¾ç½®åˆ†æçº§åˆ«
            level_mapping = {
                "basic": AnalysisLevel.BASIC,
                "standard": AnalysisLevel.STANDARD,
                "advanced": AnalysisLevel.ADVANCED,
                "complete": AnalysisLevel.COMPLETE
            }
            analysis_level_enum = level_mapping.get(analysis_level, AnalysisLevel.COMPLETE)
            
            # æ‰§è¡Œç¼ è®ºåˆ†æ
            result = self.chan_engine.analyze(
                data=data,
                symbol=symbol,
                time_level=time_level,
                analysis_level=analysis_level_enum
            )
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ‡å‡†æ ¼å¼
            frontend_data = self._convert_to_frontend_format(result, timeframe, days)
            
            logger.info(f"âœ… {symbol} ç¼ è®ºv2åˆ†æå®Œæˆ")
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æ {symbol} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_empty_result(symbol, timeframe)
    
    def analyze_multi_level(self, 
                          symbol: str, 
                          levels: List[str] = ["daily", "30min", "5min"],
                          days: int = 90) -> Dict[str, Any]:
        """
        å¤šçº§åˆ«ç¼ è®ºåˆ†æ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            levels: åˆ†æçº§åˆ«åˆ—è¡¨
            days: åˆ†æå¤©æ•°
            
        Returns:
            å¤šçº§åˆ«åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹å¤šçº§åˆ«ç¼ è®ºåˆ†æ {symbol} ({levels}, {days}å¤©)")
            
            # å‡†å¤‡å¤šçº§åˆ«æ•°æ®
            level_data = {}
            for level_str in levels:
                time_level = self._get_time_level(level_str)
                data = self._fetch_stock_data(symbol, time_level, days)
                if data:
                    level_data[time_level] = data
                    logger.info(f"âœ… {level_str}æ•°æ®: {len(data)} æ¡")
                else:
                    logger.warning(f"âš ï¸ æ— æ³•è·å–{level_str}æ•°æ®")
            
            if not level_data:
                logger.warning(f"âš ï¸ æ— ä»»ä½•çº§åˆ«æ•°æ®å¯ç”¨äº {symbol}")
                return self._generate_empty_multi_level_result(symbol, levels)
            
            # æ‰§è¡Œå¤šçº§åˆ«åˆ†æ
            results = self.chan_engine.analyze_multi_level(level_data, symbol)
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
            frontend_data = self._convert_multi_level_to_frontend(results, symbol, levels, days)
            
            logger.info(f"âœ… {symbol} å¤šçº§åˆ«åˆ†æå®Œæˆï¼Œå…±{len(results)}ä¸ªçº§åˆ«")
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ å¤šçº§åˆ«åˆ†æ {symbol} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_empty_multi_level_result(symbol, levels)
    
    def get_trading_signals(self, symbol: str, timeframe: str = "daily", days: int = 30) -> Dict[str, Any]:
        """
        è·å–äº¤æ˜“ä¿¡å·
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´çº§åˆ«
            days: åˆ†æå¤©æ•°
            
        Returns:
            äº¤æ˜“ä¿¡å·æ•°æ®
        """
        try:
            logger.info(f"ğŸ¯ è·å–äº¤æ˜“ä¿¡å· {symbol} ({timeframe}, {days}å¤©)")
            
            # æ‰§è¡Œæ ‡å‡†åˆ†æï¼ˆåŒ…å«åŠ¨åŠ›å­¦ï¼‰
            time_level = self._get_time_level(timeframe)
            data = self._fetch_stock_data(symbol, time_level, days)
            
            if not data:
                return {"signals": [], "summary": {"total": 0, "buy": 0, "sell": 0}}
            
            result = self.chan_engine.analyze(
                data=data,
                symbol=symbol,
                time_level=time_level,
                analysis_level=AnalysisLevel.STANDARD
            )
            
            # è·å–äº¤æ˜“ä¿¡å·
            signals = self.chan_engine.get_trading_signals(result)
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
            frontend_signals = self._convert_signals_to_frontend(signals)
            
            logger.info(f"âœ… è·å–åˆ° {len(signals['signals'])} ä¸ªäº¤æ˜“ä¿¡å·")
            return frontend_signals
            
        except Exception as e:
            logger.error(f"âŒ è·å–äº¤æ˜“ä¿¡å·å¤±è´¥: {e}")
            return {"signals": [], "summary": {"total": 0, "buy": 0, "sell": 0}}
    
    def _fetch_stock_data(self, symbol: str, time_level: TimeLevel, days: int) -> List[Dict]:
        """è·å–è‚¡ç¥¨æ•°æ®"""
        try:
            # æ ¹æ®æ—¶é—´çº§åˆ«é€‰æ‹©é›†åˆ
            collection_mapping = {
                TimeLevel.MIN_5: "stock_kline_5min",
                TimeLevel.MIN_30: "stock_kline_30min", 
                TimeLevel.DAILY: "stock_kline_daily"
            }
            
            collection_name = collection_mapping.get(time_level, "stock_kline_daily")
            collection = self.db[collection_name]
            
            # æ„å»ºæŸ¥è¯¢
            query = {"ts_code": symbol}
            
            # è®¡ç®—æ•°æ®é‡ï¼ˆæ ¹æ®çº§åˆ«è°ƒæ•´ï¼‰
            if time_level == TimeLevel.MIN_5:
                limit = days * 48  # 5åˆ†é’Ÿ: æ¯å¤©çº¦48ä¸ªæ•°æ®ç‚¹
            elif time_level == TimeLevel.MIN_30:
                limit = days * 8   # 30åˆ†é’Ÿ: æ¯å¤©çº¦8ä¸ªæ•°æ®ç‚¹
            else:
                limit = days       # æ—¥çº¿: æ¯å¤©1ä¸ªæ•°æ®ç‚¹
            
            # è·å–æ•°æ®
            if time_level == TimeLevel.DAILY:
                # ä½¿ç”¨äº¤æ˜“æ—¥å†è·å–äº¤æ˜“æ—¥èŒƒå›´
                from datetime import datetime, timedelta
                from chan_theory_v2.core import get_trading_dates, get_nearest_trading_date
                
                # è·å–å½“å‰æ—¥æœŸçš„æœ€è¿‘äº¤æ˜“æ—¥ä½œä¸ºç»“æŸæ—¥æœŸ
                end_date = get_nearest_trading_date(datetime.now(), direction='backward')
                if not end_date:
                    end_date = datetime.now()
                    
                # è·å–æŒ‡å®šå¤©æ•°èŒƒå›´å†…çš„æ‰€æœ‰äº¤æ˜“æ—¥
                trading_dates = get_trading_dates(end_date - timedelta(days=days*2), end_date)
                
                # å¦‚æœäº¤æ˜“æ—¥æ•°é‡ä¸è¶³ï¼Œåˆ™æ‰©å¤§èŒƒå›´å†æ¬¡æŸ¥è¯¢
                if len(trading_dates) < days:
                    trading_dates = get_trading_dates(end_date - timedelta(days=days*3), end_date)
                
                # å–æœ€è¿‘çš„daysä¸ªäº¤æ˜“æ—¥
                trading_dates = trading_dates[-days:] if len(trading_dates) >= days else trading_dates
                
                if trading_dates:
                    # è®¾ç½®æŸ¥è¯¢çš„èµ·æ­¢æ—¥æœŸ
                    start_date = trading_dates[0]
                    # å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸æ•°æ®åº“ä¸­çš„æ ¼å¼åŒ¹é…
                    end_date_str = end_date.strftime('%Y%m%d')
                    start_date_str = start_date.strftime('%Y%m%d')
                    
                    # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸè¿›è¡ŒæŸ¥è¯¢ï¼Œä¸æ•°æ®åº“ä¸­çš„æ ¼å¼åŒ¹é…
                    query.update({
                        'trade_date': {
                            '$gte': start_date_str,
                            '$lte': end_date_str
                        }
                    })
                    
                    logger.info(f"ğŸ“… æ—¥KæŸ¥è¯¢èŒƒå›´: {start_date_str} è‡³ {end_date_str} (äº¤æ˜“æ—¥æ€»æ•°: {len(trading_dates)})")                
                else:
                    # å¦‚æœæ— æ³•è·å–äº¤æ˜“æ—¥ï¼Œåˆ™ä½¿ç”¨è‡ªç„¶æ—¥ä½œä¸ºå¤‡é€‰
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=days)
                    
                    # å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                    end_date_str = end_date.strftime('%Y%m%d')
                    start_date_str = start_date.strftime('%Y%m%d')
                    
                    query.update({
                        'trade_date': {
                            '$gte': start_date_str,
                            '$lte': end_date_str
                        }
                    })
                    
                    logger.info(f"ğŸ“… æ—¥KæŸ¥è¯¢èŒƒå›´(è‡ªç„¶æ—¥): {start_date_str} è‡³ {end_date_str}")
                
                # æŒ‰æ—¥æœŸå‡åºæ’åºï¼Œè·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                cursor = collection.find(query).sort("trade_date", 1)
            else:
                cursor = collection.find(query).sort("trade_time", -1).limit(limit)
                
            raw_data = list(cursor)
            
            if time_level != TimeLevel.DAILY:
                raw_data.reverse()  # åˆ†é’Ÿæ•°æ®è½¬ä¸ºå‡åº
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            converted_data = self._convert_data_format(raw_data, time_level)
            
            # è®°å½•æ•°æ®æ—¥æœŸèŒƒå›´
            if converted_data:
                start_date = converted_data[0]['timestamp'].strftime('%Y-%m-%d')
                end_date = converted_data[-1]['timestamp'].strftime('%Y-%m-%d')
                logger.info(f"ğŸ“Š è·å– {symbol} {time_level.value} æ•°æ®: {len(converted_data)} æ¡, æ—¥æœŸèŒƒå›´: {start_date} è‡³ {end_date}")
            else:
                logger.warning(f"âš ï¸ è·å– {symbol} {time_level.value} æ•°æ®: 0 æ¡")
                
            return converted_data
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _convert_data_format(self, raw_data: List[Dict], time_level: TimeLevel) -> List[Dict]:
        """è½¬æ¢æ•°æ®æ ¼å¼"""
        converted_data = []
        
        for item in raw_data:
            try:
                # å¤„ç†æ—¶é—´æˆ³
                if time_level == TimeLevel.DAILY:
                    trade_date_str = str(item['trade_date'])
                    timestamp = datetime.strptime(trade_date_str, '%Y%m%d')
                else:
                    trade_time_str = str(item['trade_time'])
                    timestamp = datetime.strptime(trade_time_str, '%Y-%m-%d %H:%M:%S')
                
                converted_item = {
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item.get('vol', item.get('volume', 1)))),
                    'amount': float(item.get('amount', 0)),
                    'symbol': item['ts_code']
                }
                
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if all(converted_item[k] > 0 for k in ['open', 'high', 'low', 'close']):
                    converted_data.append(converted_item)
                    
            except Exception as e:
                logger.warning(f"è½¬æ¢æ•°æ®ç‚¹å¤±è´¥: {e}")
                continue
        
        return converted_data
    
    def _convert_to_frontend_format(self, result: ChanAnalysisResult, timeframe: str, days: int) -> Dict[str, Any]:
        """è½¬æ¢åˆ†æç»“æœä¸ºå‰ç«¯æ ¼å¼"""
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = result.get_statistics()
        
        # 1. Kçº¿æ•°æ®
        kline_data = self._convert_klines_to_echarts(result.processed_klines)
        
        # 2. åˆ†å‹æ•°æ®
        fenxing_data = self._convert_fenxings_to_echarts(result.fenxings)
        
        # 3. ç¬”æ•°æ®
        bi_data = self._convert_bis_to_echarts(result.bis)
        
        # 4. çº¿æ®µæ•°æ®
        seg_data = self._convert_segs_to_echarts(result.segs)
        
        # 5. ä¸­æ¢æ•°æ®
        zhongshu_data = self._convert_zhongshus_to_echarts(result.zhongshus)
        
        # 6. ä¹°å–ç‚¹æ•°æ®
        signal_data = self._convert_buy_sell_points_to_echarts(result.buy_sell_points)
        
        # 7. èƒŒé©°åˆ†ææ•°æ®
        backchi_data = self._convert_backchi_to_echarts(result.backchi_analyses)
        
        # 8. MACDæ•°æ®ï¼ˆåŸºäºåŸå§‹Kçº¿è®¡ç®—ï¼‰
        categories = kline_data.get("categories", [])
        logger.info(f"MACDè®¡ç®—å‰: åŸå§‹Kçº¿æ•°é‡={len(result.klines)}, å¤„ç†åKçº¿æ•°é‡={len(result.processed_klines)}, categoriesé•¿åº¦={len(categories)}")
        
        # è®°å½•åŸå§‹Kçº¿å’Œå¤„ç†åKçº¿çš„æ—¶é—´èŒƒå›´ï¼Œä»¥ä¾¿äºè°ƒè¯•
        if len(result.klines) > 0 and len(result.processed_klines) > 0:
            logger.info(f"åŸå§‹Kçº¿æ—¶é—´èŒƒå›´: {result.klines[0].timestamp} è‡³ {result.klines[-1].timestamp}")
            logger.info(f"å¤„ç†åKçº¿æ—¶é—´èŒƒå›´: {result.processed_klines[0].timestamp} è‡³ {result.processed_klines[-1].timestamp}")
        
        macd_data = self._calculate_macd_from_klines(result.klines, categories)
        
        # æ„å»ºå®Œæ•´çš„å‰ç«¯æ•°æ®ç»“æ„
        frontend_data = {
            # åŸºç¡€å…ƒä¿¡æ¯
            "meta": {
                "symbol": result.symbol,
                "timeframe": timeframe,
                "analysis_level": result.analysis_level.value,
                "analysis_time": result.analysis_time.isoformat(),
                "data_range": {
                    "days": days,
                    "start_date": result.processed_klines[0].timestamp.isoformat() if result.processed_klines else (datetime.now() - timedelta(days=days)).isoformat(),
                    "end_date": result.processed_klines[-1].timestamp.isoformat() if result.processed_klines else datetime.now().isoformat()
                },
                "data_count": stats['processed_klines_count']
            },
            
            # å›¾è¡¨æ•°æ® - EChartsæ ‡å‡†æ ¼å¼
            "chart_data": {
                # Kçº¿æ•°æ®
                "kline": kline_data,
                
                # æŠ€æœ¯æŒ‡æ ‡
                "indicators": {
                    "macd": macd_data
                },
                
                # ç¼ è®ºç»“æ„
                "chan_structures": {
                    "fenxing": fenxing_data,    # åˆ†å‹ç‚¹
                    "bi": bi_data,              # ç¬”
                    "seg": seg_data,            # çº¿æ®µ
                    "zhongshu": zhongshu_data   # ä¸­æ¢
                },
                
                # åŠ¨åŠ›å­¦åˆ†æ
                "dynamics": {
                    "buy_sell_points": signal_data,  # ä¹°å–ç‚¹
                    "backchi": backchi_data          # èƒŒé©°åˆ†æ
                }
            },
            
            # åˆ†ææ‘˜è¦
            "analysis": {
                "summary": {
                    "klines_original": stats['klines_count'],
                    "klines_processed": stats['processed_klines_count'],
                    "fenxing_count": stats['fenxings_count'],
                    "bi_count": stats['bis_count'],
                    "seg_count": stats['segs_count'],
                    "zhongshu_count": stats['zhongshus_count'],
                    "backchi_count": stats['backchi_count'],
                    "buy_sell_points_count": stats['buy_sell_points_count'],
                    "buy_points_count": stats['buy_points_count'],
                    "sell_points_count": stats['sell_points_count']
                },
                
                # ç»¼åˆè¯„ä¼°
                "evaluation": {
                    "trend_direction": result.trend_direction,
                    "trend_strength": result.trend_strength,
                    "risk_level": result.risk_level,
                    "confidence_score": result.confidence_score,
                    "recommended_action": result.recommended_action,
                    "entry_price": result.entry_price,
                    "stop_loss": result.stop_loss,
                    "take_profit": result.take_profit
                },
                
                # æœ€æ–°ä¿¡å·
                "latest_signals": [
                    {
                        "type": str(point.point_type),
                        "price": point.price,
                        "timestamp": point.timestamp.isoformat(),
                        "reliability": point.reliability,
                        "strength": point.strength
                    }
                    for point in result.get_latest_signals(5)
                ]
            },
            
            # å›¾è¡¨é…ç½®
            "chart_config": {
                "colors": {
                    "up_candle": "#ef5150",      # çº¢è‰²Kçº¿
                    "down_candle": "#26a69a",    # ç»¿è‰²Kçº¿
                    "fenxing_top": "#f44336",    # é¡¶åˆ†å‹-çº¢è‰²
                    "fenxing_bottom": "#4caf50", # åº•åˆ†å‹-ç»¿è‰²
                    "bi_up": "#e91e63",          # ä¸Šç¬”-ç²‰è‰²
                    "bi_down": "#2196f3",        # ä¸‹ç¬”-è“è‰²
                    "seg_up": "#ff5722",         # ä¸Šçº¿æ®µ-æ©™çº¢è‰²
                    "seg_down": "#3f51b5",       # ä¸‹çº¿æ®µ-é›è“è‰²
                    "zhongshu": "#ff9800",       # ä¸­æ¢-æ©™è‰²
                    "buy_point": "#4caf50",      # ä¹°ç‚¹-ç»¿è‰²
                    "sell_point": "#f44336",     # å–ç‚¹-çº¢è‰²
                    "backchi": "#9c27b0"         # èƒŒé©°-ç´«è‰²
                }
            }
        }
        
        return frontend_data
    
    def _convert_klines_to_echarts(self, klines) -> Dict[str, Any]:
        """è½¬æ¢Kçº¿æ•°æ®ä¸ºEChartsæ ¼å¼"""
        if not klines or len(klines) == 0:
            return {"categories": [], "values": [], "volumes": []}
        
        categories = []  # æ—¶é—´è½´
        values = []     # Kçº¿æ•°æ® [open, close, low, high]
        volumes = []    # æˆäº¤é‡
        
        for kline in klines:
            try:
                # æ ¼å¼åŒ–æ—¶é—´
                if hasattr(kline.timestamp, 'strftime'):
                    timestamp = kline.timestamp.strftime('%Y-%m-%d %H:%M')
                else:
                    timestamp = str(kline.timestamp)
                
                categories.append(timestamp)
                
                # Kçº¿æ•°æ® [open, close, low, high]
                values.append([
                    float(kline.open),
                    float(kline.close),
                    float(kline.low),
                    float(kline.high)
                ])
                
                volumes.append(float(kline.volume))
                
            except Exception as e:
                logger.warning(f"è½¬æ¢Kçº¿æ•°æ®ç‚¹å¤±è´¥: {e}")
                continue
        
        return {
            "categories": categories,  # Xè½´æ—¶é—´
            "values": values,         # Kçº¿æ•°æ®
            "volumes": volumes        # æˆäº¤é‡
        }
    
    def _convert_fenxings_to_echarts(self, fenxings) -> List[Dict[str, Any]]:
        """è½¬æ¢åˆ†å‹æ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not fenxings or len(fenxings) == 0:
            return []
        
        fenxing_points = []
        
        for fenxing in fenxings:
            try:
                timestamp = fenxing.timestamp.strftime('%Y-%m-%d %H:%M')
                
                fenxing_points.append({
                    "name": f"{'é¡¶' if fenxing.is_top else 'åº•'}åˆ†å‹",
                    "coord": [timestamp, float(fenxing.price)],
                    "value": float(fenxing.price),
                    "type": "top" if fenxing.is_top else "bottom",
                    "direction": "up" if fenxing.is_top else "down",
                    "strength": float(fenxing.strength),
                    "confirmed": bool(fenxing.is_confirmed),
                    "symbol": "triangle",
                    "symbolSize": 8,
                    "itemStyle": {
                        "color": "#f44336" if fenxing.is_top else "#4caf50"
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢åˆ†å‹æ•°æ®å¤±è´¥: {e}")
                continue
        
        return fenxing_points
    
    def _convert_bis_to_echarts(self, bis) -> List[Dict[str, Any]]:
        """è½¬æ¢ç¬”æ•°æ®ä¸ºEChartsçº¿æ¡æ ¼å¼"""
        if not bis or len(bis) == 0:
            return []
        
        bi_lines = []
        
        for idx, bi in enumerate(bis):
            try:
                start_time = bi.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = bi.end_time.strftime('%Y-%m-%d %H:%M')
                
                bi_lines.append({
                    "id": f"bi_{idx}",
                    "name": f"{'ä¸Š' if bi.is_up else 'ä¸‹'}ç¬”",
                    "coords": [
                        [start_time, float(bi.start_price)],
                        [end_time, float(bi.end_price)]
                    ],
                    "direction": bi.direction.value,
                    "amplitude": float(bi.amplitude),
                    "strength": float(bi.strength),
                    "duration": bi.duration,
                    "lineStyle": {
                        "color": "#e91e63" if bi.is_up else "#2196f3",
                        "width": 2
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ç¬”æ•°æ®å¤±è´¥: {e}")
                continue
        
        return bi_lines
    
    def _convert_segs_to_echarts(self, segs) -> List[Dict[str, Any]]:
        """è½¬æ¢çº¿æ®µæ•°æ®ä¸ºEChartsçº¿æ¡æ ¼å¼"""
        if not segs or len(segs) == 0:
            return []
        
        seg_lines = []
        
        for idx, seg in enumerate(segs):
            try:
                start_time = seg.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = seg.end_time.strftime('%Y-%m-%d %H:%M')
                
                seg_lines.append({
                    "id": f"seg_{idx}",
                    "name": f"{'ä¸Š' if seg.is_up else 'ä¸‹'}çº¿æ®µ",
                    "coords": [
                        [start_time, float(seg.start_price)],
                        [end_time, float(seg.end_price)]
                    ],
                    "direction": seg.direction.value,
                    "amplitude": float(seg.amplitude),
                    "strength": float(seg.strength),
                    "integrity": float(seg.integrity),
                    "bi_count": seg.bi_count,
                    "duration": seg.duration,
                    "lineStyle": {
                        "color": "#ff5722" if seg.is_up else "#3f51b5",
                        "width": 3
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢çº¿æ®µæ•°æ®å¤±è´¥: {e}")
                continue
        
        return seg_lines
    
    def _convert_zhongshus_to_echarts(self, zhongshus) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸­æ¢æ•°æ®ä¸ºEChartsåŒºåŸŸæ ¼å¼"""
        if not zhongshus or len(zhongshus) == 0:
            return []
        
        zhongshu_areas = []
        
        for idx, zs in enumerate(zhongshus):
            try:
                start_time = zs.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = zs.end_time.strftime('%Y-%m-%d %H:%M')
                
                zhongshu_areas.append({
                    "id": f"zs_{idx}",
                    "name": f"ä¸­æ¢{idx+1}",
                    "coords": [
                        [start_time, float(zs.low)],    # å·¦ä¸‹è§’
                        [end_time, float(zs.high)]      # å³ä¸Šè§’
                    ],
                    "high": float(zs.high),
                    "low": float(zs.low),
                    "center": float(zs.center),
                    "strength": float(zs.strength),
                    "stability": float(zs.stability),
                    "seg_count": zs.seg_count,
                    "extend_count": zs.extend_count,
                    "is_active": zs.is_active,
                    "itemStyle": {
                        "color": "rgba(255, 152, 0, 0.2)",  # åŠé€æ˜æ©™è‰²
                        "borderColor": "#ff9800",
                        "borderWidth": 1
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¸­æ¢æ•°æ®å¤±è´¥: {e}")
                continue
        
        return zhongshu_areas
    
    def _convert_buy_sell_points_to_echarts(self, buy_sell_points) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¹°å–ç‚¹æ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not buy_sell_points:
            return []
        
        signal_marks = []
        
        for idx, point in enumerate(buy_sell_points):
            try:
                timestamp = point.timestamp.strftime('%Y-%m-%d %H:%M')
                
                signal_marks.append({
                    "id": f"signal_{idx}",
                    "name": str(point.point_type),
                    "coord": [timestamp, float(point.price)],
                    "value": float(point.price),
                    "type": "buy" if point.point_type.is_buy() else "sell",
                    "point_level": point.point_type.get_level(),  # 1, 2, 3ç±»
                    "strength": float(point.strength),
                    "reliability": float(point.reliability),
                    "confirmed_higher": point.confirmed_by_higher_level,
                    "confirmed_lower": point.confirmed_by_lower_level,
                    "symbol": "arrow",
                    "symbolSize": 12,
                    "symbolRotate": 90 if point.point_type.is_buy() else -90,
                    "itemStyle": {
                        "color": "#4caf50" if point.point_type.is_buy() else "#f44336"
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¹°å–ç‚¹æ•°æ®å¤±è´¥: {e}")
                continue
        
        return signal_marks
    
    def _convert_backchi_to_echarts(self, backchi_analyses) -> List[Dict[str, Any]]:
        """è½¬æ¢èƒŒé©°åˆ†ææ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not backchi_analyses:
            return []
        
        backchi_marks = []
        
        for idx, backchi in enumerate(backchi_analyses):
            try:
                if not backchi.is_valid_backchi():
                    continue
                
                timestamp = backchi.current_seg.end_time.strftime('%Y-%m-%d %H:%M')
                
                backchi_marks.append({
                    "id": f"backchi_{idx}",
                    "name": str(backchi.backchi_type),
                    "coord": [timestamp, float(backchi.current_seg.end_price)],
                    "value": float(backchi.current_seg.end_price),
                    "type": "top" if backchi.backchi_type == BackChi.TOP_BACKCHI else "bottom",
                    "reliability": float(backchi.reliability),
                    "strength_ratio": float(backchi.strength_ratio),
                    "macd_divergence": float(backchi.macd_divergence),
                    "symbol": "diamond",
                    "symbolSize": 10,
                    "itemStyle": {
                        "color": "#9c27b0"  # ç´«è‰²
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢èƒŒé©°æ•°æ®å¤±è´¥: {e}")
                continue
        
        return backchi_marks
    
    def _calculate_macd_from_klines(self, klines, categories: List[str]) -> Dict[str, List]:
        """åŸºäºKçº¿æ•°æ®è®¡ç®—MACDæŒ‡æ ‡"""
        try:
            if not klines or len(klines) == 0 or len(categories) == 0:
                return {"dif": [], "dea": [], "macd": []}
            
            # æå–æ”¶ç›˜ä»·
            close_prices = [float(kline.close) for kline in klines]
            
            # æ£€æŸ¥Kçº¿æ•°é‡ä¸categoriesé•¿åº¦æ˜¯å¦ä¸€è‡´
            if len(klines) != len(categories):
                logger.warning(f"MACDè®¡ç®—è­¦å‘Š: åŸå§‹Kçº¿æ•°é‡({len(klines)})ä¸categoriesé•¿åº¦({len(categories)})ä¸ä¸€è‡´")
                logger.info("è¿™æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºcategoriesæ˜¯åŸºäºå¤„ç†åçš„Kçº¿ç”Ÿæˆçš„ï¼Œè€ŒMACDæ˜¯åŸºäºåŸå§‹Kçº¿è®¡ç®—çš„")
            
            if len(close_prices) < 26:  # MACDéœ€è¦è‡³å°‘26ä¸ªæ•°æ®ç‚¹
                return {"dif": [], "dea": [], "macd": []}
            
            # MACDå‚æ•°
            fast_period = 12
            slow_period = 26
            signal_period = 9
            
            # è®¡ç®—EMA
            def calculate_ema(values, period):
                multiplier = 2.0 / (period + 1)
                ema = [sum(values[:period]) / period]  # åˆå§‹å€¼ä½¿ç”¨ç®€å•å¹³å‡
                
                for i in range(period, len(values)):
                    ema.append((values[i] * multiplier) + (ema[-1] * (1 - multiplier)))
                
                return [0.0] * (period - 1) + ema
            
            # è®¡ç®—EMA12å’ŒEMA26
            ema12 = calculate_ema(close_prices, fast_period)
            ema26 = calculate_ema(close_prices, slow_period)
            
            # è®¡ç®—DIFçº¿
            dif = [ema12[i] - ema26[i] for i in range(len(ema12))]
            
            # è®¡ç®—DEAçº¿
            dea = calculate_ema(dif[slow_period-1:], signal_period)
            dea = [0.0] * (slow_period - 1) + dea
            
            # è®¡ç®—MACDæŸ±
            macd = [(dif[i] - dea[i]) * 2 for i in range(len(dif))]
            
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸Kçº¿æ•°é‡ä¸€è‡´
            if len(categories) != len(klines):
                logger.warning(f"MACDè®¡ç®—è­¦å‘Š: categoriesé•¿åº¦({len(categories)})ä¸Kçº¿æ•°é‡({len(klines)})ä¸ä¸€è‡´")
                
            # ä¿ç•™ç²¾åº¦å¹¶å¯¹é½æ•°æ®é•¿åº¦
            # æ³¨æ„ï¼šå½“ä½¿ç”¨åŸå§‹Kçº¿è®¡ç®—MACDæ—¶ï¼ŒKçº¿æ•°é‡å¯èƒ½ä¸categoriesé•¿åº¦ä¸ä¸€è‡´
            # æˆ‘ä»¬éœ€è¦ç¡®ä¿MACDæ•°æ®é•¿åº¦ä¸categoriesä¸€è‡´ï¼Œä»¥ä¾¿å‰ç«¯æ­£ç¡®æ˜¾ç¤º
            
            # å¦‚æœåŸå§‹Kçº¿æ•°é‡å¤šäºcategoriesé•¿åº¦ï¼Œéœ€è¦æˆªå–æœ€æ–°çš„æ•°æ®
            # å› ä¸ºcategoriesæ˜¯åŸºäºå¤„ç†åçš„Kçº¿ç”Ÿæˆçš„ï¼Œè€Œå¤„ç†åçš„Kçº¿é€šå¸¸æ¯”åŸå§‹Kçº¿å°‘
            if len(dif) > len(categories):
                # æˆªå–æœ€æ–°çš„æ•°æ®ï¼ˆå°¾éƒ¨æ•°æ®ï¼‰
                dif = dif[-len(categories):]
                dea = dea[-len(categories):]
                macd = macd[-len(categories):]
                logger.info(f"MACDæ•°æ®å·²æˆªå–: ä»{len(dif)}æˆªå–åˆ°{len(categories)}")
            elif len(dif) < len(categories):
                # å¦‚æœMACDæ•°æ®å°‘äºcategoriesï¼Œéœ€è¦åœ¨å‰é¢è¡¥å……0
                padding_length = len(categories) - len(dif)
                dif = [0.0] * padding_length + dif
                dea = [0.0] * padding_length + dea
                macd = [0.0] * padding_length + macd
                logger.info(f"MACDæ•°æ®å·²è¡¥å……: ä»{len(dif)-padding_length}è¡¥å……åˆ°{len(categories)}")
            
            min_length = min(len(categories), len(dif), len(dea), len(macd))
            
            # è®°å½•MACDè®¡ç®—ä¿¡æ¯
            logger.info(f"MACDè®¡ç®—å®Œæˆ: åŸå§‹Kçº¿æ•°é‡={len(klines)}, categoriesé•¿åº¦={len(categories)}, æœ€ç»ˆæ•°æ®é•¿åº¦={min_length}")
            
            # è®°å½•MACDæ•°æ®çš„ä¸€äº›ç»Ÿè®¡ä¿¡æ¯ï¼Œä»¥ä¾¿äºè°ƒè¯•
            if len(dif) > 0:
                logger.info(f"MACDæ•°æ®ç»Ÿè®¡: DIFèŒƒå›´=[{min(dif):.4f}, {max(dif):.4f}], DEAèŒƒå›´=[{min(dea):.4f}, {max(dea):.4f}], MACDèŒƒå›´=[{min(macd):.4f}, {max(macd):.4f}]")
            
            return {
                "dif": [round(dif[i], 6) for i in range(min_length)],
                "dea": [round(dea[i], 6) for i in range(min_length)],
                "macd": [round(macd[i], 6) for i in range(min_length)]
            }
            
        except Exception as e:
            logger.error(f"è®¡ç®—MACDå¤±è´¥: {e}")
            return {"dif": [], "dea": [], "macd": []}
    
    def _convert_multi_level_to_frontend(self, results: Dict[TimeLevel, ChanAnalysisResult], 
                                       symbol: str, levels: List[str], days: int) -> Dict[str, Any]:
        """è½¬æ¢å¤šçº§åˆ«åˆ†æç»“æœä¸ºå‰ç«¯æ ¼å¼"""
        
        multi_level_data = {
            "meta": {
                "symbol": symbol,
                "levels": levels,
                "analysis_time": datetime.now().isoformat(),
                "days": days
            },
            "results": {},
            "comparison": {
                "level_consistency": {},
                "signal_confirmation": {},
                "trend_alignment": {}
            }
        }
        
        # è½¬æ¢å„çº§åˆ«ç»“æœ
        for time_level, result in results.items():
            level_str = time_level.value
            
            # è·å–åŸºç¡€æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…é‡å¤å¤§é‡Kçº¿æ•°æ®ï¼‰
            stats = result.get_statistics()
            
            multi_level_data["results"][level_str] = {
                "meta": {
                    "level": level_str,
                    "analysis_level": result.analysis_level.value,
                    "data_count": stats['processed_klines_count']
                },
                
                "structures": {
                    "fenxing_count": stats['fenxings_count'],
                    "bi_count": stats['bis_count'],
                    "seg_count": stats['segs_count'],
                    "zhongshu_count": stats['zhongshus_count']
                },
                
                "dynamics": {
                    "backchi_count": stats['backchi_count'],
                    "buy_sell_points_count": stats['buy_sell_points_count'],
                    "buy_points_count": stats['buy_points_count'],
                    "sell_points_count": stats['sell_points_count']
                },
                
                "evaluation": {
                    "trend_direction": result.trend_direction,
                    "trend_strength": result.trend_strength,
                    "risk_level": result.risk_level,
                    "confidence_score": result.confidence_score,
                    "recommended_action": result.recommended_action,
                    "level_consistency_score": result.level_consistency_score
                },
                
                "latest_signals": [
                    {
                        "type": str(point.point_type),
                        "price": point.price,
                        "timestamp": point.timestamp.isoformat(),
                        "reliability": point.reliability
                    }
                    for point in result.get_latest_signals(3)
                ]
            }
            
            # çº§åˆ«ä¸€è‡´æ€§åˆ†æ
            multi_level_data["comparison"]["level_consistency"][level_str] = result.level_consistency_score
        
        # è®¡ç®—çº§åˆ«é—´ä¿¡å·ç¡®è®¤
        self._analyze_signal_confirmation(multi_level_data, results)
        
        return multi_level_data
    
    def _analyze_signal_confirmation(self, multi_level_data: Dict, results: Dict[TimeLevel, ChanAnalysisResult]):
        """åˆ†æçº§åˆ«é—´ä¿¡å·ç¡®è®¤"""
        try:
            # è·å–æ‰€æœ‰çº§åˆ«çš„æœ€æ–°ä¿¡å·
            all_signals = {}
            for time_level, result in results.items():
                latest_signals = result.get_latest_signals(3)
                all_signals[time_level.value] = [
                    {
                        "type": str(point.point_type),
                        "timestamp": point.timestamp,
                        "reliability": point.reliability
                    }
                    for point in latest_signals
                ]
            
            # è®¡ç®—ä¿¡å·ä¸€è‡´æ€§
            signal_confirmation = {}
            level_names = list(all_signals.keys())
            
            for i, level1 in enumerate(level_names):
                for level2 in level_names[i+1:]:
                    key = f"{level1}_vs_{level2}"
                    signal_confirmation[key] = self._calculate_signal_similarity(
                        all_signals[level1], all_signals[level2]
                    )
            
            multi_level_data["comparison"]["signal_confirmation"] = signal_confirmation
            
        except Exception as e:
            logger.warning(f"åˆ†æä¿¡å·ç¡®è®¤å¤±è´¥: {e}")
    
    def _calculate_signal_similarity(self, signals1: List[Dict], signals2: List[Dict]) -> float:
        """è®¡ç®—ä¸¤ä¸ªçº§åˆ«ä¿¡å·çš„ç›¸ä¼¼åº¦"""
        if not signals1 or not signals2:
            return 0.0
        
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šåŸºäºä¿¡å·ç±»å‹å’Œæ—¶é—´æ¥è¿‘åº¦
        similarity_score = 0.0
        matches = 0
        
        for sig1 in signals1:
            for sig2 in signals2:
                # ç±»å‹åŒ¹é…
                if sig1["type"] == sig2["type"]:
                    # æ—¶é—´æ¥è¿‘ï¼ˆ7å¤©å†…ï¼‰
                    time_diff = abs((sig1["timestamp"] - sig2["timestamp"]).total_seconds())
                    if time_diff < 7 * 24 * 3600:  # 7å¤©
                        similarity_score += min(sig1["reliability"], sig2["reliability"])
                        matches += 1
        
        return similarity_score / max(matches, 1)
    
    def _convert_signals_to_frontend(self, signals: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢äº¤æ˜“ä¿¡å·ä¸ºå‰ç«¯æ ¼å¼"""
        
        frontend_signals = {
            "signals": [],
            "summary": signals.get("summary", {}),
            "timestamp": signals.get("timestamp", datetime.now()).isoformat() if hasattr(signals.get("timestamp"), 'isoformat') else str(signals.get("timestamp", datetime.now()))
        }
        
        for signal in signals.get("signals", []):
            try:
                frontend_signal = {
                    "type": signal.get("type"),
                    "point_type": signal.get("point_type", signal.get("type")),
                    "price": signal.get("price"),
                    "timestamp": signal.get("timestamp").isoformat() if hasattr(signal.get("timestamp"), 'isoformat') else str(signal.get("timestamp")),
                    "reliability": signal.get("reliability"),
                    "strength": signal.get("strength", 0.0),
                    "confirmed": signal.get("confirmed", False)
                }
                
                # å¤„ç†èƒŒé©°ä¿¡å·çš„ç‰¹æ®Šå­—æ®µ
                if signal.get("type") == "backchi":
                    frontend_signal.update({
                        "backchi_type": signal.get("backchi_type"),
                        "strength_ratio": signal.get("strength_ratio")
                    })
                
                frontend_signals["signals"].append(frontend_signal)
                
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¿¡å·æ•°æ®å¤±è´¥: {e}")
                continue
        
        return frontend_signals
    
    def _get_time_level(self, timeframe: str) -> TimeLevel:
        """è·å–æ—¶é—´çº§åˆ«æšä¸¾"""
        mapping = {
            "5min": TimeLevel.MIN_5,
            "30min": TimeLevel.MIN_30,
            "daily": TimeLevel.DAILY
        }
        return mapping.get(timeframe, TimeLevel.DAILY)
    
    def _generate_empty_result(self, symbol: str, timeframe: str) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºçš„åˆ†æç»“æœ"""
        return {
            "meta": {
                "symbol": symbol,
                "timeframe": timeframe,
                "analysis_level": "complete",
                "analysis_time": datetime.now().isoformat(),
                "data_range": {"days": 0, "start_date": datetime.now().isoformat(), "end_date": datetime.now().isoformat()},
                "data_count": 0
            },
            "chart_data": {
                "kline": {"categories": [], "values": [], "volumes": []},
                "indicators": {"macd": {"dif": [], "dea": [], "macd": []}},
                "chan_structures": {"fenxing": [], "bi": [], "seg": [], "zhongshu": []},
                "dynamics": {"buy_sell_points": [], "backchi": []}
            },
            "analysis": {
                "summary": {
                    "klines_original": 0, "klines_processed": 0,
                    "fenxing_count": 0, "bi_count": 0, "seg_count": 0, "zhongshu_count": 0,
                    "backchi_count": 0, "buy_sell_points_count": 0
                },
                "evaluation": {
                    "trend_direction": None, "trend_strength": 0.0,
                    "risk_level": 0.0, "confidence_score": 0.0,
                    "recommended_action": "wait"
                },
                "latest_signals": []
            },
            "chart_config": {
                "colors": {
                    "up_candle": "#ef5150", "down_candle": "#26a69a",
                    "fenxing_top": "#f44336", "fenxing_bottom": "#4caf50",
                    "bi_up": "#e91e63", "bi_down": "#2196f3",
                    "seg_up": "#ff5722", "seg_down": "#3f51b5",
                    "zhongshu": "#ff9800", "buy_point": "#4caf50", "sell_point": "#f44336"
                }
            }
        }
    
    def _generate_empty_multi_level_result(self, symbol: str, levels: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºçš„å¤šçº§åˆ«åˆ†æç»“æœ"""
        return {
            "meta": {
                "symbol": symbol,
                "levels": levels,
                "analysis_time": datetime.now().isoformat(),
                "days": 0
            },
            "results": {},
            "comparison": {
                "level_consistency": {},
                "signal_confirmation": {},
                "trend_alignment": {}
            }
        }
    
    def save_analysis_to_json(self, symbol: str, timeframe: str = "daily", 
                            days: int = 90, analysis_level: str = "complete",
                            output_file: str = None) -> str:
        """åˆ†æè‚¡ç¥¨å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        # æ‰§è¡Œåˆ†æ
        data = self.analyze_symbol_complete(symbol, timeframe, days, analysis_level)
        
        # ç”Ÿæˆæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"chan_v2_data_{symbol}_{timeframe}_{analysis_level}_{timestamp}.json"
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ… ç¼ è®ºv2åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    # ==================== é€‰è‚¡åŠŸèƒ½ ====================
    
    def run_stock_selection(self, max_results: int = 50, custom_config: Dict = None) -> Dict[str, Any]:
        """
        æ‰§è¡Œç¼ è®ºå¤šçº§åˆ«èƒŒé©°é€‰è‚¡
        
        Args:
            max_results: æœ€å¤§è¿”å›ç»“æœæ•°é‡
            custom_config: è‡ªå®šä¹‰é…ç½®å‚æ•°
            
        Returns:
            é€‰è‚¡ç»“æœæ•°æ®
        """
        try:
            logger.info(f"ğŸ¯ å¼€å§‹æ‰§è¡Œç¼ è®ºå¤šçº§åˆ«èƒŒé©°é€‰è‚¡ï¼Œæœ€å¤§ç»“æœæ•°: {max_results}")
            
            # å¦‚æœæœ‰è‡ªå®šä¹‰é…ç½®ï¼Œæ›´æ–°é€‰è‚¡å™¨é…ç½®
            if custom_config:
                self.stock_selector.config.update(custom_config)
                logger.info(f"ğŸ“ å·²æ›´æ–°é€‰è‚¡é…ç½®: {custom_config}")
            
            # æ‰§è¡Œé€‰è‚¡
            signals = self.stock_selector.run_stock_selection(max_results)
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
            frontend_data = self._convert_stock_selection_to_frontend(signals, max_results)
            
            logger.info(f"âœ… é€‰è‚¡å®Œæˆï¼Œç­›é€‰å‡º {len(signals)} ä¸ªä¿¡å·")
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ é€‰è‚¡æ‰§è¡Œå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_empty_stock_selection_result()
    
    def get_stock_selection_config(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰é€‰è‚¡é…ç½®
        
        Returns:
            é€‰è‚¡é…ç½®ä¿¡æ¯
        """
        try:
            config = self.stock_selector.config.copy()
            
            return {
                "current_config": config,
                "config_description": {
                    "days_30min": "30åˆ†é’Ÿçº§åˆ«åˆ†æå¤©æ•°",
                    "min_backchi_strength": "æœ€å°èƒŒé©°å¼ºåº¦é˜ˆå€¼(0-1)",
                    "min_area_ratio": "ç»¿æŸ±é¢ç§¯æ¯”é˜ˆå€¼(>1.0)",
                    "max_area_shrink_ratio": "çº¢æŸ±é¢ç§¯ç¼©å°æ¯”ä¾‹(0-1)",
                    "confirm_days": "é‡‘å‰ç¡®è®¤å¤©æ•°",
                    "death_cross_confirm_days": "æ­»å‰ç¡®è®¤å¤©æ•°",
                    "max_stocks_per_batch": "æ¯æ‰¹å¤„ç†è‚¡ç¥¨æ•°é‡ä¸Šé™"
                },
                "recommendations": {
                    "conservative": {
                        "min_backchi_strength": 0.8,
                        "min_area_ratio": 2.0,
                        "max_area_shrink_ratio": 0.7,
                        "death_cross_confirm_days": 3,
                        "description": "ä¿å®ˆé…ç½®ï¼šé«˜é˜ˆå€¼ä¸¥æ ¼ç­›é€‰"
                    },
                    "balanced": {
                        "min_backchi_strength": 0.6,
                        "min_area_ratio": 1.5,
                        "max_area_shrink_ratio": 0.8,
                        "death_cross_confirm_days": 2,
                        "description": "å¹³è¡¡é…ç½®ï¼šä¸­ç­‰é˜ˆå€¼ç­›é€‰"
                    },
                    "aggressive": {
                        "min_backchi_strength": 0.4,
                        "min_area_ratio": 1.2,
                        "max_area_shrink_ratio": 0.85,
                        "death_cross_confirm_days": 1,
                        "description": "æ¿€è¿›é…ç½®ï¼šä½é˜ˆå€¼å¿«é€Ÿå“åº”"
                    }
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ è·å–é€‰è‚¡é…ç½®å¤±è´¥: {e}")
            return {"current_config": {}, "config_description": {}, "recommendations": {}}
    
    def update_stock_selection_config(self, new_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ›´æ–°é€‰è‚¡é…ç½®
        
        Args:
            new_config: æ–°çš„é…ç½®å‚æ•°
            
        Returns:
            æ›´æ–°ç»“æœ
        """
        try:
            old_config = self.stock_selector.config.copy()
            
            # éªŒè¯é…ç½®å‚æ•°
            valid_keys = {
                'days_30min', 'min_backchi_strength', 'min_area_ratio',
                'max_area_shrink_ratio', 'confirm_days', 'death_cross_confirm_days',
                'max_stocks_per_batch'
            }
            
            validated_config = {}
            for key, value in new_config.items():
                if key in valid_keys:
                    # æ•°å€¼èŒƒå›´éªŒè¯
                    if key in ['min_backchi_strength', 'max_area_shrink_ratio']:
                        if 0 <= value <= 1:
                            validated_config[key] = value
                        else:
                            raise ValueError(f"{key} å¿…é¡»åœ¨ 0-1 èŒƒå›´å†…")
                    elif key == 'min_area_ratio':
                        if value > 1.0:
                            validated_config[key] = float(value)
                        else:
                            raise ValueError(f"{key} å¿…é¡»å¤§äº 1.0")
                    elif key in ['confirm_days', 'death_cross_confirm_days']:
                        if value > 0:
                            validated_config[key] = int(value)
                        else:
                            raise ValueError(f"{key} å¿…é¡»å¤§äº 0")
                    elif key == 'days_30min':
                        if value > 0:
                            validated_config[key] = int(value)
                        else:
                            raise ValueError(f"{key} å¿…é¡»å¤§äº 0")
                    elif key == 'max_stocks_per_batch':
                        if value >= 0:  # å…è®¸0ï¼Œè¡¨ç¤ºä¸é™åˆ¶
                            validated_config[key] = int(value)
                        else:
                            raise ValueError(f"{key} å¿…é¡»å¤§äºç­‰äº 0ï¼ˆ0è¡¨ç¤ºä¸é™åˆ¶ï¼‰")
                    else:
                        validated_config[key] = value
                else:
                    logger.warning(f"âš ï¸ å¿½ç•¥æ— æ•ˆé…ç½®é¡¹: {key}")
            
            # æ›´æ–°é…ç½®
            self.stock_selector.config.update(validated_config)
            
            logger.info(f"âœ… é€‰è‚¡é…ç½®å·²æ›´æ–°: {validated_config}")
            
            return {
                "success": True,
                "message": "é…ç½®æ›´æ–°æˆåŠŸ",
                "old_config": old_config,
                "new_config": self.stock_selector.config.copy(),
                "updated_fields": list(validated_config.keys())
            }
            
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°é€‰è‚¡é…ç½®å¤±è´¥: {e}")
            return {
                "success": False,
                "message": f"é…ç½®æ›´æ–°å¤±è´¥: {str(e)}",
                "old_config": {},
                "new_config": {},
                "updated_fields": []
            }
    
    def get_stock_selection_history(self, limit: int = 20) -> Dict[str, Any]:
        """
        è·å–é€‰è‚¡å†å²è®°å½•ï¼ˆç®€åŒ–ç‰ˆï¼Œåç»­å¯æ‰©å±•åˆ°æ•°æ®åº“å­˜å‚¨ï¼‰
        
        Args:
            limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            å†å²è®°å½•æ•°æ®
        """
        # è¿™é‡Œå¯ä»¥åç»­æ‰©å±•ä¸ºä»æ•°æ®åº“è¯»å–å†å²è®°å½•
        # ç›®å‰è¿”å›ç©ºæ•°æ®ç»“æ„
        return {
            "history": [],
            "total_count": 0,
            "message": "å†å²è®°å½•åŠŸèƒ½å¾…å®ç°ï¼Œå¯ç»“åˆæ•°æ®åº“å­˜å‚¨é€‰è‚¡ç»“æœ"
        }
    
    def _convert_stock_selection_to_frontend(self, signals: List, max_results: int) -> Dict[str, Any]:
        """è½¬æ¢é€‰è‚¡ç»“æœä¸ºå‰ç«¯æ ¼å¼ï¼ˆåŸºäºæ–°çš„StockSignalç»“æ„ï¼‰"""
        try:
            # ç»Ÿè®¡ä¹°å…¥å’Œå–å‡ºä¿¡å·
            buy_signals = [s for s in signals if s.signal_type == "ä¹°å…¥"]
            sell_signals = [s for s in signals if s.signal_type == "å–å‡º"]
            
            frontend_data = {
                "meta": {
                    "analysis_time": datetime.now().isoformat(),
                    "max_results": max_results,
                    "actual_results": len(signals),
                    "selection_criteria": {
                        "min_backchi_strength": self.stock_selector.config.get('min_backchi_strength', 0.3),
                        "require_macd_golden_cross": self.stock_selector.config.get('require_macd_golden_cross', True),
                        "analysis_days_30min": self.stock_selector.config.get('days_30min', 30)
                    }
                },
                
                "results": {
                    "buy_signals": [],
                    "sell_signals": []
                },
                
                "statistics": {
                    "total_signals": len(signals),
                    "buy_signals_count": len(buy_signals),
                    "sell_signals_count": len(sell_signals),
                    "strength_distribution": {
                        "strong": len([s for s in signals if s.signal_strength.value == "strong"]),
                        "medium": len([s for s in signals if s.signal_strength.value == "medium"]),
                        "weak": len([s for s in signals if s.signal_strength.value == "weak"])
                    },
                    "recommendation_distribution": {}
                },
                
                "config_used": self.stock_selector.config.copy()
            }
            
            # è½¬æ¢ä¹°å…¥ä¿¡å·
            for signal in buy_signals:
                try:
                    frontend_signal = {
                        "basic_info": {
                            "symbol": signal.symbol,
                            "name": signal.name,
                            "signal_type": signal.signal_type,
                            "analysis_time": signal.analysis_time.isoformat()
                        },
                        
                        "scoring": {
                            "overall_score": round(signal.overall_score, 2),
                            "signal_strength": signal.signal_strength.value,
                            "recommendation": signal.recommendation
                        },
                        
                        "backchi_analysis": {
                            "backchi_type": getattr(signal, 'backchi_type', None),
                            "reliability": round(getattr(signal, 'reliability', 0.0), 3),
                            "description": getattr(signal, 'description', ''),
                            "has_macd_golden_cross": getattr(signal, 'has_macd_golden_cross', False),
                            "has_macd_death_cross": getattr(signal, 'has_macd_death_cross', False)
                        },
                        
                        "key_prices": {
                            "entry_price": round(signal.entry_price, 2) if signal.entry_price else None,
                            "stop_loss": round(signal.stop_loss, 2) if signal.stop_loss else None,
                            "take_profit": round(signal.take_profit, 2) if signal.take_profit else None,
                            "risk_reward_ratio": round((signal.take_profit - signal.entry_price) / (signal.entry_price - signal.stop_loss), 2) if (signal.entry_price and signal.stop_loss and signal.take_profit) else None
                        }
                    }
                    
                    frontend_data["results"]["buy_signals"].append(frontend_signal)
                    
                except Exception as e:
                    logger.warning(f"è½¬æ¢ä¹°å…¥ä¿¡å·å¤±è´¥: {e}")
                    continue
            
            # è½¬æ¢å–å‡ºä¿¡å·
            for signal in sell_signals:
                try:
                    frontend_signal = {
                        "basic_info": {
                            "symbol": signal.symbol,
                            "name": signal.name,
                            "signal_type": signal.signal_type,
                            "analysis_time": signal.analysis_time.isoformat()
                        },
                        
                        "scoring": {
                            "overall_score": round(signal.overall_score, 2),
                            "signal_strength": signal.signal_strength.value,
                            "recommendation": signal.recommendation
                        },
                        
                        "backchi_analysis": {
                            "backchi_type": getattr(signal, 'backchi_type', None),
                            "reliability": round(getattr(signal, 'reliability', 0.0), 3),
                            "description": getattr(signal, 'description', ''),
                            "has_macd_golden_cross": getattr(signal, 'has_macd_golden_cross', False),
                            "has_macd_death_cross": getattr(signal, 'has_macd_death_cross', False)
                        },
                        
                        "key_prices": {
                            "entry_price": round(signal.entry_price, 2) if signal.entry_price else None,
                            "stop_loss": round(signal.stop_loss, 2) if signal.stop_loss else None,
                            "take_profit": round(signal.take_profit, 2) if signal.take_profit else None,
                            "risk_reward_ratio": round((signal.take_profit - signal.entry_price) / (signal.entry_price - signal.stop_loss), 2) if (signal.entry_price and signal.stop_loss and signal.take_profit) else None
                        }
                    }
                    
                    frontend_data["results"]["sell_signals"].append(frontend_signal)
                    
                except Exception as e:
                    logger.warning(f"è½¬æ¢å–å‡ºä¿¡å·å¤±è´¥: {e}")
                    continue
            
            # æ›´æ–°æ¨èåˆ†å¸ƒç»Ÿè®¡
            for signal in signals:
                rec = signal.recommendation
                if rec not in frontend_data["statistics"]["recommendation_distribution"]:
                    frontend_data["statistics"]["recommendation_distribution"][rec] = 0
                frontend_data["statistics"]["recommendation_distribution"][rec] += 1
            
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢é€‰è‚¡ç»“æœå¤±è´¥: {e}")
            return self._generate_empty_stock_selection_result()
    
    def _generate_empty_stock_selection_result(self) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºçš„é€‰è‚¡ç»“æœ"""
        return {
            "meta": {
                "analysis_time": datetime.now().isoformat(),
                "max_results": 0,
                "actual_results": 0,
                "selection_criteria": {}
            },
            "results": [],
            "statistics": {
                "total_processed": 0,
                "signals_found": 0,
                "success_rate": 0.0,
                "strength_distribution": {"strong": 0, "medium": 0, "weak": 0},
                "recommendation_distribution": {"å¼ºçƒˆå…³æ³¨": 0, "å¯†åˆ‡ç›‘æ§": 0, "é€‚åº¦å…³æ³¨": 0, "è§‚æœ›": 0}
            },
            "config_used": {}
        }


# åˆ›å»ºå…¨å±€APIå®ä¾‹
chan_api_v2 = ChanDataAPIv2()

if __name__ == '__main__':
    # å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¼ è®ºæ•°æ®API v2')
    parser.add_argument('--symbol', '-s', required=True, help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--timeframe', '-t', default='daily', choices=['5min', '30min', 'daily'], help='æ—¶é—´çº§åˆ«')
    parser.add_argument('--days', '-d', type=int, default=90, help='åˆ†æå¤©æ•°')
    parser.add_argument('--level', '-l', default='complete', choices=['basic', 'standard', 'advanced', 'complete'], help='åˆ†æçº§åˆ«')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--multi-level', '-m', action='store_true', help='å¤šçº§åˆ«åˆ†æ')
    
    args = parser.parse_args()
    
    print(f"ğŸ” ç¼ è®ºv2åˆ†æ {args.symbol} ({args.timeframe}, {args.days}å¤©, {args.level}çº§åˆ«)")
    
    if args.multi_level:
        # å¤šçº§åˆ«åˆ†æ
        data = chan_api_v2.analyze_multi_level(args.symbol, ["daily", "30min", "5min"], args.days)
        output_file = args.output or f"chan_v2_multi_level_{args.symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
        print(f"ğŸ‰ å¤šçº§åˆ«åˆ†æå®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    else:
        # å•çº§åˆ«åˆ†æ
        output_file = chan_api_v2.save_analysis_to_json(
            args.symbol, args.timeframe, args.days, args.level, args.output
        )
        
        if output_file:
            print(f"ğŸ‰ åˆ†æå®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š åŸºäºæœ€æ–°ç¼ è®ºv2å¼•æ“ï¼ˆå½¢æ€å­¦+åŠ¨åŠ›å­¦ï¼‰çš„å®Œæ•´åˆ†æ")
        else:
            print("âŒ åˆ†æå¤±è´¥")