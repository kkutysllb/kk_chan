#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¼ è®ºåˆ†ææ•°æ®API v2
åŸºäºæœ€æ–°çš„ç¼ è®ºv2æ ¸å¿ƒæ¨¡å—ï¼ˆå½¢æ€å­¦+åŠ¨åŠ›å­¦ï¼‰ï¼Œæä¾›å®Œæ•´çš„ç¼ è®ºåˆ†ææœåŠ¡
æ”¯æŒå¤šçº§åˆ«åˆ†æã€åŠ¨åŠ›å­¦åˆ†æã€ä¹°å–ç‚¹è¯†åˆ«ç­‰é«˜çº§åŠŸèƒ½
"""

import sys
import os
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# å¯¼å…¥ç¼ è®ºv2æ ¸å¿ƒç»„ä»¶
from chan_theory_v2.core.chan_engine import ChanEngine, ChanAnalysisResult, AnalysisLevel, quick_analyze, multi_level_analyze
from chan_theory_v2.models.enums import TimeLevel, BiDirection, SegDirection, ZhongShuType
from chan_theory_v2.models.dynamics import BuySellPointType, BackChi, DynamicsConfig
from chan_theory_v2.config.chan_config import ChanConfig
from database.db_handler import get_db_handler

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChanDataAPIv2:
    """ç¼ è®ºæ•°æ®API v2 - åŸºäºæœ€æ–°ç¼ è®ºå¼•æ“çš„å®Œæ•´åˆ†ææœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–API"""
        self.db_handler = get_db_handler()
        self.db = self.db_handler.db
        
        # åˆå§‹åŒ–ç¼ è®ºå¼•æ“
        self.chan_engine = ChanEngine()
        
        logger.info("ğŸš€ ç¼ è®ºæ•°æ®API v2åˆå§‹åŒ–å®Œæˆ")
    
    def get_symbols_list(self, query: str = "") -> List[Dict[str, str]]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        try:
            collection = self.db["infrastructure_stock_basic"]
            
            # æ„å»ºæœç´¢æ¡ä»¶
            search_filter = {"ts_code": {"$exists": True}, "name": {"$exists": True}}
            
            if query and query.strip():
                # æ”¯æŒæŒ‰è‚¡ç¥¨ä»£ç æˆ–åç§°æœç´¢
                query = query.strip().upper()
                search_filter["$or"] = [
                    {"ts_code": {"$regex": query, "$options": "i"}},
                    {"name": {"$regex": query, "$options": "i"}}
                ]
                logger.info(f"ğŸ” æœç´¢è‚¡ç¥¨: {query}")
            
            cursor = collection.find(
                search_filter,
                {"ts_code": 1, "name": 1, "_id": 0}
            ).limit(100)  # æœç´¢æ—¶é™åˆ¶100ä¸ªç»“æœ
            
            stocks = []
            for doc in cursor:
                stocks.append({
                    "value": doc.get("ts_code", ""),
                    "label": f"{doc.get('ts_code', '')} - {doc.get('name', '')}",
                    "name": doc.get("name", "")
                })
            
            logger.info(f"ğŸ“‹ è·å–åˆ° {len(stocks)} ä¸ªè‚¡ç¥¨{'ï¼ˆæœç´¢ç»“æœï¼‰' if query else ''}")
            return stocks
            
        except Exception as e:
            logger.error(f"âŒ è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def analyze_symbol_complete(self, 
                              symbol: str, 
                              timeframe: str = "daily", 
                              days: int = 90,
                              analysis_level: str = "complete") -> Dict[str, Any]:
        """
        å®Œæ•´çš„ç¼ è®ºåˆ†æ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´çº§åˆ« ("5min", "30min", "daily")
            days: åˆ†æå¤©æ•°
            analysis_level: åˆ†æçº§åˆ« ("basic", "standard", "advanced", "complete")
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹ç¼ è®ºv2åˆ†æ {symbol} ({timeframe}, {days}å¤©, {analysis_level}çº§åˆ«)")
            
            # è·å–æ•°æ®
            time_level = self._get_time_level(timeframe)
            data = self._fetch_stock_data(symbol, time_level, days)
            
            if not data:
                logger.warning(f"âš ï¸ æ— æ³•è·å– {symbol} çš„æ•°æ®")
                return self._generate_empty_result(symbol, timeframe)
            
            # è®¾ç½®åˆ†æçº§åˆ«
            level_mapping = {
                "basic": AnalysisLevel.BASIC,
                "standard": AnalysisLevel.STANDARD,
                "advanced": AnalysisLevel.ADVANCED,
                "complete": AnalysisLevel.COMPLETE
            }
            analysis_level_enum = level_mapping.get(analysis_level, AnalysisLevel.COMPLETE)
            
            # æ‰§è¡Œç¼ è®ºåˆ†æ
            result = self.chan_engine.analyze(
                data=data,
                symbol=symbol,
                time_level=time_level,
                analysis_level=analysis_level_enum
            )
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ‡å‡†æ ¼å¼
            frontend_data = self._convert_to_frontend_format(result, timeframe, days)
            
            logger.info(f"âœ… {symbol} ç¼ è®ºv2åˆ†æå®Œæˆ")
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æ {symbol} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_empty_result(symbol, timeframe)
    
    def analyze_multi_level(self, 
                          symbol: str, 
                          levels: List[str] = ["daily", "30min", "5min"],
                          days: int = 90) -> Dict[str, Any]:
        """
        å¤šçº§åˆ«ç¼ è®ºåˆ†æ
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            levels: åˆ†æçº§åˆ«åˆ—è¡¨
            days: åˆ†æå¤©æ•°
            
        Returns:
            å¤šçº§åˆ«åˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹å¤šçº§åˆ«ç¼ è®ºåˆ†æ {symbol} ({levels}, {days}å¤©)")
            
            # å‡†å¤‡å¤šçº§åˆ«æ•°æ®
            level_data = {}
            for level_str in levels:
                time_level = self._get_time_level(level_str)
                data = self._fetch_stock_data(symbol, time_level, days)
                if data:
                    level_data[time_level] = data
                    logger.info(f"âœ… {level_str}æ•°æ®: {len(data)} æ¡")
                else:
                    logger.warning(f"âš ï¸ æ— æ³•è·å–{level_str}æ•°æ®")
            
            if not level_data:
                logger.warning(f"âš ï¸ æ— ä»»ä½•çº§åˆ«æ•°æ®å¯ç”¨äº {symbol}")
                return self._generate_empty_multi_level_result(symbol, levels)
            
            # æ‰§è¡Œå¤šçº§åˆ«åˆ†æ
            results = self.chan_engine.analyze_multi_level(level_data, symbol)
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
            frontend_data = self._convert_multi_level_to_frontend(results, symbol, levels, days)
            
            logger.info(f"âœ… {symbol} å¤šçº§åˆ«åˆ†æå®Œæˆï¼Œå…±{len(results)}ä¸ªçº§åˆ«")
            return frontend_data
            
        except Exception as e:
            logger.error(f"âŒ å¤šçº§åˆ«åˆ†æ {symbol} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._generate_empty_multi_level_result(symbol, levels)
    
    def get_trading_signals(self, symbol: str, timeframe: str = "daily", days: int = 30) -> Dict[str, Any]:
        """
        è·å–äº¤æ˜“ä¿¡å·
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            timeframe: æ—¶é—´çº§åˆ«
            days: åˆ†æå¤©æ•°
            
        Returns:
            äº¤æ˜“ä¿¡å·æ•°æ®
        """
        try:
            logger.info(f"ğŸ¯ è·å–äº¤æ˜“ä¿¡å· {symbol} ({timeframe}, {days}å¤©)")
            
            # æ‰§è¡Œæ ‡å‡†åˆ†æï¼ˆåŒ…å«åŠ¨åŠ›å­¦ï¼‰
            time_level = self._get_time_level(timeframe)
            data = self._fetch_stock_data(symbol, time_level, days)
            
            if not data:
                return {"signals": [], "summary": {"total": 0, "buy": 0, "sell": 0}}
            
            result = self.chan_engine.analyze(
                data=data,
                symbol=symbol,
                time_level=time_level,
                analysis_level=AnalysisLevel.STANDARD
            )
            
            # è·å–äº¤æ˜“ä¿¡å·
            signals = self.chan_engine.get_trading_signals(result)
            
            # è½¬æ¢ä¸ºå‰ç«¯æ ¼å¼
            frontend_signals = self._convert_signals_to_frontend(signals)
            
            logger.info(f"âœ… è·å–åˆ° {len(signals['signals'])} ä¸ªäº¤æ˜“ä¿¡å·")
            return frontend_signals
            
        except Exception as e:
            logger.error(f"âŒ è·å–äº¤æ˜“ä¿¡å·å¤±è´¥: {e}")
            return {"signals": [], "summary": {"total": 0, "buy": 0, "sell": 0}}
    
    def _fetch_stock_data(self, symbol: str, time_level: TimeLevel, days: int) -> List[Dict]:
        """è·å–è‚¡ç¥¨æ•°æ®"""
        try:
            # æ ¹æ®æ—¶é—´çº§åˆ«é€‰æ‹©é›†åˆ
            collection_mapping = {
                TimeLevel.MIN_5: "stock_kline_5min",
                TimeLevel.MIN_30: "stock_kline_30min", 
                TimeLevel.DAILY: "stock_kline_daily"
            }
            
            collection_name = collection_mapping.get(time_level, "stock_kline_daily")
            collection = self.db[collection_name]
            
            # æ„å»ºæŸ¥è¯¢
            query = {"ts_code": symbol}
            
            # è®¡ç®—æ•°æ®é‡ï¼ˆæ ¹æ®çº§åˆ«è°ƒæ•´ï¼‰
            if time_level == TimeLevel.MIN_5:
                limit = days * 48  # 5åˆ†é’Ÿ: æ¯å¤©çº¦48ä¸ªæ•°æ®ç‚¹
            elif time_level == TimeLevel.MIN_30:
                limit = days * 8   # 30åˆ†é’Ÿ: æ¯å¤©çº¦8ä¸ªæ•°æ®ç‚¹
            else:
                limit = days       # æ—¥çº¿: æ¯å¤©1ä¸ªæ•°æ®ç‚¹
            
            # è·å–æ•°æ®
            if time_level == TimeLevel.DAILY:
                # æ·»åŠ æ—¥æœŸèŒƒå›´é™åˆ¶ï¼Œè·å–æœ€è¿‘çš„æ•°æ®
                from datetime import datetime, timedelta
                end_date = datetime.now()
                start_date = end_date - timedelta(days=days)
                
                # å°†æ—¥æœŸè½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¸æ•°æ®åº“ä¸­çš„æ ¼å¼åŒ¹é…
                end_date_str = end_date.strftime('%Y%m%d')
                start_date_str = start_date.strftime('%Y%m%d')
                
                # ç¡®ä¿ä½¿ç”¨å­—ç¬¦ä¸²æ ¼å¼çš„æ—¥æœŸè¿›è¡ŒæŸ¥è¯¢ï¼Œä¸æ•°æ®åº“ä¸­çš„æ ¼å¼åŒ¹é…
                query.update({
                    'trade_date': {
                        '$gte': start_date_str,
                        '$lte': end_date_str
                    }
                })
                
                # æŒ‰æ—¥æœŸå‡åºæ’åºï¼Œè·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                cursor = collection.find(query).sort("trade_date", 1)
                logger.info(f"ğŸ“… æ—¥KæŸ¥è¯¢èŒƒå›´: {start_date_str} è‡³ {end_date_str}")
            else:
                cursor = collection.find(query).sort("trade_time", -1).limit(limit)
                
            raw_data = list(cursor)
            
            if time_level != TimeLevel.DAILY:
                raw_data.reverse()  # åˆ†é’Ÿæ•°æ®è½¬ä¸ºå‡åº
            
            # è½¬æ¢æ•°æ®æ ¼å¼
            converted_data = self._convert_data_format(raw_data, time_level)
            
            logger.info(f"ğŸ“Š è·å– {symbol} {time_level.value} æ•°æ®: {len(converted_data)} æ¡")
            return converted_data
            
        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
            return []
    
    def _convert_data_format(self, raw_data: List[Dict], time_level: TimeLevel) -> List[Dict]:
        """è½¬æ¢æ•°æ®æ ¼å¼"""
        converted_data = []
        
        for item in raw_data:
            try:
                # å¤„ç†æ—¶é—´æˆ³
                if time_level == TimeLevel.DAILY:
                    trade_date_str = str(item['trade_date'])
                    timestamp = datetime.strptime(trade_date_str, '%Y%m%d')
                else:
                    trade_time_str = str(item['trade_time'])
                    timestamp = datetime.strptime(trade_time_str, '%Y-%m-%d %H:%M:%S')
                
                converted_item = {
                    'timestamp': timestamp,
                    'open': float(item['open']),
                    'high': float(item['high']),
                    'low': float(item['low']),
                    'close': float(item['close']),
                    'volume': int(float(item.get('vol', item.get('volume', 1)))),
                    'amount': float(item.get('amount', 0)),
                    'symbol': item['ts_code']
                }
                
                # éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
                if all(converted_item[k] > 0 for k in ['open', 'high', 'low', 'close']):
                    converted_data.append(converted_item)
                    
            except Exception as e:
                logger.warning(f"è½¬æ¢æ•°æ®ç‚¹å¤±è´¥: {e}")
                continue
        
        return converted_data
    
    def _convert_to_frontend_format(self, result: ChanAnalysisResult, timeframe: str, days: int) -> Dict[str, Any]:
        """è½¬æ¢åˆ†æç»“æœä¸ºå‰ç«¯æ ¼å¼"""
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = result.get_statistics()
        
        # 1. Kçº¿æ•°æ®
        kline_data = self._convert_klines_to_echarts(result.processed_klines)
        
        # 2. åˆ†å‹æ•°æ®
        fenxing_data = self._convert_fenxings_to_echarts(result.fenxings)
        
        # 3. ç¬”æ•°æ®
        bi_data = self._convert_bis_to_echarts(result.bis)
        
        # 4. çº¿æ®µæ•°æ®
        seg_data = self._convert_segs_to_echarts(result.segs)
        
        # 5. ä¸­æ¢æ•°æ®
        zhongshu_data = self._convert_zhongshus_to_echarts(result.zhongshus)
        
        # 6. ä¹°å–ç‚¹æ•°æ®
        signal_data = self._convert_buy_sell_points_to_echarts(result.buy_sell_points)
        
        # 7. èƒŒé©°åˆ†ææ•°æ®
        backchi_data = self._convert_backchi_to_echarts(result.backchi_analyses)
        
        # 8. MACDæ•°æ®ï¼ˆåŸºäºå¤„ç†åçš„Kçº¿è®¡ç®—ï¼‰
        macd_data = self._calculate_macd_from_klines(result.processed_klines, kline_data.get("categories", []))
        
        # æ„å»ºå®Œæ•´çš„å‰ç«¯æ•°æ®ç»“æ„
        frontend_data = {
            # åŸºç¡€å…ƒä¿¡æ¯
            "meta": {
                "symbol": result.symbol,
                "timeframe": timeframe,
                "analysis_level": result.analysis_level.value,
                "analysis_time": result.analysis_time.isoformat(),
                "data_range": {
                    "days": days,
                    "start_date": (datetime.now() - timedelta(days=days)).isoformat(),
                    "end_date": datetime.now().isoformat()
                },
                "data_count": stats['processed_klines_count']
            },
            
            # å›¾è¡¨æ•°æ® - EChartsæ ‡å‡†æ ¼å¼
            "chart_data": {
                # Kçº¿æ•°æ®
                "kline": kline_data,
                
                # æŠ€æœ¯æŒ‡æ ‡
                "indicators": {
                    "macd": macd_data
                },
                
                # ç¼ è®ºç»“æ„
                "chan_structures": {
                    "fenxing": fenxing_data,    # åˆ†å‹ç‚¹
                    "bi": bi_data,              # ç¬”
                    "seg": seg_data,            # çº¿æ®µ
                    "zhongshu": zhongshu_data   # ä¸­æ¢
                },
                
                # åŠ¨åŠ›å­¦åˆ†æ
                "dynamics": {
                    "buy_sell_points": signal_data,  # ä¹°å–ç‚¹
                    "backchi": backchi_data          # èƒŒé©°åˆ†æ
                }
            },
            
            # åˆ†ææ‘˜è¦
            "analysis": {
                "summary": {
                    "klines_original": stats['klines_count'],
                    "klines_processed": stats['processed_klines_count'],
                    "fenxing_count": stats['fenxings_count'],
                    "bi_count": stats['bis_count'],
                    "seg_count": stats['segs_count'],
                    "zhongshu_count": stats['zhongshus_count'],
                    "backchi_count": stats['backchi_count'],
                    "buy_sell_points_count": stats['buy_sell_points_count'],
                    "buy_points_count": stats['buy_points_count'],
                    "sell_points_count": stats['sell_points_count']
                },
                
                # ç»¼åˆè¯„ä¼°
                "evaluation": {
                    "trend_direction": result.trend_direction,
                    "trend_strength": result.trend_strength,
                    "risk_level": result.risk_level,
                    "confidence_score": result.confidence_score,
                    "recommended_action": result.recommended_action,
                    "entry_price": result.entry_price,
                    "stop_loss": result.stop_loss,
                    "take_profit": result.take_profit
                },
                
                # æœ€æ–°ä¿¡å·
                "latest_signals": [
                    {
                        "type": str(point.point_type),
                        "price": point.price,
                        "timestamp": point.timestamp.isoformat(),
                        "reliability": point.reliability,
                        "strength": point.strength
                    }
                    for point in result.get_latest_signals(5)
                ]
            },
            
            # å›¾è¡¨é…ç½®
            "chart_config": {
                "colors": {
                    "up_candle": "#ef5150",      # çº¢è‰²Kçº¿
                    "down_candle": "#26a69a",    # ç»¿è‰²Kçº¿
                    "fenxing_top": "#f44336",    # é¡¶åˆ†å‹-çº¢è‰²
                    "fenxing_bottom": "#4caf50", # åº•åˆ†å‹-ç»¿è‰²
                    "bi_up": "#e91e63",          # ä¸Šç¬”-ç²‰è‰²
                    "bi_down": "#2196f3",        # ä¸‹ç¬”-è“è‰²
                    "seg_up": "#ff5722",         # ä¸Šçº¿æ®µ-æ©™çº¢è‰²
                    "seg_down": "#3f51b5",       # ä¸‹çº¿æ®µ-é›è“è‰²
                    "zhongshu": "#ff9800",       # ä¸­æ¢-æ©™è‰²
                    "buy_point": "#4caf50",      # ä¹°ç‚¹-ç»¿è‰²
                    "sell_point": "#f44336",     # å–ç‚¹-çº¢è‰²
                    "backchi": "#9c27b0"         # èƒŒé©°-ç´«è‰²
                }
            }
        }
        
        return frontend_data
    
    def _convert_klines_to_echarts(self, klines) -> Dict[str, Any]:
        """è½¬æ¢Kçº¿æ•°æ®ä¸ºEChartsæ ¼å¼"""
        if not klines or len(klines) == 0:
            return {"categories": [], "values": [], "volumes": []}
        
        categories = []  # æ—¶é—´è½´
        values = []     # Kçº¿æ•°æ® [open, close, low, high]
        volumes = []    # æˆäº¤é‡
        
        for kline in klines:
            try:
                # æ ¼å¼åŒ–æ—¶é—´
                if hasattr(kline.timestamp, 'strftime'):
                    timestamp = kline.timestamp.strftime('%Y-%m-%d %H:%M')
                else:
                    timestamp = str(kline.timestamp)
                
                categories.append(timestamp)
                
                # Kçº¿æ•°æ® [open, close, low, high]
                values.append([
                    float(kline.open),
                    float(kline.close),
                    float(kline.low),
                    float(kline.high)
                ])
                
                volumes.append(float(kline.volume))
                
            except Exception as e:
                logger.warning(f"è½¬æ¢Kçº¿æ•°æ®ç‚¹å¤±è´¥: {e}")
                continue
        
        return {
            "categories": categories,  # Xè½´æ—¶é—´
            "values": values,         # Kçº¿æ•°æ®
            "volumes": volumes        # æˆäº¤é‡
        }
    
    def _convert_fenxings_to_echarts(self, fenxings) -> List[Dict[str, Any]]:
        """è½¬æ¢åˆ†å‹æ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not fenxings or len(fenxings) == 0:
            return []
        
        fenxing_points = []
        
        for fenxing in fenxings:
            try:
                timestamp = fenxing.timestamp.strftime('%Y-%m-%d %H:%M')
                
                fenxing_points.append({
                    "name": f"{'é¡¶' if fenxing.is_top else 'åº•'}åˆ†å‹",
                    "coord": [timestamp, float(fenxing.price)],
                    "value": float(fenxing.price),
                    "type": "top" if fenxing.is_top else "bottom",
                    "direction": "up" if fenxing.is_top else "down",
                    "strength": float(fenxing.strength),
                    "confirmed": bool(fenxing.is_confirmed),
                    "symbol": "triangle",
                    "symbolSize": 8,
                    "itemStyle": {
                        "color": "#f44336" if fenxing.is_top else "#4caf50"
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢åˆ†å‹æ•°æ®å¤±è´¥: {e}")
                continue
        
        return fenxing_points
    
    def _convert_bis_to_echarts(self, bis) -> List[Dict[str, Any]]:
        """è½¬æ¢ç¬”æ•°æ®ä¸ºEChartsçº¿æ¡æ ¼å¼"""
        if not bis or len(bis) == 0:
            return []
        
        bi_lines = []
        
        for idx, bi in enumerate(bis):
            try:
                start_time = bi.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = bi.end_time.strftime('%Y-%m-%d %H:%M')
                
                bi_lines.append({
                    "id": f"bi_{idx}",
                    "name": f"{'ä¸Š' if bi.is_up else 'ä¸‹'}ç¬”",
                    "coords": [
                        [start_time, float(bi.start_price)],
                        [end_time, float(bi.end_price)]
                    ],
                    "direction": bi.direction.value,
                    "amplitude": float(bi.amplitude),
                    "strength": float(bi.strength),
                    "duration": bi.duration,
                    "lineStyle": {
                        "color": "#e91e63" if bi.is_up else "#2196f3",
                        "width": 2
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ç¬”æ•°æ®å¤±è´¥: {e}")
                continue
        
        return bi_lines
    
    def _convert_segs_to_echarts(self, segs) -> List[Dict[str, Any]]:
        """è½¬æ¢çº¿æ®µæ•°æ®ä¸ºEChartsçº¿æ¡æ ¼å¼"""
        if not segs or len(segs) == 0:
            return []
        
        seg_lines = []
        
        for idx, seg in enumerate(segs):
            try:
                start_time = seg.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = seg.end_time.strftime('%Y-%m-%d %H:%M')
                
                seg_lines.append({
                    "id": f"seg_{idx}",
                    "name": f"{'ä¸Š' if seg.is_up else 'ä¸‹'}çº¿æ®µ",
                    "coords": [
                        [start_time, float(seg.start_price)],
                        [end_time, float(seg.end_price)]
                    ],
                    "direction": seg.direction.value,
                    "amplitude": float(seg.amplitude),
                    "strength": float(seg.strength),
                    "integrity": float(seg.integrity),
                    "bi_count": seg.bi_count,
                    "duration": seg.duration,
                    "lineStyle": {
                        "color": "#ff5722" if seg.is_up else "#3f51b5",
                        "width": 3
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢çº¿æ®µæ•°æ®å¤±è´¥: {e}")
                continue
        
        return seg_lines
    
    def _convert_zhongshus_to_echarts(self, zhongshus) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¸­æ¢æ•°æ®ä¸ºEChartsåŒºåŸŸæ ¼å¼"""
        if not zhongshus or len(zhongshus) == 0:
            return []
        
        zhongshu_areas = []
        
        for idx, zs in enumerate(zhongshus):
            try:
                start_time = zs.start_time.strftime('%Y-%m-%d %H:%M')
                end_time = zs.end_time.strftime('%Y-%m-%d %H:%M')
                
                zhongshu_areas.append({
                    "id": f"zs_{idx}",
                    "name": f"ä¸­æ¢{idx+1}",
                    "coords": [
                        [start_time, float(zs.low)],    # å·¦ä¸‹è§’
                        [end_time, float(zs.high)]      # å³ä¸Šè§’
                    ],
                    "high": float(zs.high),
                    "low": float(zs.low),
                    "center": float(zs.center),
                    "strength": float(zs.strength),
                    "stability": float(zs.stability),
                    "seg_count": zs.seg_count,
                    "extend_count": zs.extend_count,
                    "is_active": zs.is_active,
                    "itemStyle": {
                        "color": "rgba(255, 152, 0, 0.2)",  # åŠé€æ˜æ©™è‰²
                        "borderColor": "#ff9800",
                        "borderWidth": 1
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¸­æ¢æ•°æ®å¤±è´¥: {e}")
                continue
        
        return zhongshu_areas
    
    def _convert_buy_sell_points_to_echarts(self, buy_sell_points) -> List[Dict[str, Any]]:
        """è½¬æ¢ä¹°å–ç‚¹æ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not buy_sell_points:
            return []
        
        signal_marks = []
        
        for idx, point in enumerate(buy_sell_points):
            try:
                timestamp = point.timestamp.strftime('%Y-%m-%d %H:%M')
                
                signal_marks.append({
                    "id": f"signal_{idx}",
                    "name": str(point.point_type),
                    "coord": [timestamp, float(point.price)],
                    "value": float(point.price),
                    "type": "buy" if point.point_type.is_buy() else "sell",
                    "point_level": point.point_type.get_level(),  # 1, 2, 3ç±»
                    "strength": float(point.strength),
                    "reliability": float(point.reliability),
                    "confirmed_higher": point.confirmed_by_higher_level,
                    "confirmed_lower": point.confirmed_by_lower_level,
                    "symbol": "arrow",
                    "symbolSize": 12,
                    "symbolRotate": 90 if point.point_type.is_buy() else -90,
                    "itemStyle": {
                        "color": "#4caf50" if point.point_type.is_buy() else "#f44336"
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¹°å–ç‚¹æ•°æ®å¤±è´¥: {e}")
                continue
        
        return signal_marks
    
    def _convert_backchi_to_echarts(self, backchi_analyses) -> List[Dict[str, Any]]:
        """è½¬æ¢èƒŒé©°åˆ†ææ•°æ®ä¸ºEChartsæ ‡è®°æ ¼å¼"""
        if not backchi_analyses:
            return []
        
        backchi_marks = []
        
        for idx, backchi in enumerate(backchi_analyses):
            try:
                if not backchi.is_valid_backchi():
                    continue
                
                timestamp = backchi.current_seg.end_time.strftime('%Y-%m-%d %H:%M')
                
                backchi_marks.append({
                    "id": f"backchi_{idx}",
                    "name": str(backchi.backchi_type),
                    "coord": [timestamp, float(backchi.current_seg.end_price)],
                    "value": float(backchi.current_seg.end_price),
                    "type": "top" if backchi.backchi_type == BackChi.TOP_BACKCHI else "bottom",
                    "reliability": float(backchi.reliability),
                    "strength_ratio": float(backchi.strength_ratio),
                    "macd_divergence": float(backchi.macd_divergence),
                    "symbol": "diamond",
                    "symbolSize": 10,
                    "itemStyle": {
                        "color": "#9c27b0"  # ç´«è‰²
                    }
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢èƒŒé©°æ•°æ®å¤±è´¥: {e}")
                continue
        
        return backchi_marks
    
    def _calculate_macd_from_klines(self, klines, categories: List[str]) -> Dict[str, List]:
        """åŸºäºKçº¿æ•°æ®è®¡ç®—MACDæŒ‡æ ‡"""
        try:
            if not klines or len(klines) == 0 or len(categories) == 0:
                return {"dif": [], "dea": [], "macd": []}
            
            # æå–æ”¶ç›˜ä»·
            close_prices = [float(kline.close) for kline in klines]
            
            if len(close_prices) < 26:  # MACDéœ€è¦è‡³å°‘26ä¸ªæ•°æ®ç‚¹
                return {"dif": [], "dea": [], "macd": []}
            
            # MACDå‚æ•°
            fast_period = 12
            slow_period = 26
            signal_period = 9
            
            # è®¡ç®—EMA
            def calculate_ema(values, period):
                multiplier = 2.0 / (period + 1)
                ema = [sum(values[:period]) / period]  # åˆå§‹å€¼ä½¿ç”¨ç®€å•å¹³å‡
                
                for i in range(period, len(values)):
                    ema.append((values[i] * multiplier) + (ema[-1] * (1 - multiplier)))
                
                return [0.0] * (period - 1) + ema
            
            # è®¡ç®—EMA12å’ŒEMA26
            ema12 = calculate_ema(close_prices, fast_period)
            ema26 = calculate_ema(close_prices, slow_period)
            
            # è®¡ç®—DIFçº¿
            dif = [ema12[i] - ema26[i] for i in range(len(ema12))]
            
            # è®¡ç®—DEAçº¿
            dea = calculate_ema(dif[slow_period-1:], signal_period)
            dea = [0.0] * (slow_period - 1) + dea
            
            # è®¡ç®—MACDæŸ±
            macd = [(dif[i] - dea[i]) * 2 for i in range(len(dif))]
            
            # ä¿ç•™ç²¾åº¦å¹¶å¯¹é½æ•°æ®é•¿åº¦
            min_length = min(len(categories), len(dif), len(dea), len(macd))
            
            return {
                "dif": [round(dif[i], 6) for i in range(min_length)],
                "dea": [round(dea[i], 6) for i in range(min_length)],
                "macd": [round(macd[i], 6) for i in range(min_length)]
            }
            
        except Exception as e:
            logger.error(f"è®¡ç®—MACDå¤±è´¥: {e}")
            return {"dif": [], "dea": [], "macd": []}
    
    def _convert_multi_level_to_frontend(self, results: Dict[TimeLevel, ChanAnalysisResult], 
                                       symbol: str, levels: List[str], days: int) -> Dict[str, Any]:
        """è½¬æ¢å¤šçº§åˆ«åˆ†æç»“æœä¸ºå‰ç«¯æ ¼å¼"""
        
        multi_level_data = {
            "meta": {
                "symbol": symbol,
                "levels": levels,
                "analysis_time": datetime.now().isoformat(),
                "days": days
            },
            "results": {},
            "comparison": {
                "level_consistency": {},
                "signal_confirmation": {},
                "trend_alignment": {}
            }
        }
        
        # è½¬æ¢å„çº§åˆ«ç»“æœ
        for time_level, result in results.items():
            level_str = time_level.value
            
            # è·å–åŸºç¡€æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…é‡å¤å¤§é‡Kçº¿æ•°æ®ï¼‰
            stats = result.get_statistics()
            
            multi_level_data["results"][level_str] = {
                "meta": {
                    "level": level_str,
                    "analysis_level": result.analysis_level.value,
                    "data_count": stats['processed_klines_count']
                },
                
                "structures": {
                    "fenxing_count": stats['fenxings_count'],
                    "bi_count": stats['bis_count'],
                    "seg_count": stats['segs_count'],
                    "zhongshu_count": stats['zhongshus_count']
                },
                
                "dynamics": {
                    "backchi_count": stats['backchi_count'],
                    "buy_sell_points_count": stats['buy_sell_points_count'],
                    "buy_points_count": stats['buy_points_count'],
                    "sell_points_count": stats['sell_points_count']
                },
                
                "evaluation": {
                    "trend_direction": result.trend_direction,
                    "trend_strength": result.trend_strength,
                    "risk_level": result.risk_level,
                    "confidence_score": result.confidence_score,
                    "recommended_action": result.recommended_action,
                    "level_consistency_score": result.level_consistency_score
                },
                
                "latest_signals": [
                    {
                        "type": str(point.point_type),
                        "price": point.price,
                        "timestamp": point.timestamp.isoformat(),
                        "reliability": point.reliability
                    }
                    for point in result.get_latest_signals(3)
                ]
            }
            
            # çº§åˆ«ä¸€è‡´æ€§åˆ†æ
            multi_level_data["comparison"]["level_consistency"][level_str] = result.level_consistency_score
        
        # è®¡ç®—çº§åˆ«é—´ä¿¡å·ç¡®è®¤
        self._analyze_signal_confirmation(multi_level_data, results)
        
        return multi_level_data
    
    def _analyze_signal_confirmation(self, multi_level_data: Dict, results: Dict[TimeLevel, ChanAnalysisResult]):
        """åˆ†æçº§åˆ«é—´ä¿¡å·ç¡®è®¤"""
        try:
            # è·å–æ‰€æœ‰çº§åˆ«çš„æœ€æ–°ä¿¡å·
            all_signals = {}
            for time_level, result in results.items():
                latest_signals = result.get_latest_signals(3)
                all_signals[time_level.value] = [
                    {
                        "type": str(point.point_type),
                        "timestamp": point.timestamp,
                        "reliability": point.reliability
                    }
                    for point in latest_signals
                ]
            
            # è®¡ç®—ä¿¡å·ä¸€è‡´æ€§
            signal_confirmation = {}
            level_names = list(all_signals.keys())
            
            for i, level1 in enumerate(level_names):
                for level2 in level_names[i+1:]:
                    key = f"{level1}_vs_{level2}"
                    signal_confirmation[key] = self._calculate_signal_similarity(
                        all_signals[level1], all_signals[level2]
                    )
            
            multi_level_data["comparison"]["signal_confirmation"] = signal_confirmation
            
        except Exception as e:
            logger.warning(f"åˆ†æä¿¡å·ç¡®è®¤å¤±è´¥: {e}")
    
    def _calculate_signal_similarity(self, signals1: List[Dict], signals2: List[Dict]) -> float:
        """è®¡ç®—ä¸¤ä¸ªçº§åˆ«ä¿¡å·çš„ç›¸ä¼¼åº¦"""
        if not signals1 or not signals2:
            return 0.0
        
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šåŸºäºä¿¡å·ç±»å‹å’Œæ—¶é—´æ¥è¿‘åº¦
        similarity_score = 0.0
        matches = 0
        
        for sig1 in signals1:
            for sig2 in signals2:
                # ç±»å‹åŒ¹é…
                if sig1["type"] == sig2["type"]:
                    # æ—¶é—´æ¥è¿‘ï¼ˆ7å¤©å†…ï¼‰
                    time_diff = abs((sig1["timestamp"] - sig2["timestamp"]).total_seconds())
                    if time_diff < 7 * 24 * 3600:  # 7å¤©
                        similarity_score += min(sig1["reliability"], sig2["reliability"])
                        matches += 1
        
        return similarity_score / max(matches, 1)
    
    def _convert_signals_to_frontend(self, signals: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢äº¤æ˜“ä¿¡å·ä¸ºå‰ç«¯æ ¼å¼"""
        
        frontend_signals = {
            "signals": [],
            "summary": signals.get("summary", {}),
            "timestamp": signals.get("timestamp", datetime.now()).isoformat() if hasattr(signals.get("timestamp"), 'isoformat') else str(signals.get("timestamp", datetime.now()))
        }
        
        for signal in signals.get("signals", []):
            try:
                frontend_signal = {
                    "type": signal.get("type"),
                    "point_type": signal.get("point_type", signal.get("type")),
                    "price": signal.get("price"),
                    "timestamp": signal.get("timestamp").isoformat() if hasattr(signal.get("timestamp"), 'isoformat') else str(signal.get("timestamp")),
                    "reliability": signal.get("reliability"),
                    "strength": signal.get("strength", 0.0),
                    "confirmed": signal.get("confirmed", False)
                }
                
                # å¤„ç†èƒŒé©°ä¿¡å·çš„ç‰¹æ®Šå­—æ®µ
                if signal.get("type") == "backchi":
                    frontend_signal.update({
                        "backchi_type": signal.get("backchi_type"),
                        "strength_ratio": signal.get("strength_ratio")
                    })
                
                frontend_signals["signals"].append(frontend_signal)
                
            except Exception as e:
                logger.warning(f"è½¬æ¢ä¿¡å·æ•°æ®å¤±è´¥: {e}")
                continue
        
        return frontend_signals
    
    def _get_time_level(self, timeframe: str) -> TimeLevel:
        """è·å–æ—¶é—´çº§åˆ«æšä¸¾"""
        mapping = {
            "5min": TimeLevel.MIN_5,
            "30min": TimeLevel.MIN_30,
            "daily": TimeLevel.DAILY
        }
        return mapping.get(timeframe, TimeLevel.DAILY)
    
    def _generate_empty_result(self, symbol: str, timeframe: str) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºçš„åˆ†æç»“æœ"""
        return {
            "meta": {
                "symbol": symbol,
                "timeframe": timeframe,
                "analysis_level": "complete",
                "analysis_time": datetime.now().isoformat(),
                "data_range": {"days": 0, "start_date": "", "end_date": ""},
                "data_count": 0
            },
            "chart_data": {
                "kline": {"categories": [], "values": [], "volumes": []},
                "indicators": {"macd": {"dif": [], "dea": [], "macd": []}},
                "chan_structures": {"fenxing": [], "bi": [], "seg": [], "zhongshu": []},
                "dynamics": {"buy_sell_points": [], "backchi": []}
            },
            "analysis": {
                "summary": {
                    "klines_original": 0, "klines_processed": 0,
                    "fenxing_count": 0, "bi_count": 0, "seg_count": 0, "zhongshu_count": 0,
                    "backchi_count": 0, "buy_sell_points_count": 0
                },
                "evaluation": {
                    "trend_direction": None, "trend_strength": 0.0,
                    "risk_level": 0.0, "confidence_score": 0.0,
                    "recommended_action": "wait"
                },
                "latest_signals": []
            },
            "chart_config": {
                "colors": {
                    "up_candle": "#ef5150", "down_candle": "#26a69a",
                    "fenxing_top": "#f44336", "fenxing_bottom": "#4caf50",
                    "bi_up": "#e91e63", "bi_down": "#2196f3",
                    "seg_up": "#ff5722", "seg_down": "#3f51b5",
                    "zhongshu": "#ff9800", "buy_point": "#4caf50", "sell_point": "#f44336"
                }
            }
        }
    
    def _generate_empty_multi_level_result(self, symbol: str, levels: List[str]) -> Dict[str, Any]:
        """ç”Ÿæˆç©ºçš„å¤šçº§åˆ«åˆ†æç»“æœ"""
        return {
            "meta": {
                "symbol": symbol,
                "levels": levels,
                "analysis_time": datetime.now().isoformat(),
                "days": 0
            },
            "results": {},
            "comparison": {
                "level_consistency": {},
                "signal_confirmation": {},
                "trend_alignment": {}
            }
        }
    
    def save_analysis_to_json(self, symbol: str, timeframe: str = "daily", 
                            days: int = 90, analysis_level: str = "complete",
                            output_file: str = None) -> str:
        """åˆ†æè‚¡ç¥¨å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶"""
        # æ‰§è¡Œåˆ†æ
        data = self.analyze_symbol_complete(symbol, timeframe, days, analysis_level)
        
        # ç”Ÿæˆæ–‡ä»¶å
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"chan_v2_data_{symbol}_{timeframe}_{analysis_level}_{timestamp}.json"
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ… ç¼ è®ºv2åˆ†ææ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            return output_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return ""


# åˆ›å»ºå…¨å±€APIå®ä¾‹
chan_api_v2 = ChanDataAPIv2()

if __name__ == '__main__':
    # å‘½ä»¤è¡Œä½¿ç”¨ç¤ºä¾‹
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¼ è®ºæ•°æ®API v2')
    parser.add_argument('--symbol', '-s', required=True, help='è‚¡ç¥¨ä»£ç ')
    parser.add_argument('--timeframe', '-t', default='daily', choices=['5min', '30min', 'daily'], help='æ—¶é—´çº§åˆ«')
    parser.add_argument('--days', '-d', type=int, default=90, help='åˆ†æå¤©æ•°')
    parser.add_argument('--level', '-l', default='complete', choices=['basic', 'standard', 'advanced', 'complete'], help='åˆ†æçº§åˆ«')
    parser.add_argument('--output', '-o', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--multi-level', '-m', action='store_true', help='å¤šçº§åˆ«åˆ†æ')
    
    args = parser.parse_args()
    
    print(f"ğŸ” ç¼ è®ºv2åˆ†æ {args.symbol} ({args.timeframe}, {args.days}å¤©, {args.level}çº§åˆ«)")
    
    if args.multi_level:
        # å¤šçº§åˆ«åˆ†æ
        data = chan_api_v2.analyze_multi_level(args.symbol, ["daily", "30min", "5min"], args.days)
        output_file = args.output or f"chan_v2_multi_level_{args.symbol}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
            
        print(f"ğŸ‰ å¤šçº§åˆ«åˆ†æå®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    else:
        # å•çº§åˆ«åˆ†æ
        output_file = chan_api_v2.save_analysis_to_json(
            args.symbol, args.timeframe, args.days, args.level, args.output
        )
        
        if output_file:
            print(f"ğŸ‰ åˆ†æå®Œæˆï¼æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"ğŸ“Š åŸºäºæœ€æ–°ç¼ è®ºv2å¼•æ“ï¼ˆå½¢æ€å­¦+åŠ¨åŠ›å­¦ï¼‰çš„å®Œæ•´åˆ†æ")
        else:
            print("âŒ åˆ†æå¤±è´¥")