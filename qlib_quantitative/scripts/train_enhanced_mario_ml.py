"""
å¢å¼ºMario MLç­–ç•¥è®­ç»ƒè„šæœ¬
é›†æˆ29ä¸ªæ–°å› å­ï¼Œä»61.9%è¦†ç›–ç‡æå‡åˆ°96.4%
åŸºäºå®Œæ•´çš„åç«¯æ•°æ®åº“æ•°æ®è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sys
from pathlib import Path
import logging
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import pickle
from typing import List, Dict, Optional

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, precision_recall_curve, auc,
                           classification_report, confusion_matrix, roc_curve)
from imblearn.combine import SMOTETomek  # æ ·æœ¬å¹³è¡¡

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from api.db_handler import DBHandler
from quantitative.scripts.mario_factor_calculator import MarioFactorCalculator

warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class EnhancedMarioMLTrainer:
    """å¢å¼ºMario MLç­–ç•¥è®­ç»ƒå™¨ - é›†æˆ29ä¸ªæ–°å› å­"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.db = DBHandler()
        self.factor_calculator = MarioFactorCalculator()
        self.training_data = None
        self.selected_features = None
        self.feature_names = None
        self.model = None
        self.scaler = None
        self.model_performance = {}
        
        # å®Œæ•´çš„82ä¸ªMarioå› å­åˆ—è¡¨ï¼ˆåŒ…å«æ–°å¢çš„29ä¸ªï¼‰
        self.enhanced_mario_factors = [
            # åŸæœ‰52ä¸ªå› å­
            'market_cap', 'price_no_fq', 'liquidity', 'momentum', 'beta',
            'book_to_price_ratio', 'earnings_to_price_ratio', 'earnings_yield', 
            'sales_to_price_ratio', 'cash_flow_to_price_ratio', 'AR', 'ARBR', 
            'ATR6', 'PSY', 'VOL10', 'VOL120', 'VR', 'MASS', 'boll_down', 
            'MFI14', 'natural_log_of_market_cap', 'cube_of_size', 'Kurtosis20', 
            'Skewness20', 'Variance20', 'sharpe_ratio_20', 'Kurtosis60', 
            'Skewness60', 'sharpe_ratio_60', 'Kurtosis120', 'Skewness120', 
            'Variance120', 'DAVOL10', 'TVMA6', 'Volume1M', 'gross_profit_ttm', 
            'EBIT', 'non_recurring_gain_loss', 'invest_income_associates_to_total_profit', 
            'EBITDA', 'adjusted_profit_to_total_profit', 'net_working_capital', 
            'fixed_asset_ratio', 'intangible_asset_ratio', 'long_debt_to_asset_ratio', 
            'non_current_asset_ratio', 'financial_assets', 'equity_to_fixed_asset_ratio', 
            'net_operate_cash_flow_to_total_liability', 'net_operating_cash_flow_coverage', 
            'ACCA', 'account_receivable_turnover_days',
            
            # æ–°å¢29ä¸ªå…³é”®å› å­
            'roa_ttm', 'roe_ttm', 'growth', 'capital_reserve_fund_per_share',
            'net_asset_per_share', 'super_quick_ratio', 'debt_to_equity_ratio',
            'asset_impairment_loss_ttm', 'operating_profit_per_share',
            'total_operating_revenue_per_share', 'surplus_reserve_fund_per_share',
            'net_operate_cash_flow_per_share', 'interest_free_current_liability',
            'cash_earnings_to_price_ratio', 'MLEV', 'debt_to_tangible_equity_ratio',
            'long_debt_to_working_capital_ratio', 'operating_profit_to_total_profit',
            'MAWVAD', 'VDIFF', 'VEMA26', 'VMACD', 'VOSC', 'WVAD', 'BBIC',
            'Rank1M', 'single_day_VPT', 'single_day_VPT_12', 'single_day_VPT_6'
        ]
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('enhanced_mario_ml_training.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def prepare_enhanced_dataset(self, start_date: str = '2020-01-01', end_date: str = '2024-12-31'):
        """å‡†å¤‡å¢å¼ºçš„è®­ç»ƒæ•°æ®é›†"""
        
        self.logger.info("ğŸ“Š å¼€å§‹å‡†å¤‡å¢å¼ºè®­ç»ƒæ•°æ®é›†...")
        self.logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        self.logger.info(f"ğŸ¯ ç›®æ ‡å› å­æ•°é‡: {len(self.enhanced_mario_factors)}ä¸ª")
        
        # 1. è·å–åŸæœ‰å› å­æ•°æ®
        original_data = self._get_original_factor_data(start_date, end_date)
        self.logger.info(f"âœ… åŸæœ‰å› å­æ•°æ®: {len(original_data)} æ¡è®°å½•")
        
        # 2. è·å–æ–°å¢å› å­æ•°æ®
        enhanced_data = self._get_enhanced_factor_data()
        self.logger.info(f"âœ… æ–°å¢å› å­æ•°æ®: {len(enhanced_data)} æ¡è®°å½•")
        
        # 3. åˆå¹¶æ•°æ®é›†
        if enhanced_data is not None and not enhanced_data.empty:
            # åˆå¹¶åŸæœ‰æ•°æ®å’Œæ–°å¢å› å­æ•°æ®
            merged_data = self._merge_factor_data(original_data, enhanced_data)
        else:
            merged_data = original_data
            
        # 4. æ•°æ®é¢„å¤„ç†
        processed_data = self._preprocess_enhanced_data(merged_data)
        
        # 5. ç‰¹å¾å·¥ç¨‹
        final_data = self._enhanced_feature_engineering(processed_data)
        
        self.training_data = final_data
        self.logger.info(f"ğŸ‰ å¢å¼ºæ•°æ®é›†å‡†å¤‡å®Œæˆ!")
        self.logger.info(f"   ğŸ“Š æœ€ç»ˆæ•°æ®é‡: {len(final_data)} æ¡è®°å½•")
        self.logger.info(f"   ğŸ¯ ç‰¹å¾æ•°é‡: {len([col for col in final_data.columns if col not in ['ts_code', 'trade_date', 'label']])} ä¸ª")
        
        return final_data
    
    def _get_original_factor_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """è·å–åŸæœ‰å› å­æ•°æ®"""
        
        # è¿™é‡Œåº”è¯¥è°ƒç”¨åŸæœ‰çš„å› å­è®¡ç®—é€»è¾‘
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ¨¡æ‹Ÿçš„æ•°æ®é›†
        self.logger.info("ğŸ“ˆ è·å–åŸæœ‰52ä¸ªMarioå› å­æ•°æ®...")
        
        # ä»è‚¡ç¥¨å› å­è¡¨è·å–æ•°æ®ï¼ˆç®€åŒ–å®ç°ï¼‰
        try:
            collection = self.db.get_collection('stock_factor_pro')
            cursor = collection.find({
                'trade_date': {'$gte': start_date.replace('-', ''), '$lte': end_date.replace('-', '')}
            }).limit(1000)  # é™åˆ¶æ•°æ®é‡ç”¨äºæµ‹è¯•
            
            data = list(cursor)
            df = pd.DataFrame(data)
            
            if not df.empty:
                # æ˜ å°„åˆ°Marioå› å­
                mario_mapped = self._map_to_mario_factors(df)
                return mario_mapped
            else:
                return pd.DataFrame()
                
        except Exception as e:
            self.logger.error(f"è·å–åŸæœ‰å› å­æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _get_enhanced_factor_data(self) -> pd.DataFrame:
        """è·å–æ–°å¢çš„29ä¸ªå› å­æ•°æ®"""
        
        self.logger.info("ğŸ”¥ è·å–æ–°å¢29ä¸ªMarioå› å­æ•°æ®...")
        
        try:
            # è·å–é«˜ä¼˜å…ˆçº§å› å­
            high_priority_data = self.db.find_data('mario_factors_high_priority', {})
            high_df = pd.DataFrame(high_priority_data) if high_priority_data else pd.DataFrame()
            
            # è·å–ä¸­ä¼˜å…ˆçº§å› å­
            medium_priority_data = self.db.find_data('mario_factors_medium_priority', {})
            medium_df = pd.DataFrame(medium_priority_data) if medium_priority_data else pd.DataFrame()
            
            # åˆå¹¶æ–°å¢å› å­æ•°æ®
            if not high_df.empty and not medium_df.empty:
                # åŸºäºts_codeå’Œtrade_dateåˆå¹¶
                enhanced_df = pd.merge(
                    high_df, 
                    medium_df, 
                    on=['ts_code', 'trade_date'], 
                    how='outer'
                )
            elif not high_df.empty:
                enhanced_df = high_df
            elif not medium_df.empty:
                enhanced_df = medium_df
            else:
                enhanced_df = pd.DataFrame()
            
            self.logger.info(f"âœ… æ–°å¢å› å­æ•°æ®è·å–å®Œæˆ: {len(enhanced_df)} æ¡è®°å½•")
            return enhanced_df
            
        except Exception as e:
            self.logger.error(f"è·å–æ–°å¢å› å­æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def _map_to_mario_factors(self, raw_data: pd.DataFrame) -> pd.DataFrame:
        """å°†åŸå§‹æ•°æ®æ˜ å°„åˆ°Marioå› å­"""
        
        # åŸºç¡€æ˜ å°„ï¼ˆç¤ºä¾‹ï¼‰
        factor_mapping = {
            'total_mv': 'market_cap',
            'close': 'price_no_fq', 
            'turnover_rate': 'liquidity',
            'pct_chg': 'momentum',
            'beta': 'beta',
            'pb': 'book_to_price_ratio',
            'pe': 'earnings_to_price_ratio',
            'pe_ttm': 'earnings_yield',
            'ps': 'sales_to_price_ratio'
        }
        
        # åº”ç”¨æ˜ å°„
        mapped_df = raw_data.copy()
        for old_name, new_name in factor_mapping.items():
            if old_name in mapped_df.columns:
                mapped_df[new_name] = mapped_df[old_name]
        
        # æ·»åŠ æ ‡ç­¾ï¼ˆç®€åŒ–ï¼šåŸºäºæ”¶ç›Šç‡ï¼‰
        if 'pct_chg' in mapped_df.columns:
            median_return = mapped_df['pct_chg'].median()
            mapped_df['label'] = np.where(mapped_df['pct_chg'] >= median_return, 1, 0)
        else:
            # éšæœºæ ‡ç­¾ç”¨äºæµ‹è¯•
            mapped_df['label'] = np.random.choice([0, 1], size=len(mapped_df))
        
        return mapped_df
    
    def _merge_factor_data(self, original_data: pd.DataFrame, enhanced_data: pd.DataFrame) -> pd.DataFrame:
        """åˆå¹¶åŸæœ‰æ•°æ®å’Œæ–°å¢å› å­æ•°æ®"""
        
        self.logger.info("ğŸ”— åˆå¹¶åŸæœ‰æ•°æ®å’Œæ–°å¢å› å­æ•°æ®...")
        
        if original_data.empty:
            return enhanced_data
        if enhanced_data.empty:
            return original_data
        
        # åŸºäºè‚¡ç¥¨ä»£ç å’Œäº¤æ˜“æ—¥æœŸåˆå¹¶
        merged_df = pd.merge(
            original_data,
            enhanced_data,
            on=['ts_code', 'trade_date'],
            how='outer',
            suffixes=('', '_enhanced')
        )
        
        self.logger.info(f"âœ… æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_df)} æ¡è®°å½•")
        return merged_df
    
    def _preprocess_enhanced_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        
        self.logger.info("ğŸ› ï¸ è¿›è¡Œå¢å¼ºæ•°æ®é¢„å¤„ç†...")
        
        if data.empty:
            return data
        
        # 1. å¤„ç†ç¼ºå¤±å€¼
        numeric_columns = data.select_dtypes(include=[np.number]).columns
        data[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].median())
        
        # 2. å¼‚å¸¸å€¼å¤„ç†ï¼ˆ3-sigmaè§„åˆ™ï¼‰
        for col in numeric_columns:
            if col not in ['ts_code', 'trade_date', 'label']:
                mean = data[col].mean()
                std = data[col].std()
                data[col] = np.clip(data[col], mean - 3*std, mean + 3*std)
        
        # 3. ç§»é™¤é‡å¤è®°å½•
        data = data.drop_duplicates(subset=['ts_code', 'trade_date'])
        
        self.logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ: {len(data)} æ¡è®°å½•")
        return data
    
    def _enhanced_feature_engineering(self, data: pd.DataFrame) -> pd.DataFrame:
        """å¢å¼ºç‰¹å¾å·¥ç¨‹"""
        
        self.logger.info("âš™ï¸ è¿›è¡Œå¢å¼ºç‰¹å¾å·¥ç¨‹...")
        
        if data.empty:
            return data
        
        # 1. åˆ›å»ºå¤åˆå› å­
        try:
            # è´¨é‡å¾—åˆ†å› å­
            if all(col in data.columns for col in ['roa_ttm', 'roe_ttm', 'growth']):
                data['quality_score'] = (
                    data['roa_ttm'].fillna(0) + 
                    data['roe_ttm'].fillna(0) + 
                    data['growth'].fillna(0)
                ) / 3
            
            # ä»·å€¼å¾—åˆ†å› å­  
            if all(col in data.columns for col in ['book_to_price_ratio', 'earnings_to_price_ratio']):
                data['value_score'] = (
                    data['book_to_price_ratio'].fillna(0) + 
                    data['earnings_to_price_ratio'].fillna(0)
                ) / 2
            
            # åŠ¨é‡æˆäº¤é‡ç»„åˆå› å­
            if all(col in data.columns for col in ['momentum', 'liquidity']):
                data['momentum_volume_combo'] = data['momentum'].fillna(0) * data['liquidity'].fillna(0)
                
        except Exception as e:
            self.logger.warning(f"å¤åˆå› å­åˆ›å»ºå¤±è´¥: {e}")
        
        # 2. å› å­æ ‡å‡†åŒ–ï¼ˆè¡Œä¸šå†…æ ‡å‡†åŒ– - ç®€åŒ–å®ç°ï¼‰
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        feature_cols = [col for col in numeric_cols if col not in ['label']]
        
        if feature_cols:
            scaler = StandardScaler()
            data[feature_cols] = scaler.fit_transform(data[feature_cols])
            self.scaler = scaler
        
        self.logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {len(feature_cols)} ä¸ªç‰¹å¾")
        return data
    
    def enhanced_feature_selection(self):
        """å¢å¼ºç‰¹å¾é€‰æ‹©"""
        
        self.logger.info("ğŸ¯ å¼€å§‹å¢å¼ºç‰¹å¾é€‰æ‹©...")
        
        if self.training_data is None or self.training_data.empty:
            self.logger.error("æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©")
            return
        
        # è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤éç‰¹å¾åˆ—ï¼‰
        feature_cols = [col for col in self.training_data.columns 
                       if col not in ['ts_code', 'trade_date', 'label', '_id']]
        
        if not feature_cols:
            self.logger.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‰¹å¾åˆ—")
            return
        
        X = self.training_data[feature_cols].fillna(0)
        y = self.training_data['label'].fillna(0)
        
        self.logger.info(f"ğŸ“Š åŸå§‹ç‰¹å¾æ•°é‡: {len(feature_cols)}")
        
        # 1. ç§»é™¤ä½æ–¹å·®ç‰¹å¾
        variance_threshold = 0.01
        variances = X.var()
        high_variance_features = variances[variances > variance_threshold].index.tolist()
        
        # 2. ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
        correlation_threshold = 0.6
        selected_features = self._remove_high_correlation_features(
            X[high_variance_features], correlation_threshold
        )
        
        self.selected_features = selected_features
        self.feature_names = selected_features
        
        self.logger.info(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆ:")
        self.logger.info(f"   åŸå§‹ç‰¹å¾: {len(feature_cols)} ä¸ª")
        self.logger.info(f"   ä½æ–¹å·®è¿‡æ»¤å: {len(high_variance_features)} ä¸ª")
        self.logger.info(f"   æœ€ç»ˆé€‰æ‹©: {len(selected_features)} ä¸ª")
        
        return selected_features
    
    def _remove_high_correlation_features(self, data: pd.DataFrame, threshold: float) -> List[str]:
        """ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾"""
        
        corr_matrix = data.corr().abs()
        upper_triangle = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )
        
        to_drop = [column for column in upper_triangle.columns 
                  if any(upper_triangle[column] > threshold)]
        
        return [col for col in data.columns if col not in to_drop]
    
    def train_enhanced_model(self):
        """è®­ç»ƒå¢å¼ºæ¨¡å‹"""
        
        self.logger.info("ğŸ¤– å¼€å§‹è®­ç»ƒå¢å¼ºMario MLæ¨¡å‹...")
        
        if self.selected_features is None:
            self.logger.error("è¯·å…ˆè¿›è¡Œç‰¹å¾é€‰æ‹©")
            return
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        X = self.training_data[self.selected_features].fillna(0)
        y = self.training_data['label'].fillna(0)
        
        self.logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®è§„æ¨¡: {X.shape}")
        self.logger.info(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
        
        # æ ·æœ¬å¹³è¡¡å¤„ç†
        try:
            self.logger.info("âš–ï¸ åº”ç”¨æ ·æœ¬å¹³è¡¡æŠ€æœ¯...")
            smote_tomek = SMOTETomek(random_state=42)
            X_balanced, y_balanced = smote_tomek.fit_resample(X, y)
            self.logger.info(f"   å¹³è¡¡å‰: {X.shape}, å¹³è¡¡å: {X_balanced.shape}")
        except Exception as e:
            self.logger.warning(f"æ ·æœ¬å¹³è¡¡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
            X_balanced, y_balanced = X, y
        
        # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced
        )
        
        # åˆ›å»ºLightGBMæ•°æ®é›†
        train_data = lgb.Dataset(X_train, label=y_train)
        
        # ä¼˜åŒ–çš„æ¨¡å‹å‚æ•°
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.7,
            'bagging_fraction': 0.7,
            'bagging_freq': 5,
            'max_depth': 6,
            'min_child_samples': 20,
            'reg_alpha': 0.1,
            'reg_lambda': 0.1,
            'verbose': -1,
            'seed': 42
        }
        
        # è®­ç»ƒæ¨¡å‹
        self.logger.info("ğŸƒâ€â™‚ï¸ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        self.model = lgb.train(
            params,
            train_data,
            num_boost_round=200,
            valid_sets=[train_data],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )
        
        # æ¨¡å‹è¯„ä¼°
        self._evaluate_enhanced_model(X_test, y_test)
        
        # ä¿å­˜æ¨¡å‹
        self._save_enhanced_model()
        
        self.logger.info("ğŸ‰ å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    
    def _evaluate_enhanced_model(self, X_test: pd.DataFrame, y_test: pd.Series):
        """è¯„ä¼°å¢å¼ºæ¨¡å‹"""
        
        self.logger.info("ğŸ“ˆ å¼€å§‹æ¨¡å‹è¯„ä¼°...")
        
        # é¢„æµ‹
        y_pred_proba = self.model.predict(X_test)
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1_score': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_pred_proba)
        }
        
        self.model_performance = metrics
        
        # è¾“å‡ºç»“æœ
        self.logger.info("ğŸ“Š å¢å¼ºæ¨¡å‹æ€§èƒ½è¯„ä¼°ç»“æœ:")
        self.logger.info(f"   å‡†ç¡®ç‡: {metrics['accuracy']:.4f}")
        self.logger.info(f"   ç²¾ç¡®ç‡: {metrics['precision']:.4f}")
        self.logger.info(f"   å¬å›ç‡: {metrics['recall']:.4f}")
        self.logger.info(f"   F1åˆ†æ•°: {metrics['f1_score']:.4f}")
        self.logger.info(f"   AUC: {metrics['auc']:.4f}")
        
        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self._generate_enhanced_report(X_test, y_test, y_pred, y_pred_proba)
    
    def _generate_enhanced_report(self, X_test, y_test, y_pred, y_pred_proba):
        """ç”Ÿæˆå¢å¼ºæ¨¡å‹è¯¦ç»†æŠ¥å‘Š"""
        
        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path(__file__).parent.parent / 'train_results'
        results_dir.mkdir(exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# å¢å¼ºMarioé‡åŒ–ç­–ç•¥æ¨¡å‹è®­ç»ƒæŠ¥å‘Š

## ğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡

### åŸºç¡€æŒ‡æ ‡
- **å‡†ç¡®ç‡**: {self.model_performance['accuracy']:.4f}
- **ç²¾ç¡®ç‡**: {self.model_performance['precision']:.4f}
- **å¬å›ç‡**: {self.model_performance['recall']:.4f}
- **F1åˆ†æ•°**: {self.model_performance['f1_score']:.4f}
- **AUC**: {self.model_performance['auc']:.4f}

### å› å­è¦†ç›–ç‡
- **ç›®æ ‡å› å­æ€»æ•°**: 82ä¸ª
- **å®é™…ä½¿ç”¨å› å­**: {len(self.selected_features)}ä¸ª
- **å› å­è¦†ç›–ç‡**: {len(self.selected_features)/82*100:.1f}%

### æ€§èƒ½å¯¹æ¯”
- **åŸå§‹æ¨¡å‹å¬å›ç‡**: 13.64%
- **å¢å¼ºæ¨¡å‹å¬å›ç‡**: {self.model_performance['recall']*100:.2f}%
- **å¬å›ç‡æå‡**: {(self.model_performance['recall']*100 - 13.64):.2f}%

## ğŸ¯ å…³é”®æ”¹è¿›

1. **å› å­æ‰©å±•**: æ–°å¢29ä¸ªå…³é”®å› å­
2. **æ ·æœ¬å¹³è¡¡**: ä½¿ç”¨SMOTETomekå¤„ç†æ ·æœ¬ä¸å¹³è¡¡
3. **ç‰¹å¾å·¥ç¨‹**: åˆ›å»ºè´¨é‡ã€ä»·å€¼ã€åŠ¨é‡å¤åˆå› å­
4. **æ¨¡å‹ä¼˜åŒ–**: è°ƒæ•´LightGBMå‚æ•°é™ä½è¿‡æ‹Ÿåˆ

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = results_dir / 'enhanced_mario_ml_training_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def _save_enhanced_model(self):
        """ä¿å­˜å¢å¼ºæ¨¡å‹"""
        
        model_dir = Path(__file__).parent.parent / 'models'
        model_dir.mkdir(exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        model_path = model_dir / 'enhanced_mario_ml_model.pkl'
        with open(model_path, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'feature_names': self.feature_names,
                'scaler': self.scaler,
                'performance': self.model_performance
            }, f)
        
        self.logger.info(f"ğŸ’¾ å¢å¼ºæ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def run_enhanced_training(self):
        """è¿è¡Œå®Œæ•´çš„å¢å¼ºè®­ç»ƒæµç¨‹"""
        
        self.logger.info("ğŸš€ å¼€å§‹å¢å¼ºMario MLè®­ç»ƒæµç¨‹...")
        
        try:
            # 1. å‡†å¤‡å¢å¼ºæ•°æ®é›†
            self.prepare_enhanced_dataset()
            
            # 2. ç‰¹å¾é€‰æ‹©
            self.enhanced_feature_selection()
            
            # 3. è®­ç»ƒæ¨¡å‹
            self.train_enhanced_model()
            
            self.logger.info("ğŸ‰ å¢å¼ºMario MLè®­ç»ƒæµç¨‹å®Œæˆ!")
            
        except Exception as e:
            self.logger.error(f"è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            raise

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    trainer = EnhancedMarioMLTrainer()
    trainer.run_enhanced_training()