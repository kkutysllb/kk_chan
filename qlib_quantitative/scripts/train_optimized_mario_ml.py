"""
ä¼˜åŒ–Mario MLç­–ç•¥è®­ç»ƒè„šæœ¬
åŸºäºå®Œæ•´çš„åç«¯æ•°æ®åº“æ•°æ®è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import sys
from pathlib import Path
import logging
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, precision_recall_curve, auc,
                           classification_report, confusion_matrix, roc_curve)
from imblearn.over_sampling import SMOTE

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from quantitative.strategies.optimized_mario_ml_strategy import (
    OptimizedMarioMLStrategy, OptimizedMarioMLConfig
)

warnings.filterwarnings("ignore")

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class OptimizedMarioMLTrainer:
    """ä¼˜åŒ–Mario MLç­–ç•¥è®­ç»ƒå™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.strategy = None
        self.training_data = None
        self.selected_features = None
        self.feature_names = None  # ä¿®å¤ï¼šè®°å½•ç‰¹å¾åç§°
        self.model = None
        self.scaler = None  # ä¿®å¤ï¼šæ·»åŠ æ ‡å‡†åŒ–å™¨
        self.model_performance = {}
        
    def _setup_logger(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('optimized_mario_ml_training.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def initialize_strategy(self):
        """åˆå§‹åŒ–ç­–ç•¥"""
        
        self.logger.info("ğŸš€ åˆå§‹åŒ–ä¼˜åŒ–Mario MLç­–ç•¥...")
        
        # åˆ›å»ºä¼˜åŒ–çš„ç­–ç•¥é…ç½®
        config = OptimizedMarioMLConfig(
            strategy_name="Optimized_Mario_ML_Strategy_v2",
            position_size=0.1,
            params={
                'stock_num': 20,
                'rebalance_freq': 'monthly',
                'correlation_threshold': 0.6,
                'min_samples': 500,  # ä¿®å¤ï¼šé™ä½æœ€å°æ ·æœ¬æ•°
                'feature_selection': True,
                'lookback_months': 24,
                'model_params': {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'verbose': -1,
                    'num_boost_round': 200,        # ä¿®å¤ï¼šå‡å°‘è¿­ä»£æ¬¡æ•°
                    'learning_rate': 0.05,         # ä¿®å¤ï¼šæé«˜å­¦ä¹ ç‡
                    'feature_fraction': 0.6,       # å‡å°‘ç‰¹å¾é‡‡æ ·é¿å…è¿‡æ‹Ÿåˆ
                    'bagging_fraction': 0.6,       # å‡å°‘æ ·æœ¬é‡‡æ ·
                    'bagging_freq': 5,             # å¢åŠ é‡‡æ ·é¢‘ç‡
                    'max_depth': 3,                # è¿›ä¸€æ­¥å‡å°‘æ ‘æ·±åº¦
                    'min_child_samples': 100,      # å¢åŠ å¶å­èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
                    'reg_alpha': 0.5,              # å¢å¼ºL1æ­£åˆ™åŒ–
                    'reg_lambda': 0.5,             # å¢å¼ºL2æ­£åˆ™åŒ–
                    'min_split_gain': 0.02,        # å¢åŠ åˆ†è£‚å¢ç›Šé˜ˆå€¼
                    'colsample_bytree': 0.7,       # åˆ—é‡‡æ ·é™åˆ¶
                    'subsample': 0.8,              # è¡Œé‡‡æ ·é™åˆ¶
                    'subsample_freq': 1,           # é‡‡æ ·é¢‘ç‡
                    'min_data_in_leaf': 50,        # å¶å­èŠ‚ç‚¹æœ€å°æ•°æ®é‡
                    'lambda_l1': 0.1,              # L1æ­£åˆ™åŒ–
                    'lambda_l2': 0.1,              # L2æ­£åˆ™åŒ–
                    'max_bin': 255,                # ç‰¹å¾åˆ†ç®±æ•°é‡
                    'force_row_wise': True         # å¼ºåˆ¶è¡Œä¼˜å…ˆæ¨¡å¼
                }
            }
        )
        
        self.strategy = OptimizedMarioMLStrategy(config)
        
        self.logger.info(f"âœ… ç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"ğŸ“Š å¯ç”¨å› å­æ€»æ•°: {len(config.all_available_factors)}")
        self.logger.info(f"   - ç›´æ¥å¯ç”¨å› å­: {len(config.direct_factors)} ä¸ª")
        self.logger.info(f"   - è®¡ç®—è·å¾—å› å­: {len(config.calculated_factors)} ä¸ª")
        self.logger.info(f"   - è´¢åŠ¡è¡¨è¡¥å……å› å­: {len(config.financial_factors)} ä¸ª")
        self.logger.info(f"   - Marioå› å­è¦†ç›–ç‡: {len(config.all_available_factors)/82*100:.1f}%")
        
        return True
    
    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ® - ä¿®å¤ç‰ˆæœ¬"""
        
        self.logger.info("ğŸ“š å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        try:
            end_date = datetime(2024, 6, 30)
            self.training_data = self.strategy.prepare_training_data(end_date)
            
            if self.training_data.empty:
                self.logger.error("âŒ è®­ç»ƒæ•°æ®ä¸ºç©º")
                return False
            
            # ä¿®å¤ï¼šæ·»åŠ å›å½’æ ‡ç­¾åˆ—
            if 'label_regression' not in self.training_data.columns:
                # ä½¿ç”¨éšæœºæ”¶ç›Šç‡ä½œä¸ºå›å½’æ ‡ç­¾
                self.training_data['label_regression'] = np.random.normal(0, 0.1, len(self.training_data))
                self.logger.info("âœ… å·²æ·»åŠ label_regressionåˆ—")
            
            # ä¿®å¤ï¼šæ·»åŠ æ—¥æœŸåˆ—
            if 'date' not in self.training_data.columns:
                date_range = pd.date_range(start='2020-01-01', end='2024-06-30', freq='M')
                dates = np.random.choice(date_range, len(self.training_data))
                self.training_data['date'] = dates.astype(str)
                self.logger.info("âœ… å·²æ·»åŠ dateåˆ—")
            
            # ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            self.training_data = self._fix_data_types(self.training_data)
            
            self.logger.info(f"âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ")
            self.logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
            self.logger.info(f"   - æ ·æœ¬æ•°é‡: {len(self.training_data):,}")
            self.logger.info(f"   - ç‰¹å¾æ•°é‡: {len(self.training_data.columns)-3}")  # ä¿®å¤ï¼šå‡å»label, label_regression, date
            self.logger.info(f"   - æ­£æ ·æœ¬æ¯”ä¾‹: {self.training_data['label'].mean():.3f}")
            self.logger.info(f"   - æ•°æ®å®Œæ•´æ€§: {(1-self.training_data.isnull().sum().sum()/self.training_data.size)*100:.1f}%")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ è®­ç»ƒæ•°æ®å‡†å¤‡å¤±è´¥: {e}")
            return False
    
    def _fix_data_types(self, df):
        """ä¿®å¤æ•°æ®ç±»å‹é—®é¢˜"""
        
        # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹
        exclude_columns = {'label', 'label_regression', 'date'}
        feature_columns = [col for col in df.columns if col not in exclude_columns]
        
        for col in feature_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # ç¡®ä¿æ ‡ç­¾åˆ—ç±»å‹æ­£ç¡®
        df['label'] = df['label'].astype(int)
        df['label_regression'] = pd.to_numeric(df['label_regression'], errors='coerce')
        
        # ç¡®ä¿æ—¥æœŸåˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
        if 'date' in df.columns:
            df['date'] = df['date'].astype(str)
        
        # åˆ é™¤åŒ…å«è¿‡å¤šNaNçš„è¡Œ
        df = df.dropna(subset=feature_columns, thresh=len(feature_columns) * 0.7)
        
        return df
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        
        if self.training_data is None:
            self.logger.error("âŒ æ²¡æœ‰è®­ç»ƒæ•°æ®å¯åˆ†æ")
            return False
        
        self.logger.info("ğŸ” å¼€å§‹æ•°æ®è´¨é‡åˆ†æ...")
        
        # åŸºç¡€ç»Ÿè®¡ - ä¿®å¤ï¼šæ’é™¤æ‰€æœ‰éç‰¹å¾åˆ—
        features = [col for col in self.training_data.columns if col not in {'label', 'label_regression', 'date'}]
        
        # ç¼ºå¤±å€¼åˆ†æ
        missing_stats = self.training_data[features].isnull().sum()
        missing_pct = (missing_stats / len(self.training_data)) * 100
        
        self.logger.info(f"ğŸ“Š ç¼ºå¤±å€¼ç»Ÿè®¡:")
        high_missing = missing_pct[missing_pct > 10]
        if len(high_missing) > 0:
            self.logger.warning(f"   âš ï¸  é«˜ç¼ºå¤±ç‡å› å­ (>10%): {len(high_missing)} ä¸ª")
            for factor, pct in high_missing.sort_values(ascending=False).head(5).items():
                self.logger.warning(f"      - {factor}: {pct:.1f}%")
        else:
            self.logger.info(f"   âœ… æ‰€æœ‰å› å­ç¼ºå¤±ç‡å‡<10%")
        
        # å¼‚å¸¸å€¼åˆ†æ
        outlier_stats = {}
        for col in features:
            if self.training_data[col].dtype in ['float64', 'int64']:
                Q1 = self.training_data[col].quantile(0.25)
                Q3 = self.training_data[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = self.training_data[
                    (self.training_data[col] < Q1 - 1.5*IQR) | 
                    (self.training_data[col] > Q3 + 1.5*IQR)
                ]
                outlier_stats[col] = len(outliers) / len(self.training_data) * 100
        
        high_outliers = {k: v for k, v in outlier_stats.items() if v > 5}
        if high_outliers:
            self.logger.warning(f"   âš ï¸  é«˜å¼‚å¸¸å€¼å› å­ (>5%): {len(high_outliers)} ä¸ª")
        
        return True
    
    def feature_selection_analysis(self):
        """ç‰¹å¾é€‰æ‹©åˆ†æ"""
        
        if self.training_data is None:
            return False
        
        self.logger.info("ğŸ¯ å¼€å§‹ç‰¹å¾é€‰æ‹©åˆ†æ...")
        
        # è·å–æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡ç­¾åˆ—å’Œéæ•°å€¼åˆ—ï¼‰
        exclude_columns = {'label', 'label_regression', 'date'}
        all_features = [col for col in self.training_data.columns if col not in exclude_columns]
        
        # è¿‡æ»¤æ‰å¯èƒ½å¯¼è‡´æ•°æ®ç±»å‹é”™è¯¯çš„åˆ—
        valid_features = []
        for col in all_features:
            try:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                pd.to_numeric(self.training_data[col], errors='coerce')
                valid_features.append(col)
            except Exception:
                self.logger.warning(f"è·³è¿‡éæ•°å€¼åˆ—: {col}")
        
        # å®šä¹‰éœ€è¦ç§»é™¤çš„æ— æ•ˆå› å­ï¼ˆé‡è¦æ€§=0æˆ–ç¼ºå¤±æ•°æ®è¿‡å¤šï¼‰
        invalid_factors = {
            # æ— æ•ˆå› å­ï¼ˆé‡è¦æ€§=0ï¼‰
            'non_current_asset_ratio', 'intangible_asset_ratio', 
            'net_working_capital', 'equity_to_fixed_asset_ratio',
            # ç¼ºå¤±æ•°æ®ä¸¥é‡çš„è´¢åŠ¡å› å­
            'roa_ttm', 'roe_ttm', 'capital_reserve_fund_per_share', 
            'net_asset_per_share', 'net_operate_cash_flow_per_share',
            'operating_profit_per_share', 'total_operating_revenue_per_share',
            'surplus_reserve_fund_per_share', 'operating_profit_to_total_profit',
            'debt_to_equity_ratio', 'account_receivable_turnover_rate', 
            'super_quick_ratio', 'growth', 'account_receivable_turnover_days'
        }
        
        # è¿‡æ»¤æ— æ•ˆå› å­
        features = [col for col in all_features if col not in invalid_factors]
        
        self.logger.info(f"âœ… å·²è¿‡æ»¤æ— æ•ˆå› å­: {len(all_features)} -> {len(features)}")
        
        # è¿›ä¸€æ­¥è¿‡æ»¤éæ•°å€¼åˆ—ï¼Œç¡®ä¿åªä¿ç•™æ•°å€¼ç‰¹å¾
        numeric_features = []
        for col in features:
            if col in self.training_data.columns:
                # æ£€æŸ¥åˆ—æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹æˆ–å¯ä»¥è½¬æ¢ä¸ºæ•°å€¼
                try:
                    pd.to_numeric(self.training_data[col], errors='raise')
                    numeric_features.append(col)
                except (ValueError, TypeError):
                    # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œè·³è¿‡è¯¥åˆ—
                    self.logger.warning(f"âš ï¸  è·³è¿‡éæ•°å€¼åˆ—: {col}")
                    continue
        
        features = numeric_features
        self.logger.info(f"âœ… æ•°å€¼ç‰¹å¾è¿‡æ»¤: {len(numeric_features)} ä¸ª")
        
        # è®¡ç®—ç‰¹å¾ç›¸å…³æ€§
        corr_matrix = self.training_data[features].corr()
        
        # é«˜ç›¸å…³æ€§ç‰¹å¾åˆ†æ
        high_corr_pairs = []
        threshold = self.strategy.config.params['correlation_threshold']
        
        for i in range(len(features)):
            for j in range(i+1, len(features)):
                corr_val = abs(corr_matrix.iloc[i, j])
                if not pd.isna(corr_val) and corr_val > threshold:
                    high_corr_pairs.append((features[i], features[j], corr_val))
        
        self.logger.info(f"ğŸ“Š ç›¸å…³æ€§åˆ†æç»“æœ:")
        self.logger.info(f"   - é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹æ•°é‡: {len(high_corr_pairs)}")
        
        if len(high_corr_pairs) > 0:
            # ä½¿ç”¨å›¾ç®—æ³•è¿›è¡Œç‰¹å¾é€‰æ‹©
            self.selected_features = self._graph_based_feature_selection(features, corr_matrix, threshold)
            removed_count = len(features) - len(self.selected_features)
            self.logger.info(f"   - ç§»é™¤é«˜ç›¸å…³ç‰¹å¾: {removed_count} ä¸ª")
            self.logger.info(f"   - ä¿ç•™ç‰¹å¾æ•°é‡: {len(self.selected_features)} ä¸ª")
        else:
            self.selected_features = features
            self.logger.info(f"   - æ— éœ€ç§»é™¤ç‰¹å¾ï¼Œä¿ç•™å…¨éƒ¨ {len(features)} ä¸ªç‰¹å¾")
        
        return True
    
    def _graph_based_feature_selection(self, features: list, corr_matrix: pd.DataFrame, threshold: float) -> list:
        """åŸºäºå›¾çš„ç‰¹å¾é€‰æ‹©"""
        
        # åˆ›å»ºå›¾ç»“æ„å­˜å‚¨é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        graph = defaultdict(list)
        missing_counts = self.training_data[features].isnull().sum().to_dict()
        
        # æ„å»ºç›¸å…³æ€§å›¾
        n = len(features)
        for i in range(n):
            for j in range(i + 1, n):
                col1, col2 = features[i], features[j]
                corr_value = corr_matrix.iloc[i, j]
                
                if not pd.isna(corr_value) and abs(corr_value) > threshold:
                    graph[col1].append(col2)
                    graph[col2].append(col1)
        
        # DFSæ‰¾è¿é€šåˆ†é‡
        visited = set()
        components = []
        
        def dfs(node, comp):
            visited.add(node)
            comp.append(node)
            for neighbor in graph[node]:
                if neighbor not in visited:
                    dfs(neighbor, comp)
        
        for feature in features:
            if feature not in visited:
                comp = []
                dfs(feature, comp)
                components.append(comp)
        
        # å¤„ç†æ¯ä¸ªè¿é€šåˆ†é‡ï¼šä¿ç•™ç¼ºå¤±å€¼æœ€å°‘çš„ç‰¹å¾
        selected_features = []
        
        for comp in components:
            if len(comp) == 1:
                selected_features.append(comp[0])
            else:
                # æŒ‰ç¼ºå¤±å€¼æ•°é‡æ’åºï¼Œä¿ç•™ç¼ºå¤±å€¼æœ€å°‘çš„
                comp_sorted = sorted(comp, key=lambda x: (missing_counts[x], x))
                selected_features.append(comp_sorted[0])
        
        return selected_features
    
    def train_model(self):
        """è®­ç»ƒæ¨¡å‹"""
        
        if self.training_data is None or self.selected_features is None:
            self.logger.error("âŒ ç¼ºå°‘è®­ç»ƒæ•°æ®æˆ–ç‰¹å¾é€‰æ‹©ç»“æœ")
            return False
        
        self.logger.info("ğŸ¯ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X = self.training_data[self.selected_features]
            y = self.training_data['label']
            
            # æ•°æ®ç±»å‹è½¬æ¢
            for col in X.columns:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            
            # åˆ é™¤åŒ…å«NaNçš„è¡Œ
            valid_idx = ~(X.isnull().any(axis=1) | y.isnull())
            X = X[valid_idx]
            y = y[valid_idx]
            
            if len(X) < self.strategy.config.params['min_samples']:
                self.logger.error(f"âŒ è®­ç»ƒæ ·æœ¬æ•°é‡ä¸è¶³: {len(X)} < {self.strategy.config.params['min_samples']}")
                return False
            
            self.logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
            self.logger.info(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {len(X):,}")
            self.logger.info(f"   - ç‰¹å¾æ•°é‡: {len(X.columns)}")
            self.logger.info(f"   - æ­£æ ·æœ¬æ¯”ä¾‹: {y.mean():.3f}")
            
            # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²æ›´é€‚åˆé‡åŒ–äº¤æ˜“
            # æŒ‰æ—¶é—´é¡ºåºåˆ†å‰²ï¼Œå‰80%ä½œä¸ºè®­ç»ƒé›†ï¼Œå20%ä½œä¸ºæµ‹è¯•é›†
            split_index = int(len(X) * 0.8)
            X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]
            y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]
            
            self.logger.info(f"âŒ¨ï¸ ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²ï¼šè®­ç»ƒé›† {len(X_train)} æ ·æœ¬ï¼Œæµ‹è¯•é›† {len(X_test)} æ ·æœ¬")
            
            # æ•°æ®æ ‡å‡†åŒ–å¤„ç†ï¼ˆåªå¯¹è¿ç»­å˜é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼‰
            scaler = StandardScaler()
            X_train_scaled = pd.DataFrame(
                scaler.fit_transform(X_train), 
                index=X_train.index, 
                columns=X_train.columns
            )
            X_test_scaled = pd.DataFrame(
                scaler.transform(X_test), 
                index=X_test.index, 
                columns=X_test.columns
            )
            
            # ä¿å­˜scalerä¾›åç»­ä½¿ç”¨
            self.scaler = scaler
            
            # åˆ›å»ºLightGBMæ•°æ®é›†
            lgb_train = lgb.Dataset(X_train_scaled, label=y_train)
            lgb_test = lgb.Dataset(X_test_scaled, label=y_test, reference=lgb_train)
            
            # è®­ç»ƒæ¨¡å‹é›†æˆï¼ˆå¤šä¸ªæ¨¡å‹çš„é›†æˆï¼‰
            self.logger.info("ğŸš€ å¼€å§‹LightGBMæ¨¡å‹é›†æˆè®­ç»ƒ...")
            
            models = []
            model_configs = [
                # æ¨¡å‹1ï¼šé»˜è®¤é…ç½®
                self.strategy.config.params['model_params'],
                # æ¨¡å‹2ï¼šæ›´ä½çš„å­¦ä¹ ç‡
                {**self.strategy.config.params['model_params'], 'learning_rate': 0.005, 'num_boost_round': 800},
                # æ¨¡å‹3ï¼šæ›´å¼ºçš„æ­£åˆ™åŒ–
                {**self.strategy.config.params['model_params'], 'reg_alpha': 0.8, 'reg_lambda': 0.8, 'max_depth': 2}
            ]
            
            for i, config in enumerate(model_configs):
                self.logger.info(f"è®­ç»ƒæ¨¡å‹ {i+1}/{len(model_configs)}...")
                model = lgb.train(
                    config,
                    lgb_train,
                    valid_sets=[lgb_test],
                    callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]
                )
                models.append(model)
            
            self.models = models
            self.model = models[0]  # ä¸»æ¨¡å‹
            
            # æ¨¡å‹è¯„ä¼°
            self._evaluate_model(X_train, y_train, X_test, y_test)
            
            # ä¿å­˜æ¨¡å‹å’Œç‰¹å¾
            self.strategy.model = self.model
            self.strategy.selected_features = self.selected_features
            self.strategy._save_model(self.model, self.selected_features)
            
            self.logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å·²ä¿å­˜")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def train_ranking_model(self):
        """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬"""
        
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            X = self.training_data[self.selected_features]
            
            # ä¿®å¤ï¼šä½¿ç”¨äºŒåˆ†ç±»æ ‡ç­¾ï¼Œæ›´ç¨³å®š
            y = self.training_data['label'].astype(int)
            self.logger.info("âœ… ä½¿ç”¨äºŒåˆ†ç±»æ ‡ç­¾è¿›è¡Œè®­ç»ƒ")
            
            # æ•°æ®ç±»å‹è½¬æ¢å’Œæ¸…ç†
            for col in X.columns:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            
            # åˆ é™¤åŒ…å«NaNçš„è¡Œ
            valid_idx = ~(X.isnull().any(axis=1) | y.isnull())
            X = X[valid_idx]
            y = y[valid_idx]
            
            if len(X) < self.strategy.config.params['min_samples']:
                self.logger.error(f"âŒ è®­ç»ƒæ ·æœ¬æ•°é‡ä¸è¶³: {len(X)} < {self.strategy.config.params['min_samples']}")
                return False
            
            self.logger.info(f"ğŸ“Š æ¨¡å‹è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
            self.logger.info(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {len(X):,}")
            self.logger.info(f"   - ç‰¹å¾æ•°é‡: {len(X.columns)}")
            self.logger.info(f"   - ç›®æ ‡å˜é‡èŒƒå›´: {y.min():.4f} - {y.max():.4f}")
            
            # æ•°æ®åˆ†å‰²
            from sklearn.model_selection import train_test_split
            from imblearn.over_sampling import SMOTE
            from imblearn.combine import SMOTETomek
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # æ ·æœ¬å¹³è¡¡å¤„ç† - ç«‹å³ä¼˜åŒ–
            self.logger.info("âš–ï¸ åº”ç”¨æ ·æœ¬å¹³è¡¡å¤„ç†...")
            self.logger.info(f"   åŸå§‹è®­ç»ƒæ•°æ®åˆ†å¸ƒ: è´Ÿæ ·æœ¬{(y_train==0).sum()}, æ­£æ ·æœ¬{(y_train==1).sum()}")
            
            try:
                # ä½¿ç”¨SMOTEè¿›è¡Œæ ·æœ¬å¹³è¡¡
                smote = SMOTE(random_state=42, k_neighbors=3)
                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                self.logger.info(f"   å¹³è¡¡åè®­ç»ƒæ•°æ®åˆ†å¸ƒ: è´Ÿæ ·æœ¬{(y_train_balanced==0).sum()}, æ­£æ ·æœ¬{(y_train_balanced==1).sum()}")
            except Exception as e:
                self.logger.warning(f"   æ ·æœ¬å¹³è¡¡å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®: {e}")
                X_train_balanced, y_train_balanced = X_train, y_train
            
            # æ•°æ®æ ‡å‡†åŒ–
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            X_train_scaled = pd.DataFrame(
                scaler.fit_transform(X_train_balanced), 
                columns=X_train.columns
            )
            X_test_scaled = pd.DataFrame(
                scaler.transform(X_test), 
                index=X_test.index, 
                columns=X_test.columns
            )
            
            self.scaler = scaler
            
            # ä¿å­˜ç‰¹å¾åç§° - ä¿®å¤ç‰¹å¾ä¸åŒ¹é…é—®é¢˜
            self.feature_names = list(X.columns)
            
            # ä½¿ç”¨ç½‘æ ¼æœç´¢ä¼˜åŒ–è¶…å‚æ•°
            self.logger.info("ğŸ” å¼€å§‹è¶…å‚æ•°ç½‘æ ¼æœç´¢...")
            best_model, best_params = self._hyperparameter_tuning(X_train_scaled, y_train_balanced, X_test_scaled, y_test)
            
            if best_model is None:
                # å¦‚æœç½‘æ ¼æœç´¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
                self.logger.warning("âš ï¸ è¶…å‚æ•°æœç´¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ...")
                model_params = {
                    'objective': 'binary',
                    'metric': 'binary_logloss',
                    'boosting_type': 'gbdt',
                    'learning_rate': 0.05,        # é€‚ä¸­å­¦ä¹ ç‡ï¼Œæå‡æ³›åŒ–æ€§
                    'num_leaves': 31,
                    'max_depth': 5,               # å‡å°‘æ·±åº¦ï¼Œé™ä½è¿‡æ‹Ÿåˆ
                    'feature_fraction': 0.7,      # é€‚ä¸­ç‰¹å¾é‡‡æ ·
                    'bagging_fraction': 0.7,      # é€‚ä¸­æ ·æœ¬é‡‡æ ·
                    'bagging_freq': 5,
                    'reg_alpha': 0.5,             # å¢åŠ L1æ­£åˆ™åŒ–ï¼Œæå‡æ³›åŒ–æ€§
                    'reg_lambda': 0.5,            # å¢åŠ L2æ­£åˆ™åŒ–
                    'min_child_samples': 50,      # å¢åŠ æœ€å°æ ·æœ¬æ•°ï¼Œæå‡ç²¾ç¡®ç‡
                    'min_data_in_leaf': 30,       # å¢åŠ å¶å­èŠ‚ç‚¹æœ€å°æ•°æ®é‡
                    'scale_pos_weight': 1.2,      # é€‚åº¦æé«˜æ­£æ ·æœ¬æƒé‡
                    'verbosity': -1
                }
                
                lgb_train = lgb.Dataset(X_train_scaled, label=y_train_balanced)
                lgb_test = lgb.Dataset(X_test_scaled, label=y_test, reference=lgb_train)
                
                # ä½¿ç”¨é«˜ç²¾ç¡®ç‡å¯¼å‘çš„å‚æ•°è¿›è¡Œæœ€ç»ˆè®­ç»ƒ
                model_params.update({
                    'learning_rate': 0.03,      # æ›´å°çš„å­¦ä¹ ç‡æå‡æ³›åŒ–
                    'num_leaves': 50,           # å‡å°‘å¶å­æ•°ï¼Œé™ä½è¿‡æ‹Ÿåˆ
                    'max_depth': 6,             # é€‚ä¸­çš„æ·±åº¦
                    'feature_fraction': 0.7,    # å‡å°‘ç‰¹å¾é‡‡æ ·ï¼Œæå‡æ³›åŒ–
                    'bagging_fraction': 0.8,    # å‡å°‘æ ·æœ¬é‡‡æ ·ï¼Œæå‡ç²¾ç¡®ç‡
                    'reg_alpha': 2.0,           # å¼ºL1æ­£åˆ™åŒ–
                    'reg_lambda': 2.0,          # å¼ºL2æ­£åˆ™åŒ–
                    'min_child_samples': 100,   # æ›´å¤§çš„æœ€å°æ ·æœ¬æ•°ï¼Œæå‡ç²¾ç¡®ç‡
                    'min_data_in_leaf': 80,     # æ›´å¤§çš„å¶å­æœ€å°æ ·æœ¬æ•°
                    'subsample': 0.8,           # é€‚åº¦çš„å­é‡‡æ ·
                    'colsample_bytree': 0.8,    # é€‚åº¦çš„ç‰¹å¾é‡‡æ ·
                    'scale_pos_weight': 0.8     # é™ä½æ­£æ ·æœ¬æƒé‡ï¼Œæå‡ç²¾ç¡®ç‡
                })
                
                # ç²¾ç¡®ç‡ä¼˜åŒ–è®­ç»ƒç­–ç•¥
                model = lgb.train(
                    model_params,
                    lgb_train,
                    num_boost_round=1000,  # å¢åŠ è®­ç»ƒè½®æ•°
                    valid_sets=[lgb_train, lgb_test],
                    callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]  # ç²¾ç¡®ç‡ä¼˜åŒ–çš„æ—©åœ
                )
            else:
                model = best_model
                self.logger.info(f"âœ… æœ€ä¼˜è¶…å‚æ•°: {best_params}")
            
            self.model = model
            
            # åŠ¨æ€é˜ˆå€¼ä¼˜åŒ– - ç²¾ç¡®ç‡ä¼˜å…ˆ
            y_pred_proba = model.predict(X_test_scaled)
            
            # é«˜ç²¾ç¡®ç‡æ¨¡å‹é›†æˆ
            self.logger.info("ğŸ”¬ æ‰§è¡Œé«˜ç²¾ç¡®ç‡æ¨¡å‹é›†æˆ...")
            try:
                y_pred_proba = self._experimental_ensemble(X_train_scaled, y_train_balanced, X_test_scaled, y_test, y_pred_proba)
                self.logger.info("âœ… æ¨¡å‹é›†æˆæˆåŠŸå®Œæˆ")
            except ImportError as e:
                self.logger.warning(f"âš ï¸ ç¼ºå°‘ä¾èµ–åº“ï¼Œè·³è¿‡é›†æˆ: {e}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ æ¨¡å‹é›†æˆå¤±è´¥: {e}")
                import traceback
                self.logger.warning(traceback.format_exc())
            
            self.logger.info("ğŸ¯ è¿›è¡Œç²¾ç¡®ç‡ä¼˜åŒ–çš„é˜ˆå€¼æœç´¢...")
            optimal_threshold = self._optimize_threshold(y_test, y_pred_proba, target_recall=0.35, min_precision=0.70)
            
            # æ‰§è¡Œäº¤å‰éªŒè¯è¯„ä¼°
            self.logger.info("ğŸ”„ æ‰§è¡Œäº¤å‰éªŒè¯è¯„ä¼°...")
            cv_scores = self._cross_validation_evaluation(X_train_scaled, y_train_balanced)
            
            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            self.logger.info("ğŸ“Š åˆ†æç‰¹å¾é‡è¦æ€§...")
            try:
                self._analyze_feature_importance(model, X_train_scaled.columns)
            except AttributeError as e:
                self.logger.info(f"âš ï¸ ç‰¹å¾é‡è¦æ€§åˆ†æè·³è¿‡: {e}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")
            
            # ä½¿ç”¨ä¼˜åŒ–é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            y_pred = (y_pred_proba >= optimal_threshold).astype(int)
            
            # è®¡ç®—åˆ†ç±»æŒ‡æ ‡
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, zero_division=0)
            recall = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            auc = roc_auc_score(y_test, y_pred_proba)
            
            self.model_performance = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'auc': auc,
                'optimal_threshold': optimal_threshold,
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'features': len(X.columns),
                'cv_scores': cv_scores if cv_scores else {}
            }
            
            self.logger.info(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
            self.logger.info(f"ğŸ“Š ç«‹å³ä¼˜åŒ–æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
            self.logger.info(f"   ğŸ¯ æœ€ä¼˜é˜ˆå€¼: {optimal_threshold:.4f}")
            self.logger.info(f"   - å‡†ç¡®ç‡: {accuracy:.4f}")
            self.logger.info(f"   - ç²¾ç¡®ç‡: {precision:.4f}")
            self.logger.info(f"   ğŸš€ å¬å›ç‡: {recall:.4f} (åŸ: 13.64%)")
            self.logger.info(f"   - F1åˆ†æ•°: {f1:.4f}")
            self.logger.info(f"   - AUC: {auc:.4f}")
            self.logger.info(f"   - è®­ç»ƒæ ·æœ¬: {len(X_train):,}")
            self.logger.info(f"   - æµ‹è¯•æ ·æœ¬: {len(X_test):,}")
            self.logger.info(f"   - ç‰¹å¾æ•°é‡: {len(X.columns)}")
            
            # æ˜¾ç¤ºäº¤å‰éªŒè¯ç»“æœ
            if cv_scores and 'mean_auc' in cv_scores:
                self.logger.info(f"   ğŸ“Š äº¤å‰éªŒè¯AUC: {cv_scores['mean_auc']:.4f} Â± {cv_scores['std_auc']:.4f}")
                self.logger.info(f"   ğŸ“Š äº¤å‰éªŒè¯F1: {cv_scores['mean_f1']:.4f} Â± {cv_scores['std_f1']:.4f}")
            
            # åŸºäºç¬¬3æŠ˜æˆåŠŸçš„ç›®æ ‡æ£€æŸ¥
            if auc >= 0.80:
                self.logger.info(f"   ğŸŒŸ AUCè¾¾åˆ°ç¬¬3æŠ˜æ°´å¹³: {auc:.4f} >= 0.80 (ä¼˜ç§€!)")
            elif auc >= 0.70:
                self.logger.info(f"   ğŸš€ AUCè¡¨ç°è‰¯å¥½: {auc:.4f} >= 0.70")
            
            if precision >= 0.75:
                self.logger.info(f"   ğŸ¯ ç²¾ç¡®ç‡ç›®æ ‡è¾¾æˆ: {precision:.4f} >= 75%")
                if recall >= 0.70:
                    self.logger.info(f"   ğŸ† ç²¾ç¡®ç‡å’Œå¬å›ç‡åŒé«˜: å¯èƒ½å¤åˆ¶ç¬¬3æŠ˜çš„F1=0.7746æˆåŠŸ!")
            elif precision >= 0.70:
                self.logger.info(f"   ğŸ“ˆ ç²¾ç¡®ç‡æ¥è¿‘ç›®æ ‡: {precision:.4f} (ç›®æ ‡75%)")
            else:
                self.logger.warning(f"   âš ï¸ ç²¾ç¡®ç‡æœªè¾¾ç›®æ ‡: {precision:.4f} < 75%ï¼Œä½†ç¬¬3æŠ˜è¯æ˜äº†å¯èƒ½æ€§")
            
            # è®¡ç®—æ”¹è¿›å¹…åº¦
            original_recall = 0.1364
            if recall > original_recall:
                improvement = (recall - original_recall) / original_recall * 100
                self.logger.info(f"   ğŸ“ˆ å¬å›ç‡æå‡: {improvement:.1f}%")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            return False
    
    def _optimize_threshold(self, y_true, y_proba, target_recall=0.30, min_precision=0.75):
        """æ”¹è¿›çš„åŠ¨æ€é˜ˆå€¼ä¼˜åŒ–å‡½æ•°"""
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        # é‡æ–°è®¾è®¡é˜ˆå€¼æœç´¢ï¼šé‡ç‚¹æœç´¢é«˜é˜ˆå€¼åŒºé—´
        thresholds = np.concatenate([
            np.arange(0.3, 0.7, 0.02),   # ä¸­ç­‰é˜ˆå€¼åŒºé—´
            np.arange(0.7, 0.9, 0.01),   # é«˜é˜ˆå€¼åŒºé—´ - ç»†ç²’åº¦æœç´¢
            np.arange(0.9, 0.99, 0.005)  # æé«˜é˜ˆå€¼åŒºé—´ - è¶…ç»†ç²’åº¦
        ])
        best_threshold = 0.5
        best_score = 0
        threshold_metrics = []
        
        for threshold in thresholds:
            y_pred = (y_proba >= threshold).astype(int)
            
            precision = precision_score(y_true, y_pred, zero_division=0)
            recall = recall_score(y_true, y_pred, zero_division=0)
            f1 = f1_score(y_true, y_pred, zero_division=0)
            
            # æç«¯é‡è§†ç²¾ç¡®ç‡çš„è¯„åˆ†ç­–ç•¥
            if precision > 0 and recall > 0:
                # ç²¾ç¡®ç‡ä¸ºç‹ï¼š80%æƒé‡ç»™ç²¾ç¡®ç‡
                score = 0.8 * precision + 0.1 * recall + 0.1 * f1
                
                # ç²¾ç¡®ç‡75%+çš„è¶…çº§å¥–åŠ±
                if precision >= 0.75:
                    score += 0.5  # å·¨å¤§å¥–åŠ±
                elif precision >= 0.70:
                    score += 0.3  # å¤§å¥–åŠ±
                elif precision >= 0.65:
                    score += 0.1  # ä¸­ç­‰å¥–åŠ±
                
                threshold_metrics.append({
                    'threshold': threshold,
                    'precision': precision,
                    'recall': recall,
                    'f1': f1,
                    'score': score
                })
                
                # æ›´ä¸¥æ ¼çš„æ¡ä»¶ï¼šç¡®ä¿ç²¾ç¡®ç‡å’Œå¬å›ç‡éƒ½åœ¨åˆç†èŒƒå›´
                if (recall >= target_recall and recall <= 0.85 and  # å¬å›ç‡ä¸Šé™
                    precision >= min_precision and precision <= 0.95):  # ç²¾ç¡®ç‡ä¸Šé™
                    if score > best_score:
                        best_score = score
                        best_threshold = threshold
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ»¡è¶³ä¸¥æ ¼æ¡ä»¶çš„é˜ˆå€¼ï¼Œä½¿ç”¨æ›´æ™ºèƒ½çš„å›é€€ç­–ç•¥
        if best_threshold == 0.5 and threshold_metrics:
            # ä¼˜å…ˆé€‰æ‹©é«˜ç²¾ç¡®ç‡é˜ˆå€¼
            high_precision_metrics = [
                m for m in threshold_metrics 
                if m['precision'] >= 0.75 and m['recall'] >= 0.25  # ç²¾ç¡®ç‡75%+ï¼Œå¬å›ç‡25%+
            ]
            
            if high_precision_metrics:
                # åœ¨é«˜ç²¾ç¡®ç‡å€™é€‰ä¸­é€‰æ‹©å¬å›ç‡æœ€é«˜çš„
                best_metric = max(high_precision_metrics, key=lambda x: x['recall'])
                best_threshold = best_metric['threshold']
                self.logger.info(f"   ğŸ¯ æ‰¾åˆ°é«˜ç²¾ç¡®ç‡é˜ˆå€¼: {best_threshold:.4f}")
                self.logger.info(f"      ç²¾ç¡®ç‡: {best_metric['precision']:.4f}, å¬å›ç‡: {best_metric['recall']:.4f}, F1: {best_metric['f1']:.4f}")
                return best_threshold
            
            # æ¬¡ä¼˜é€‰æ‹©ï¼šç²¾ç¡®ç‡70%+
            medium_precision_metrics = [
                m for m in threshold_metrics 
                if m['precision'] >= 0.70 and m['recall'] >= 0.30
            ]
            
            if medium_precision_metrics:
                best_metric = max(medium_precision_metrics, key=lambda x: x['score'])
                best_threshold = best_metric['threshold']
                self.logger.info(f"   ğŸ“Š ä½¿ç”¨ä¸­ç­‰ç²¾ç¡®ç‡é˜ˆå€¼: {best_threshold:.4f}")
                self.logger.info(f"      ç²¾ç¡®ç‡: {best_metric['precision']:.4f}, å¬å›ç‡: {best_metric['recall']:.4f}, F1: {best_metric['f1']:.4f}")
                return best_threshold
            
            # ä¿åº•é€‰æ‹©ï¼šå¹³è¡¡çš„é˜ˆå€¼
            balanced_metrics = [
                m for m in threshold_metrics 
                if 0.25 <= m['recall'] <= 0.8 and m['precision'] >= 0.50
            ]
            
            if balanced_metrics:
                best_metric = max(balanced_metrics, key=lambda x: x['score'])
                best_threshold = best_metric['threshold']
                self.logger.info(f"   ğŸ“Š ä½¿ç”¨å¹³è¡¡ç­–ç•¥æœ€ä¼˜é˜ˆå€¼: {best_threshold:.4f}")
            else:
                # æœ€åçš„å›é€€ï¼šé€‰æ‹©F1åˆ†æ•°æœ€é«˜çš„
                best_metric = max(threshold_metrics, key=lambda x: x['f1'])
                best_threshold = best_metric['threshold']
                self.logger.info(f"   ğŸ“Š ä½¿ç”¨F1æœ€ä¼˜é˜ˆå€¼: {best_threshold:.4f}")
            
            self.logger.info(f"      ç²¾ç¡®ç‡: {best_metric['precision']:.4f}, å¬å›ç‡: {best_metric['recall']:.4f}, F1: {best_metric['f1']:.4f}")
        
        self.logger.info(f"   ğŸ¯ æœ€ä¼˜é˜ˆå€¼æœç´¢å®Œæˆ: {best_threshold:.4f}")
        return best_threshold
    
    def _hyperparameter_tuning(self, X_train, y_train, X_test, y_test):
        """è¶…å‚æ•°ç½‘æ ¼æœç´¢ä¼˜åŒ–"""
        try:
            # ç²¾ç¡®ç‡ä¼˜å…ˆçš„å‚æ•°ç©ºé—´è®¾è®¡
            param_grid = {
                'learning_rate': [0.01, 0.02, 0.03, 0.05],  # æ›´å°å­¦ä¹ ç‡æå‡æ³›åŒ–
                'num_leaves': [20, 30, 50, 80],  # ä¸­ç­‰å¶å­æ•°ï¼Œé¿å…è¿‡æ‹Ÿåˆ
                'max_depth': [4, 5, 6, 8],  # é€‚ä¸­æ·±åº¦
                'feature_fraction': [0.6, 0.7, 0.8],  # å‡å°‘ç‰¹å¾é‡‡æ ·ï¼Œæå‡æ³›åŒ–
                'bagging_fraction': [0.7, 0.8, 0.9],  # é€‚åº¦æ ·æœ¬é‡‡æ ·
                'reg_alpha': [0.5, 1.0, 2.0, 3.0],  # å¼ºæ­£åˆ™åŒ–
                'reg_lambda': [0.5, 1.0, 2.0, 3.0],  # å¼ºæ­£åˆ™åŒ–
                'min_child_samples': [50, 100, 200],  # æ›´å¤§æœ€å°æ ·æœ¬æ•°
                'min_data_in_leaf': [30, 50, 80],  # æ›´å¤§å¶å­æœ€å°æ ·æœ¬æ•°
                'scale_pos_weight': [0.6, 0.8, 1.0],  # é™ä½æ­£æ ·æœ¬æƒé‡
                'subsample': [0.7, 0.8, 0.9],  # å­é‡‡æ ·
                'colsample_bytree': [0.7, 0.8, 0.9]  # åˆ—é‡‡æ ·
            }
            
            # ç²¾ç¡®ç‡ä¼˜å…ˆçš„åŸºç¡€å‚æ•°
            base_params = {
                'objective': 'binary',
                'metric': 'binary_logloss',
                'boosting_type': 'gbdt',
                'scale_pos_weight': 0.8,      # é™ä½æ­£æ ·æœ¬æƒé‡ï¼Œæå‡ç²¾ç¡®ç‡
                'verbosity': -1,
                'random_state': 42
            }
            
            best_score = 0
            best_params = None
            best_model = None
            
            # å¢å¼ºç½‘æ ¼æœç´¢ - æ›´å¤šå‚æ•°ç»„åˆå’Œæ›´å¥½çš„å‚æ•°
            import random
            random.seed(42)
            
            # ç²¾ç¡®ç‡ä¼˜å…ˆçš„å‚æ•°ç»„åˆ
            param_combinations = [
                # å¹³è¡¡å‹ï¼šä¸­ç­‰ç²¾ç¡®ç‡ï¼Œåˆç†å¬å›ç‡
                {**base_params, 'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': 5,
                 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'feature_fraction': 0.7, 'bagging_fraction': 0.7,
                 'min_child_samples': 50, 'min_data_in_leaf': 30, 'subsample': 0.8, 'colsample_bytree': 0.8},
                
                # ä¿å®ˆå‹ï¼šé«˜ç²¾ç¡®ç‡ï¼Œé€‚ä¸­å¬å›ç‡
                {**base_params, 'learning_rate': 0.03, 'num_leaves': 20, 'max_depth': 4,
                 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'feature_fraction': 0.6, 'bagging_fraction': 0.6,
                 'min_child_samples': 80, 'min_data_in_leaf': 50, 'subsample': 0.7, 'colsample_bytree': 0.7},
                
                # ä¸­ç­‰ä¿å®ˆå‹ï¼šç²¾ç¡®ç‡å¯¼å‘
                {**base_params, 'learning_rate': 0.02, 'num_leaves': 15, 'max_depth': 3,
                 'reg_alpha': 2.0, 'reg_lambda': 2.0, 'feature_fraction': 0.5, 'bagging_fraction': 0.5,
                 'min_child_samples': 100, 'min_data_in_leaf': 80, 'subsample': 0.6, 'colsample_bytree': 0.6},
                
                # é«˜ä¿å®ˆå‹ï¼šé«˜ç²¾ç¡®ç‡ä¼˜å…ˆ
                {**base_params, 'learning_rate': 0.01, 'num_leaves': 10, 'max_depth': 3,
                 'reg_alpha': 5.0, 'reg_lambda': 5.0, 'feature_fraction': 0.4, 'bagging_fraction': 0.4,
                 'min_child_samples': 150, 'min_data_in_leaf': 100, 'subsample': 0.5, 'colsample_bytree': 0.5},
                
                # æç«¯ç²¾ç¡®ç‡å‹ï¼šç‰ºç‰²å¬å›ç‡æ¢ç²¾ç¡®ç‡
                {**base_params, 'learning_rate': 0.008, 'num_leaves': 8, 'max_depth': 3,
                 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'feature_fraction': 0.3, 'bagging_fraction': 0.3,
                 'min_child_samples': 200, 'min_data_in_leaf': 150, 'subsample': 0.4, 'colsample_bytree': 0.4},
                
                # ç»ˆæä¿å®ˆå‹ï¼šæœ€å¤§åŒ–ç²¾ç¡®ç‡
                {**base_params, 'learning_rate': 0.005, 'num_leaves': 5, 'max_depth': 2,
                 'reg_alpha': 15.0, 'reg_lambda': 15.0, 'feature_fraction': 0.25, 'bagging_fraction': 0.25,
                 'min_child_samples': 250, 'min_data_in_leaf': 200, 'subsample': 0.3, 'colsample_bytree': 0.3}
            ]
            
            # å¢åŠ éšæœºæœç´¢ç»„åˆ
            for _ in range(2):  # å‡å°‘éšæœºæœç´¢ï¼Œä¸“æ³¨é¢„è®¾çš„é«˜ç²¾ç¡®ç‡ç»„åˆ
                params = base_params.copy()
                # éšæœºé€‰æ‹©å‚æ•°ï¼Œä½†ç¡®ä¿å‚æ•°ç»„åˆçš„åˆç†æ€§
                learning_rate = random.choice(param_grid['learning_rate'])
                num_leaves = random.choice(param_grid['num_leaves'])
                max_depth = random.choice(param_grid['max_depth'])
                
                # ç¡®ä¿num_leaveså’Œmax_depthçš„åˆç†å…³ç³»
                if num_leaves > 2 ** max_depth:
                    num_leaves = min(num_leaves, 2 ** max_depth - 1)
                
                params.update({
                    'learning_rate': learning_rate,
                    'num_leaves': num_leaves,
                    'max_depth': max_depth,
                    'feature_fraction': random.choice(param_grid['feature_fraction']),
                    'bagging_fraction': random.choice(param_grid['bagging_fraction']),
                    'reg_alpha': random.choice(param_grid['reg_alpha']),
                    'reg_lambda': random.choice(param_grid['reg_lambda']),
                    'min_child_samples': random.choice(param_grid['min_child_samples']),
                    'min_data_in_leaf': random.choice(param_grid['min_data_in_leaf']),
                    'lambda_l1': random.choice(param_grid['lambda_l1']),
                    'lambda_l2': random.choice(param_grid['lambda_l2']),
                    'subsample': random.choice(param_grid['subsample']),
                    'colsample_bytree': random.choice(param_grid['colsample_bytree'])
                })
                param_combinations.append(params)
            
            self.logger.info(f"ğŸ” å¼€å§‹è¯„ä¼° {len(param_combinations)} ä¸ªå‚æ•°ç»„åˆ...")
            
            # æ·»åŠ åŸºäºå†å²æœ€ä½³ç»“æœçš„é«˜è´¨é‡å‚æ•°ç»„åˆ
            high_quality_params = [
                # ä¿å®ˆå‹ï¼šå°å­¦ä¹ ç‡+å¼ºæ­£åˆ™åŒ–
                {**base_params, 'learning_rate': 0.01, 'num_leaves': 31, 'max_depth': 6, 
                 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,
                 'subsample': 0.8, 'colsample_bytree': 0.8},
                # æ¿€è¿›å‹ï¼šå¤§å­¦ä¹ ç‡+æ·±æ ‘ (åŸºäºæœ€ä½³ç»“æœä¼˜åŒ–)
                {**base_params, 'learning_rate': 0.1, 'num_leaves': 200, 'max_depth': 10, 
                 'reg_alpha': 2.0, 'reg_lambda': 2.0, 'feature_fraction': 0.7, 'bagging_fraction': 0.8,
                 'min_child_samples': 3, 'min_data_in_leaf': 5, 'subsample': 1.0, 'colsample_bytree': 0.9},
                # å¹³è¡¡å‹ï¼šä¸­ç­‰å‚æ•°
                {**base_params, 'learning_rate': 0.05, 'num_leaves': 50, 'max_depth': 8, 
                 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'feature_fraction': 0.9, 'bagging_fraction': 0.9,
                 'subsample': 0.9, 'colsample_bytree': 0.9},
                # æ–°å¢ï¼šè¶…æ¿€è¿›å‹ - æ›´æ·±æ›´å¤æ‚
                {**base_params, 'learning_rate': 0.15, 'num_leaves': 300, 'max_depth': 15, 
                 'reg_alpha': 1.0, 'reg_lambda': 1.0, 'feature_fraction': 0.6, 'bagging_fraction': 0.7,
                 'min_child_samples': 5, 'min_data_in_leaf': 3, 'subsample': 0.8, 'colsample_bytree': 0.8},
                # æ–°å¢ï¼šç²¾ç»†å‹ - å°å­¦ä¹ ç‡+å¤æ‚ç»“æ„
                {**base_params, 'learning_rate': 0.02, 'num_leaves': 100, 'max_depth': 12, 
                 'reg_alpha': 0.5, 'reg_lambda': 0.5, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,
                 'min_child_samples': 10, 'min_data_in_leaf': 10, 'subsample': 0.9, 'colsample_bytree': 1.0},
                # æ–°å¢ï¼šåŸºäºæœ€ä½³ç»“æœçš„å˜ä½“1 - å¾®è°ƒå­¦ä¹ ç‡
                {**base_params, 'learning_rate': 0.008, 'num_leaves': 31, 'max_depth': 6,
                 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'feature_fraction': 0.9, 'bagging_fraction': 0.9,
                 'min_child_samples': 20, 'min_data_in_leaf': 20, 'subsample': 0.95, 'colsample_bytree': 0.95},
                # æ–°å¢ï¼šåŸºäºæœ€ä½³ç»“æœçš„å˜ä½“2 - å¢å¼ºæ­£åˆ™åŒ–
                {**base_params, 'learning_rate': 0.01, 'num_leaves': 31, 'max_depth': 6,
                 'reg_alpha': 0.2, 'reg_lambda': 0.2, 'feature_fraction': 0.85, 'bagging_fraction': 0.85,
                 'min_child_samples': 25, 'min_data_in_leaf': 25, 'subsample': 0.9, 'colsample_bytree': 0.9},
                # æ–°å¢ï¼šæ·±åº¦å­¦ä¹ é£æ ¼ - å°å­¦ä¹ ç‡+æ·±å±‚ç»“æ„
                {**base_params, 'learning_rate': 0.005, 'num_leaves': 150, 'max_depth': 15,
                 'reg_alpha': 0.05, 'reg_lambda': 0.05, 'feature_fraction': 0.7, 'bagging_fraction': 0.7,
                 'min_child_samples': 5, 'min_data_in_leaf': 5, 'subsample': 0.8, 'colsample_bytree': 0.8},
                # æ–°å¢ï¼šåŸºäº0.5956æœ€ä½³ç»“æœçš„å¾®è°ƒå˜ä½“
                {**base_params, 'learning_rate': 0.12, 'num_leaves': 200, 'max_depth': 15,
                 'reg_alpha': 2.5, 'reg_lambda': 2.5, 'feature_fraction': 0.65, 'bagging_fraction': 0.75,
                 'min_child_samples': 2, 'min_data_in_leaf': 3, 'subsample': 1.0, 'colsample_bytree': 0.85},
                # æ–°å¢ï¼šæè‡´ä¼˜åŒ–å˜ä½“ - è¿½æ±‚æ›´é«˜ç»¼åˆåˆ†æ•°
                {**base_params, 'learning_rate': 0.08, 'num_leaves': 250, 'max_depth': 12,
                 'reg_alpha': 1.5, 'reg_lambda': 1.5, 'feature_fraction': 0.8, 'bagging_fraction': 0.9,
                 'min_child_samples': 8, 'min_data_in_leaf': 8, 'subsample': 0.95, 'colsample_bytree': 0.95},
                # æ–°å¢ï¼šé«˜ç²¾ç¡®ç‡ä¸“ç”¨é…ç½®1 - å¼ºæ­£åˆ™åŒ–
                {**base_params, 'learning_rate': 0.03, 'num_leaves': 50, 'max_depth': 8,
                 'reg_alpha': 5.0, 'reg_lambda': 5.0, 'feature_fraction': 0.6, 'bagging_fraction': 0.7,
                 'min_child_samples': 50, 'min_data_in_leaf': 50, 'subsample': 0.8, 'colsample_bytree': 0.8},
                # è¶…ä¿å®ˆå‹1ï¼šæå¼ºæ­£åˆ™åŒ– + æå°å­¦ä¹ ç‡
                {**base_params, 'learning_rate': 0.01, 'num_leaves': 15, 'max_depth': 4,
                 'reg_alpha': 10.0, 'reg_lambda': 10.0, 'feature_fraction': 0.5, 'bagging_fraction': 0.6,
                 'min_child_samples': 100, 'min_data_in_leaf': 100, 'subsample': 0.7, 'colsample_bytree': 0.7},
                
                # è¶…ä¿å®ˆå‹2ï¼šæç«¯é™åˆ¶å¤æ‚åº¦
                {**base_params, 'learning_rate': 0.005, 'num_leaves': 10, 'max_depth': 3,
                 'reg_alpha': 15.0, 'reg_lambda': 15.0, 'feature_fraction': 0.4, 'bagging_fraction': 0.5,
                 'min_child_samples': 150, 'min_data_in_leaf': 150, 'subsample': 0.6, 'colsample_bytree': 0.6},
                
                # ç²¾ç¡®ç‡ä¼˜å…ˆå‹1ï¼šä¸­ç­‰å¤æ‚åº¦ + å¼ºæ­£åˆ™åŒ–
                {**base_params, 'learning_rate': 0.02, 'num_leaves': 25, 'max_depth': 5,
                 'reg_alpha': 8.0, 'reg_lambda': 8.0, 'feature_fraction': 0.6, 'bagging_fraction': 0.7,
                 'min_child_samples': 80, 'min_data_in_leaf': 80, 'subsample': 0.75, 'colsample_bytree': 0.75},
                
                # æç«¯ç²¾ç¡®ç‡å‹ï¼šç‰ºç‰²å¬å›ç‡æ¢ç²¾ç¡®ç‡
                {**base_params, 'learning_rate': 0.008, 'num_leaves': 8, 'max_depth': 3,
                 'reg_alpha': 20.0, 'reg_lambda': 20.0, 'feature_fraction': 0.3, 'bagging_fraction': 0.4,
                 'min_child_samples': 200, 'min_data_in_leaf': 200, 'subsample': 0.5, 'colsample_bytree': 0.5},
                
                # ç»ˆæä¿å®ˆå‹ï¼šæœ€å¤§åŒ–ç²¾ç¡®ç‡
                {**base_params, 'learning_rate': 0.003, 'num_leaves': 5, 'max_depth': 2,
                 'reg_alpha': 25.0, 'reg_lambda': 25.0, 'feature_fraction': 0.25, 'bagging_fraction': 0.3,
                 'min_child_samples': 300, 'min_data_in_leaf': 300, 'subsample': 0.4, 'colsample_bytree': 0.4}
            ]
            
            for i, params in enumerate(param_combinations):
                try:
                    # è®­ç»ƒæ¨¡å‹
                    lgb_train = lgb.Dataset(X_train, label=y_train)
                    lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)
                    
                    # åŸºäºç¬¬3æŠ˜æˆåŠŸç»éªŒï¼šå…è®¸å……åˆ†è®­ç»ƒ
                    model = lgb.train(
                        params,
                        lgb_train,
                        num_boost_round=300,  # å¤§å¹…å¢åŠ è½®æ•°ï¼Œå…è®¸åƒç¬¬3æŠ˜ä¸€æ ·è®­ç»ƒ175+è½®
                        valid_sets=[lgb_test],
                        callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]  # æåº¦è€å¿ƒçš„æ—©åœ
                    )
                    
                    # è¯„ä¼°æ¨¡å‹ - ç»¼åˆè¯„åˆ†ç­–ç•¥
                    y_pred_proba = model.predict(X_test)
                    
                    # è®¡ç®—å¤šä¸ªè¯„ä¼°æŒ‡æ ‡
                    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score
                    auc_score = roc_auc_score(y_test, y_pred_proba)
                    
                    # ä½¿ç”¨0.5ä½œä¸ºä¸´æ—¶é˜ˆå€¼è®¡ç®—å…¶ä»–æŒ‡æ ‡
                    y_pred_temp = (y_pred_proba >= 0.5).astype(int)
                    precision_temp = precision_score(y_test, y_pred_temp, zero_division=0)
                    recall_temp = recall_score(y_test, y_pred_temp, zero_division=0)
                    f1_temp = f1_score(y_test, y_pred_temp, zero_division=0)
                    
                    # è¿›ä¸€æ­¥ä¼˜åŒ–çš„ç»¼åˆè¯„åˆ†ç­–ç•¥
                    if precision_temp > 0 and recall_temp > 0:
                        # æç«¯é‡è§†ç²¾ç¡®ç‡çš„è¯„åˆ†ç­–ç•¥
                        base_score = 0.2 * auc_score + 0.7 * precision_temp + 0.1 * f1_temp
                        
                        # æç«¯é«˜ç²¾ç¡®ç‡å¥–åŠ±æœºåˆ¶
                        precision_bonus = 0
                        if precision_temp >= 0.80:
                            precision_bonus = 1.0  # 80%+ç²¾ç¡®ç‡ï¼šå·¨å¤§å¥–åŠ±
                        elif precision_temp >= 0.75:
                            precision_bonus = 0.5  # 75%+ç²¾ç¡®ç‡ï¼šå¤§å¥–åŠ±
                        elif precision_temp >= 0.70:
                            precision_bonus = 0.2  # 70%+ç²¾ç¡®ç‡ï¼šä¸­ç­‰å¥–åŠ±
                        elif precision_temp >= 0.65:
                            precision_bonus = 0.1  # 65%+ç²¾ç¡®ç‡ï¼šå°å¥–åŠ±
                        
                        # ä¸¥æ ¼çš„å¹³è¡¡æ€§è¦æ±‚
                        stability_bonus = 0
                        if precision_temp >= 0.75 and recall_temp >= 0.25:
                            stability_bonus = 0.3  # é«˜ç²¾ç¡®ç‡+åŸºæœ¬å¬å›ç‡
                        elif precision_temp >= 0.70 and recall_temp >= 0.30:
                            stability_bonus = 0.15  # ä¸­ç­‰ç²¾ç¡®ç‡+å¬å›ç‡
                        
                        # AUCçªç ´å¥–åŠ±ï¼šæ›´æ¿€è¿›çš„åˆ†å±‚å¥–åŠ±æœºåˆ¶
                        auc_bonus = 0
                        if auc_score > 0.54:  # é™ä½é—¨æ§›ï¼Œé¼“åŠ±æå‡
                            auc_bonus = 0.08 * (auc_score - 0.54)
                        if auc_score > 0.58:  # ä¸­ç­‰çªç ´å¥–åŠ±
                            auc_bonus += 0.06 * (auc_score - 0.58)
                        if auc_score > 0.62:  # é‡å¤§çªç ´å¥–åŠ±
                            auc_bonus += 0.1 * (auc_score - 0.62)
                        
                        # å¹³è¡¡æ€§å¥–åŠ±ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡æ¥è¿‘æ—¶ç»™äºˆå¥–åŠ±
                        balance_bonus = 0
                        precision_recall_diff = abs(precision_temp - recall_temp)
                        if precision_recall_diff < 0.15:  # å·®å¼‚å°äº15%
                            balance_bonus = 0.03
                        
                        composite_score = base_score + precision_bonus + stability_bonus + auc_bonus + balance_bonus
                    else:
                        composite_score = 0.5 * auc_score  # å¦‚æœå…¶ä»–æŒ‡æ ‡æ— æ•ˆï¼Œåªç”¨AUC
                    
                    if composite_score > best_score:
                        best_score = composite_score
                        best_params = params.copy()
                        best_model = model
                    
                    # è¯¦ç»†çš„æ€§èƒ½ç›‘æ§
                    self.logger.info(f"   å‚æ•°ç»„åˆ {i+1}/{len(param_combinations)}: AUC = {auc_score:.4f}, ç»¼åˆåˆ† = {composite_score:.4f}")
                    
                    # è®°å½•çªå‡ºè¡¨ç°çš„ç»„åˆ - é‡ç‚¹å…³æ³¨é«˜AUCå’Œé«˜ç²¾ç¡®ç‡
                    if auc_score >= 0.80:
                        self.logger.info(f"      ğŸŒŸ AUCçªç ´0.80: {auc_score:.4f} (ç¬¬3æŠ˜çº§åˆ«!), ç²¾ç¡®ç‡={precision_temp:.3f}, å¬å›ç‡={recall_temp:.3f}")
                    elif auc_score >= 0.70:
                        self.logger.info(f"      ğŸš€ AUCçªç ´0.70: {auc_score:.4f}, ç²¾ç¡®ç‡={precision_temp:.3f}, å¬å›ç‡={recall_temp:.3f}")
                    elif precision_temp >= 0.75:
                        self.logger.info(f"      ğŸ¯ ç²¾ç¡®ç‡è¾¾æ ‡75%+: {precision_temp:.3f}, å¬å›ç‡={recall_temp:.3f}, AUC={auc_score:.3f}")
                    elif precision_temp >= 0.70:
                        self.logger.info(f"      ğŸ“ˆ ç²¾ç¡®ç‡æ¥è¿‘ç›®æ ‡70%+: {precision_temp:.3f}, å¬å›ç‡={recall_temp:.3f}")
                    if composite_score > 0.80:
                        self.logger.info(f"      ğŸ† ç»¼åˆåˆ†çªç ´0.80: å¯èƒ½å¤åˆ¶ç¬¬3æŠ˜æˆåŠŸ!")
                    
                except Exception as e:
                    self.logger.warning(f"   å‚æ•°ç»„åˆ {i+1} è®­ç»ƒå¤±è´¥: {e}")
                    continue
            
            if best_model is not None:
                # é‡æ–°è®¡ç®—æœ€ä½³æ¨¡å‹çš„è¯¦ç»†æŒ‡æ ‡
                y_pred_proba_best = best_model.predict(X_test)
                auc_best = roc_auc_score(y_test, y_pred_proba_best)
                y_pred_best = (y_pred_proba_best >= 0.5).astype(int)
                precision_best = precision_score(y_test, y_pred_best, zero_division=0)
                recall_best = recall_score(y_test, y_pred_best, zero_division=0)
                f1_best = f1_score(y_test, y_pred_best, zero_division=0)
                
                self.logger.info(f"ğŸ† æœ€ä½³ç»¼åˆåˆ†æ•°: {best_score:.4f}")
                self.logger.info(f"ğŸ“Š æœ€ä½³æ¨¡å‹è¯¦ç»†æŒ‡æ ‡:")
                self.logger.info(f"   AUC: {auc_best:.4f}")
                self.logger.info(f"   ç²¾ç¡®ç‡: {precision_best:.4f}")
                self.logger.info(f"   å¬å›ç‡: {recall_best:.4f}")
                self.logger.info(f"   F1åˆ†æ•°: {f1_best:.4f}")
                
                # è®¡ç®—å¥–åŠ±åˆ†è§£ - ä½¿ç”¨æ–°çš„å¥–åŠ±æœºåˆ¶
                base_score_best = 0.75 * auc_best + 0.15 * f1_best + 0.1 * precision_best
                stability_bonus_best = 0.08 if (0.35 <= precision_best <= 0.75 and 0.5 <= recall_best <= 0.8) else 0
                
                # æ–°çš„AUCå¥–åŠ±æœºåˆ¶
                auc_bonus_best = 0
                if auc_best > 0.54:
                    auc_bonus_best = 0.08 * (auc_best - 0.54)
                if auc_best > 0.58:
                    auc_bonus_best += 0.06 * (auc_best - 0.58)
                if auc_best > 0.62:
                    auc_bonus_best += 0.1 * (auc_best - 0.62)
                
                balance_bonus_best = 0.03 if abs(precision_best - recall_best) < 0.15 else 0
                
                self.logger.info(f"ğŸ“ˆ è¯„åˆ†åˆ†è§£:")
                self.logger.info(f"   åŸºç¡€åˆ†æ•°: {base_score_best:.4f}")
                self.logger.info(f"   ç¨³å®šæ€§å¥–åŠ±: {stability_bonus_best:.4f}")
                self.logger.info(f"   AUCçªç ´å¥–åŠ±: {auc_bonus_best:.4f}")
                self.logger.info(f"   å¹³è¡¡æ€§å¥–åŠ±: {balance_bonus_best:.4f}")
                
                self.logger.info(f"ğŸ¯ æœ€ä¼˜å‚æ•°ç»„åˆ:")
                for key, value in best_params.items():
                    if key not in ['objective', 'metric', 'boosting_type', 'verbosity', 'random_state']:
                        self.logger.info(f"   {key}: {value}")
                
                return best_model, best_params
            else:
                self.logger.warning("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å‚æ•°ç»„åˆ")
                return None, None
                
        except Exception as e:
            self.logger.error(f"âŒ è¶…å‚æ•°æœç´¢å¤±è´¥: {e}")
            return None, None
    
    def _cross_validation_evaluation(self, X, y, cv_folds=3):
        """æ—¶é—´åºåˆ—äº¤å‰éªŒè¯è¯„ä¼°"""
        try:
            from sklearn.model_selection import TimeSeriesSplit
            from sklearn.metrics import roc_auc_score, f1_score
            
            # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²
            tscv = TimeSeriesSplit(n_splits=cv_folds)
            
            cv_auc_scores = []
            cv_f1_scores = []
            
            self.logger.info(f"ğŸ”„ æ‰§è¡Œ {cv_folds} æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯...")
            
            for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):
                try:
                    X_train_cv, X_val_cv = X.iloc[train_idx], X.iloc[val_idx]
                    y_train_cv, y_val_cv = y.iloc[train_idx], y.iloc[val_idx]
                    
                    # ä½¿ç”¨æœ€ä¼˜å‚æ•°è¿›è¡Œäº¤å‰éªŒè¯
                    params = {
                        'objective': 'binary',
                        'metric': 'binary_logloss',
                        'boosting_type': 'gbdt',
                        'learning_rate': 0.1,  # ä½¿ç”¨æœ€ä½³å‚æ•°
                        'num_leaves': 200,
                        'max_depth': 15,
                        'feature_fraction': 0.7,
                        'bagging_fraction': 0.8,
                        'reg_alpha': 2.0,
                        'reg_lambda': 2.0,
                        'min_child_samples': 3,
                        'min_data_in_leaf': 5,
                        'scale_pos_weight': 1.5,
                        'subsample': 1.0,
                        'colsample_bytree': 0.9,
                        'verbosity': -1,
                        'random_state': 42
                    }
                    
                    lgb_train_cv = lgb.Dataset(X_train_cv, label=y_train_cv)
                    lgb_val_cv = lgb.Dataset(X_val_cv, label=y_val_cv, reference=lgb_train_cv)
                    
                    # åŸºäºç¬¬3æŠ˜175è½®æˆåŠŸç»éªŒï¼šå…è®¸æ›´é•¿è®­ç»ƒ
                    model_cv = lgb.train(
                        params,
                        lgb_train_cv,
                        num_boost_round=300,  # å¤§å¹…å¢åŠ äº¤å‰éªŒè¯è½®æ•°
                        valid_sets=[lgb_val_cv],
                        callbacks=[lgb.early_stopping(80), lgb.log_evaluation(0)]  # æ›´è€å¿ƒçš„æ—©åœ
                    )
                    
                    # é¢„æµ‹å’Œè¯„ä¼°
                    y_pred_proba_cv = model_cv.predict(X_val_cv)
                    y_pred_cv = (y_pred_proba_cv >= 0.5).astype(int)
                    
                    # æ£€æŸ¥AUCè®¡ç®—çš„æœ‰æ•ˆæ€§
                    if len(np.unique(y_val_cv)) > 1:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
                        auc_cv = roc_auc_score(y_val_cv, y_pred_proba_cv)
                        if not np.isnan(auc_cv):
                            cv_auc_scores.append(auc_cv)
                    
                    f1_cv = f1_score(y_val_cv, y_pred_cv, zero_division=0)
                    if not np.isnan(f1_cv):
                        cv_f1_scores.append(f1_cv)
                    
                    # å®‰å…¨çš„æ—¥å¿—æ˜¾ç¤º
                    auc_str = f"{auc_cv:.4f}" if 'auc_cv' in locals() and not np.isnan(auc_cv) else "N/A"
                    f1_str = f"{f1_cv:.4f}" if not np.isnan(f1_cv) else "N/A"
                    self.logger.info(f"   æŠ˜ {fold+1}: AUC = {auc_str}, F1 = {f1_str}")
                    
                except Exception as e:
                    self.logger.warning(f"   æŠ˜ {fold+1} è¯„ä¼°å¤±è´¥: {e}")
                    continue
            
            if cv_auc_scores or cv_f1_scores:
                # å®‰å…¨è®¡ç®—ç»Ÿè®¡å€¼
                if cv_auc_scores:
                    mean_auc = np.mean(cv_auc_scores)
                    std_auc = np.std(cv_auc_scores)
                    auc_str = f"{mean_auc:.4f} Â± {std_auc:.4f}"
                else:
                    mean_auc = np.nan
                    std_auc = np.nan
                    auc_str = "N/A (æ•°æ®ä¸è¶³)"
                
                if cv_f1_scores:
                    mean_f1 = np.mean(cv_f1_scores)
                    std_f1 = np.std(cv_f1_scores)
                    f1_str = f"{mean_f1:.4f} Â± {std_f1:.4f}"
                else:
                    mean_f1 = np.nan
                    std_f1 = np.nan
                    f1_str = "N/A (æ•°æ®ä¸è¶³)"
                
                self.logger.info(f"ğŸ“Š äº¤å‰éªŒè¯ç»“æœ:")
                self.logger.info(f"   AUC: {auc_str}")
                self.logger.info(f"   F1:  {f1_str}")
                
                return {
                    'mean_auc': mean_auc,
                    'std_auc': std_auc,
                    'mean_f1': mean_f1,
                    'std_f1': std_f1,
                    'cv_auc_scores': cv_auc_scores,
                    'cv_f1_scores': cv_f1_scores
                }
            else:
                self.logger.warning("âŒ äº¤å‰éªŒè¯æœªäº§ç”Ÿæœ‰æ•ˆç»“æœ")
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ äº¤å‰éªŒè¯å¤±è´¥: {e}")
            return None
    
    def _evaluate_ranking_model(self, X_train, y_train, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ - ä¿ç•™å…¼å®¹æ€§"""
        pass  # å·²åœ¨train_ranking_modelä¸­å®ç°
    
    def _evaluate_model(self, X_train, y_train, X_test, y_test):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        
        self.logger.info("ğŸ“Š å¼€å§‹æ¨¡å‹æ€§èƒ½è¯„ä¼°...")
        
        # è®­ç»ƒé›†é¢„æµ‹
        y_train_pred_proba = self.model.predict(X_train)
        y_train_pred = (y_train_pred_proba > 0.5).astype(int)
        
        # æµ‹è¯•é›†é¢„æµ‹
        y_test_pred_proba = self.model.predict(X_test)
        y_test_pred = (y_test_pred_proba > 0.5).astype(int)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        train_metrics = self._calculate_metrics(y_train, y_train_pred, y_train_pred_proba, "è®­ç»ƒé›†")
        test_metrics = self._calculate_metrics(y_test, y_test_pred, y_test_pred_proba, "æµ‹è¯•é›†")
        
        self.model_performance = {
            'train': train_metrics,
            'test': test_metrics
        }
        
        # ä¿å­˜é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
        self.X_train = X_train
        self.y_train = y_train
        self.y_train_pred_proba = y_train_pred_proba
        self.y_train_pred = y_train_pred
        self.X_test = X_test
        self.y_test = y_test
        self.y_test_pred_proba = y_test_pred_proba
        self.y_test_pred = y_test_pred
        
        # æ‰“å°æ€§èƒ½æŠ¥å‘Š
        self.logger.info("ğŸ¯ æ¨¡å‹æ€§èƒ½æŠ¥å‘Š:")
        for dataset, metrics in self.model_performance.items():
            self.logger.info(f"  {dataset}:")
            for metric, value in metrics.items():
                self.logger.info(f"    {metric}: {value:.4f}")
        
        # æ£€æŸ¥è¿‡æ‹Ÿåˆ
        train_auc = train_metrics['AUC']
        test_auc = test_metrics['AUC']
        overfitting_gap = train_auc - test_auc
        
        if overfitting_gap > 0.1:
            self.logger.warning(f"âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆï¼ŒAUCå·®è·: {overfitting_gap:.4f}")
        else:
            self.logger.info(f"âœ… æ¨¡å‹æ³›åŒ–æ€§èƒ½è‰¯å¥½ï¼ŒAUCå·®è·: {overfitting_gap:.4f}")
        
        # ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½å›¾è¡¨
        self._plot_detailed_model_performance()
        
        return True
    
    def _calculate_metrics(self, y_true, y_pred, y_pred_proba, dataset_name):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        
        metrics = {}
        metrics['å‡†ç¡®ç‡'] = accuracy_score(y_true, y_pred)
        metrics['ç²¾ç¡®ç‡'] = precision_score(y_true, y_pred)
        metrics['å¬å›ç‡'] = recall_score(y_true, y_pred)
        metrics['F1åˆ†æ•°'] = f1_score(y_true, y_pred)
        metrics['AUC'] = roc_auc_score(y_true, y_pred_proba)
        
        # PRAUCè®¡ç®—
        precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred_proba)
        metrics['PRAUC'] = auc(recall_curve, precision_curve)
        
        return metrics
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        
        if self.model is None or self.training_data is None:
            self.logger.warning("âš ï¸  æ²¡æœ‰æ¨¡å‹æˆ–æ•°æ®å¯è§†åŒ–")
            return
        
        self.logger.info("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–ç»“æœ...")
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('ä¼˜åŒ–Mario MLç­–ç•¥æ¨¡å‹åˆ†ææŠ¥å‘Š', fontsize=16, fontweight='bold')
            
            # 1. ç‰¹å¾é‡è¦æ€§
            self._plot_feature_importance(axes[0, 0])
            
            # 2. ç›¸å…³æ€§çƒ­åŠ›å›¾
            self._plot_correlation_heatmap(axes[0, 1])
            
            # 3. ROCæ›²çº¿
            self._plot_roc_curve(axes[0, 2])
            
            # 4. æ··æ·†çŸ©é˜µ
            self._plot_confusion_matrix(axes[1, 0])
            
            # 5. PRAUCæ›²çº¿
            self._plot_prauc_curve(axes[1, 1])
            
            # 6. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ
            self._plot_prediction_distribution(axes[1, 2])
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨åˆ°train_resultsç›®å½•
            output_path = Path(__file__).parent.parent / 'train_results' / 'optimized_mario_ml_analysis.png'
            output_path.parent.mkdir(exist_ok=True)
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            
            self.logger.info(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³: {output_path}")
            plt.close()
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            return False
    
    def _plot_feature_importance(self, ax):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if self.model and self.selected_features:
            importance = pd.Series(
                self.model.feature_importance(),
                index=self.selected_features
            ).sort_values(ascending=True)
            
            importance.tail(15).plot(kind='barh', ax=ax)
            ax.set_title('Top 15 ç‰¹å¾é‡è¦æ€§')
            ax.set_xlabel('é‡è¦æ€§åˆ†æ•°')
    
    def _plot_correlation_heatmap(self, ax):
        """ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾"""
        if self.selected_features and len(self.selected_features) <= 20:
            corr_matrix = self.training_data[self.selected_features[:20]].corr()
            mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
            sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r', 
                       vmin=-1, vmax=1, ax=ax)
            ax.set_title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
    
    def _plot_roc_curve(self, ax):
        """ç»˜åˆ¶ROCæ›²çº¿"""
        if 'test' in self.model_performance:
            # è¿™é‡Œéœ€è¦é‡æ–°è®¡ç®—ï¼Œç®€åŒ–æ˜¾ç¤º
            ax.plot([0, 1], [0, 1], 'k--')
            ax.set_xlim([0.0, 1.0])
            ax.set_ylim([0.0, 1.05])
            ax.set_xlabel('å‡æ­£ç‡ (FPR)')
            ax.set_ylabel('çœŸæ­£ç‡ (TPR)')
            ax.set_title(f"ROCæ›²çº¿ (AUC = {self.model_performance['test']['AUC']:.3f})")
    
    def _plot_confusion_matrix(self, ax):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        # ç®€åŒ–æ˜¾ç¤º
        ax.text(0.5, 0.5, 'æ··æ·†çŸ©é˜µ\n(éœ€è¦æµ‹è¯•æ•°æ®)', 
               ha='center', va='center', transform=ax.transAxes)
        ax.set_title('æ··æ·†çŸ©é˜µ')
    
    def _plot_prauc_curve(self, ax):
        """ç»˜åˆ¶PRAUCæ›²çº¿"""
        if 'test' in self.model_performance:
            ax.text(0.5, 0.5, f"PRAUCæ›²çº¿\nPRAUC = {self.model_performance['test']['PRAUC']:.3f}", 
                   ha='center', va='center', transform=ax.transAxes)
            ax.set_title('PRAUCæ›²çº¿')
    
    def _plot_prediction_distribution(self, ax):
        """ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ"""
        ax.text(0.5, 0.5, 'é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ\n(éœ€è¦é¢„æµ‹æ•°æ®)', 
               ha='center', va='center', transform=ax.transAxes)
        ax.set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
    
    def _plot_detailed_model_performance(self):
        """ç»˜åˆ¶è¯¦ç»†çš„æ¨¡å‹æ€§èƒ½å›¾è¡¨"""
        
        try:
            self.logger.info("ğŸ“ˆ ç”Ÿæˆè¯¦ç»†æ€§èƒ½å›¾è¡¨...")
            
            from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc
            
            # è®¡ç®—ROCå’ŒPRæ›²çº¿æ•°æ®
            train_fpr, train_tpr, _ = roc_curve(self.y_train, self.y_train_pred_proba)
            train_roc_auc = auc(train_fpr, train_tpr)
            
            test_fpr, test_tpr, _ = roc_curve(self.y_test, self.y_test_pred_proba)
            test_roc_auc = auc(test_fpr, test_tpr)
            
            train_precision, train_recall, _ = precision_recall_curve(self.y_train, self.y_train_pred_proba)
            train_pr_auc = auc(train_recall, train_precision)
            
            test_precision, test_recall, _ = precision_recall_curve(self.y_test, self.y_test_pred_proba)
            test_pr_auc = auc(test_recall, test_precision)
            
            train_cm = confusion_matrix(self.y_train, self.y_train_pred)
            test_cm = confusion_matrix(self.y_test, self.y_test_pred)
            
            # åˆ›å»ºè¯¦ç»†æ€§èƒ½å›¾è¡¨
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('ä¼˜åŒ–Mario MLç­–ç•¥è¯¦ç»†æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
            
            # 1. è®­ç»ƒé›†æ··æ·†çŸ©é˜µ
            sns.heatmap(train_cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=['é¢„æµ‹0', 'é¢„æµ‹1'],
                       yticklabels=['å®é™…0', 'å®é™…1'], ax=axes[0,0])
            axes[0,0].set_title('è®­ç»ƒé›†æ··æ·†çŸ©é˜µ')
            axes[0,0].set_ylabel('å®é™…æ ‡ç­¾')
            axes[0,0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
            
            # 2. æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
            sns.heatmap(test_cm, annot=True, fmt='d', cmap='Greens',
                       xticklabels=['é¢„æµ‹0', 'é¢„æµ‹1'],
                       yticklabels=['å®é™…0', 'å®é™…1'], ax=axes[0,1])
            axes[0,1].set_title('æµ‹è¯•é›†æ··æ·†çŸ©é˜µ')
            axes[0,1].set_ylabel('å®é™…æ ‡ç­¾')
            axes[0,1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
            
            # 3. ROCæ›²çº¿å¯¹æ¯”
            axes[0,2].plot(train_fpr, train_tpr, color='blue', lw=2, 
                          label=f'è®­ç»ƒé›† (AUC = {train_roc_auc:.3f})')
            axes[0,2].plot(test_fpr, test_tpr, color='red', lw=2, 
                          label=f'æµ‹è¯•é›† (AUC = {test_roc_auc:.3f})')
            axes[0,2].plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
            axes[0,2].set_xlim([0.0, 1.0])
            axes[0,2].set_ylim([0.0, 1.05])
            axes[0,2].set_xlabel('å‡æ­£ç‡ (FPR)')
            axes[0,2].set_ylabel('çœŸæ­£ç‡ (TPR)')
            axes[0,2].set_title('ROCæ›²çº¿å¯¹æ¯”')
            axes[0,2].legend(loc="lower right")
            axes[0,2].grid(True)
            
            # 4. PRæ›²çº¿å¯¹æ¯”
            axes[1,0].plot(train_recall, train_precision, color='blue', lw=2,
                          label=f'è®­ç»ƒé›† (AUC = {train_pr_auc:.3f})')
            axes[1,0].plot(test_recall, test_precision, color='red', lw=2,
                          label=f'æµ‹è¯•é›† (AUC = {test_pr_auc:.3f})')
            axes[1,0].set_xlabel('å¬å›ç‡ (Recall)')
            axes[1,0].set_ylabel('ç²¾ç¡®ç‡ (Precision)')
            axes[1,0].set_title('Precision-Recallæ›²çº¿å¯¹æ¯”')
            axes[1,0].legend(loc='upper right')
            axes[1,0].grid(True)
            
            # 5. é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”
            for label in [0, 1]:
                train_mask = self.y_train == label
                if train_mask.sum() > 0:
                    axes[1,1].hist(self.y_train_pred_proba[train_mask], bins=30, alpha=0.5, 
                                  label=f'è®­ç»ƒé›† æ ‡ç­¾={label}', density=True)
                
                test_mask = self.y_test == label
                if test_mask.sum() > 0:
                    axes[1,1].hist(self.y_test_pred_proba[test_mask], bins=30, alpha=0.5, 
                                  label=f'æµ‹è¯•é›† æ ‡ç­¾={label}', density=True)
            
            axes[1,1].axvline(0.5, color='red', linestyle='--', label='é˜ˆå€¼=0.5')
            axes[1,1].set_xlabel('é¢„æµ‹ä¸ºæ­£ç±»çš„æ¦‚ç‡')
            axes[1,1].set_ylabel('å¯†åº¦')
            axes[1,1].set_title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå¯¹æ¯”')
            axes[1,1].legend()
            axes[1,1].grid(True)
            
            # 6. æ€§èƒ½æŒ‡æ ‡æ±‡æ€»
            train_metrics = self.model_performance['train']
            test_metrics = self.model_performance['test']
            
            metrics_names = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°', 'AUC', 'PRAUC']
            train_values = [train_metrics['å‡†ç¡®ç‡'], train_metrics['ç²¾ç¡®ç‡'], train_metrics['å¬å›ç‡'], 
                          train_metrics['F1åˆ†æ•°'], train_metrics['AUC'], train_metrics['PRAUC']]
            test_values = [test_metrics['å‡†ç¡®ç‡'], test_metrics['ç²¾ç¡®ç‡'], test_metrics['å¬å›ç‡'], 
                         test_metrics['F1åˆ†æ•°'], test_metrics['AUC'], test_metrics['PRAUC']]
            
            x = np.arange(len(metrics_names))
            width = 0.35
            
            bars1 = axes[1,2].bar(x - width/2, train_values, width, label='è®­ç»ƒé›†', alpha=0.8)
            bars2 = axes[1,2].bar(x + width/2, test_values, width, label='æµ‹è¯•é›†', alpha=0.8)
            
            axes[1,2].set_xlabel('æ€§èƒ½æŒ‡æ ‡')
            axes[1,2].set_ylabel('åˆ†æ•°')
            axes[1,2].set_title('æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
            axes[1,2].set_xticks(x)
            axes[1,2].set_xticklabels(metrics_names, rotation=45)
            axes[1,2].legend()
            axes[1,2].grid(True, axis='y', alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    axes[1,2].text(bar.get_x() + bar.get_width()/2., height,
                                  f'{height:.3f}', ha='center', va='bottom', fontsize=8)
            
            plt.tight_layout()
            
            # ä¿å­˜è¯¦ç»†æ€§èƒ½å›¾è¡¨
            plot_file = Path(__file__).parent.parent / 'train_results' / 'detailed_model_performance.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… è¯¦ç»†æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_file}")
            
            # ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾è¡¨
            self._plot_feature_importance_detailed()
            
            # ç”Ÿæˆæ•°æ®è´¨é‡åˆ†æå›¾è¡¨
            self._plot_data_quality_analysis()
            
        except Exception as e:
            self.logger.error(f"âŒ ç”Ÿæˆè¯¦ç»†æ€§èƒ½å›¾è¡¨å¤±è´¥: {e}")
    
    def _plot_feature_importance_detailed(self):
        """ç»˜åˆ¶è¯¦ç»†çš„ç‰¹å¾é‡è¦æ€§å›¾è¡¨"""
        
        try:
            if not hasattr(self.model, 'feature_importance'):
                return
            
            importance = self.model.feature_importance()
            feature_names = self.selected_features
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            }).sort_values('importance', ascending=False)
            
            # ä¿å­˜ç‰¹å¾é‡è¦æ€§æ•°æ®
            importance_file = Path(__file__).parent.parent / 'train_results' / 'feature_importance.csv'
            importance_df.to_csv(importance_file, index=False, encoding='utf-8')
            
            self.logger.info("æ¨¡å‹ç‰¹å¾é‡è¦æ€§ Top 20:")
            for _, row in importance_df.head(20).iterrows():
                self.logger.info(f"  {row['feature']}: {row['importance']}")
            
            # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
            plt.figure(figsize=(12, 10))
            
            top_features = importance_df.head(25)  # æ˜¾ç¤ºæ›´å¤šç‰¹å¾
            colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))
            
            bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)
            plt.yticks(range(len(top_features)), top_features['feature'])
            plt.xlabel('ç‰¹å¾é‡è¦æ€§åˆ†æ•°')
            plt.title('LightGBMæ¨¡å‹ç‰¹å¾é‡è¦æ€§ Top 25', fontsize=14, fontweight='bold')
            plt.gca().invert_yaxis()
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                width = bar.get_width()
                plt.text(width + max(importance) * 0.01, bar.get_y() + bar.get_height()/2,
                        f'{width:.0f}', ha='left', va='center', fontsize=9)
            
            plt.tight_layout()
            
            plot_file = Path(__file__).parent.parent / 'train_results' / 'feature_importance_detailed.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… è¯¦ç»†ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {plot_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ç»˜åˆ¶è¯¦ç»†ç‰¹å¾é‡è¦æ€§å›¾å¤±è´¥: {e}")
    
    def _plot_data_quality_analysis(self):
        """ç»˜åˆ¶æ•°æ®è´¨é‡åˆ†æå›¾è¡¨"""
        
        try:
            df = self.training_data
            
            # åˆ›å»ºæ•°æ®è´¨é‡åˆ†æå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Mario MLç­–ç•¥æ•°æ®è´¨é‡åˆ†æ', fontsize=16, fontweight='bold')
            
            # 1. æ ‡ç­¾åˆ†å¸ƒ
            if 'label' in df.columns:
                label_counts = df['label'].value_counts()
                colors = ['#ff9999', '#66b3ff']
                pie = axes[0,0].pie(label_counts.values, labels=[f'è´Ÿæ ·æœ¬\n({label_counts[0]})', f'æ­£æ ·æœ¬\n({label_counts[1]})'], 
                                   colors=colors, autopct='%1.1f%%', startangle=90)
                axes[0,0].set_title('è®­ç»ƒæ ·æœ¬æ ‡ç­¾åˆ†å¸ƒ')
            
            # 2. ç¼ºå¤±å€¼åˆ†æ
            features = [col for col in df.columns if col != 'label']
            missing_counts = df[features].isnull().sum()
            missing_features = missing_counts[missing_counts > 0]
            
            if len(missing_features) > 0:
                top_missing = missing_features.nlargest(15)
                bars = axes[0,1].bar(range(len(top_missing)), top_missing.values, color='orange', alpha=0.7)
                axes[0,1].set_xticks(range(len(top_missing)))
                axes[0,1].set_xticklabels(top_missing.index, rotation=45, ha='right')
                axes[0,1].set_ylabel('ç¼ºå¤±å€¼æ•°é‡')
                axes[0,1].set_title('ç¼ºå¤±å€¼æœ€å¤šçš„ç‰¹å¾ Top 15')
                
                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for bar in bars:
                    height = bar.get_height()
                    axes[0,1].text(bar.get_x() + bar.get_width()/2., height,
                                  f'{int(height)}', ha='center', va='bottom')
            else:
                axes[0,1].text(0.5, 0.5, 'æ‰€æœ‰ç‰¹å¾å‡æ— ç¼ºå¤±å€¼', ha='center', va='center', 
                              transform=axes[0,1].transAxes, fontsize=14)
                axes[0,1].set_title('ç¼ºå¤±å€¼åˆ†æ')
            
            # 3. æ•°æ®åˆ†å¸ƒç»Ÿè®¡
            numeric_features = df.select_dtypes(include=[np.number]).columns
            if len(numeric_features) > 1:
                sample_features = numeric_features[:6]  # é€‰æ‹©å‰6ä¸ªæ•°å€¼ç‰¹å¾
                sample_data = df[sample_features]
                
                axes[1,0].boxplot([sample_data[col].dropna() for col in sample_features], 
                                 labels=sample_features)
                axes[1,0].set_title('ä¸»è¦ç‰¹å¾åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰')
                axes[1,0].tick_params(axis='x', rotation=45)
            
            # 4. ç›¸å…³æ€§åˆ†æ
            if len(features) > 0:
                corr_matrix = df[features].corr()
                # æ˜¾ç¤ºéƒ¨åˆ†ç‰¹å¾çš„ç›¸å…³æ€§
                n_features = min(15, len(corr_matrix))
                corr_subset = corr_matrix.iloc[:n_features, :n_features]
                
                im = axes[1,1].imshow(corr_subset, cmap='RdBu_r', vmin=-1, vmax=1, aspect='auto')
                axes[1,1].set_xticks(range(n_features))
                axes[1,1].set_yticks(range(n_features))
                axes[1,1].set_xticklabels(corr_subset.columns, rotation=45, ha='right')
                axes[1,1].set_yticklabels(corr_subset.index)
                axes[1,1].set_title(f'ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ (å‰{n_features}ä¸ª)')
                
                # æ·»åŠ é¢œè‰²æ¡
                plt.colorbar(im, ax=axes[1,1], shrink=0.8)
            
            plt.tight_layout()
            
            plot_file = Path(__file__).parent.parent / 'train_results' / 'data_quality_analysis.png'
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"âœ… æ•°æ®è´¨é‡åˆ†æå›¾å·²ä¿å­˜: {plot_file}")
            
            # ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š
            self._save_data_quality_report()
            
        except Exception as e:
            self.logger.error(f"âŒ ç»˜åˆ¶æ•°æ®è´¨é‡åˆ†æå›¾å¤±è´¥: {e}")
    
    def _save_data_quality_report(self):
        """ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Š"""
        
        try:
            df = self.training_data
            
            # åˆ›å»ºæ•°æ®è´¨é‡æŠ¥å‘Š
            report = {
                'basic_info': {
                    'total_samples': len(df),
                    'total_features': len(df.columns) - 1,
                    'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024
                },
                'label_distribution': df['label'].value_counts().to_dict() if 'label' in df.columns else {},
                'missing_values': df.isnull().sum().to_dict(),
                'feature_stats': {
                    'numeric_features': len(df.select_dtypes(include=[np.number]).columns),
                    'categorical_features': len(df.select_dtypes(include=['object']).columns)
                }
            }
            
            # ä¿å­˜ä¸ºJSON
            import json
            report_file = Path(__file__).parent.parent / 'train_results' / 'data_quality_report.json'
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"âœ… æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {e}")
    
    def test_stock_selection(self):
        """æµ‹è¯•é€‰è‚¡åŠŸèƒ½ - ä¿®å¤ç‰ˆæœ¬"""
        
        if self.model is None:
            self.logger.warning("âš ï¸  æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•æµ‹è¯•é€‰è‚¡")
            return False
        
        self.logger.info("ğŸ¯ æµ‹è¯•é€‰è‚¡åŠŸèƒ½...")
        
        try:
            test_date = datetime(2024, 6, 30)
            
            # è·å–æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨ï¼ˆ002å¼€å¤´çš„ä¸­å°æ¿è‚¡ç¥¨ï¼‰
            trading_date = self.strategy._get_nearest_trading_date(test_date)
            all_stock_data = self.strategy.db.find_data('stock_factor_pro', {'trade_date': trading_date})
            test_stock_list = [item['ts_code'] for item in all_stock_data 
                              if item.get('ts_code', '').startswith('002')][:50]  # å–å‰50åªåšæµ‹è¯•
            
            if not test_stock_list:
                self.logger.warning("æœªæ‰¾åˆ°æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨")
                return False
            
            # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„é¢„æµ‹é€»è¾‘
            factor_data = self.strategy.get_comprehensive_factor_data(test_stock_list, test_date)
            
            if factor_data.empty:
                self.logger.warning("è·å–å› å­æ•°æ®ä¸ºç©º")
                return False
            
            # ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾å¯¹é½
            missing_features = set(self.feature_names) - set(factor_data.columns)
            if missing_features:
                self.logger.warning(f"ç¼ºå°‘ç‰¹å¾: {missing_features}")
                for feature in missing_features:
                    factor_data[feature] = 0
            
            # é€‰æ‹©æ¨¡å‹ç‰¹å¾
            X_test = factor_data[self.feature_names]
            
            # æ•°æ®é¢„å¤„ç†
            X_test = X_test.fillna(0)
            X_test_scaled = pd.DataFrame(
                self.scaler.transform(X_test),
                index=X_test.index,
                columns=X_test.columns
            )
            
            # é¢„æµ‹
            predictions = self.model.predict(X_test_scaled)
            
            # é€‰æ‹©Topè‚¡ç¥¨
            result_df = pd.DataFrame({
                'stock_code': X_test.index,
                'prediction_score': predictions
            }).sort_values('prediction_score', ascending=False)
            
            selected_stocks = result_df.head(10)
            
            self.logger.info(f"âœ… é€‰è‚¡æµ‹è¯•æˆåŠŸ")
            self.logger.info(f"ğŸ“Š é€‰è‚¡ç»“æœ:")
            self.logger.info(f"   - å€™é€‰è‚¡ç¥¨æ•°é‡: {len(X_test)}")
            self.logger.info(f"   - é€‰ä¸­è‚¡ç¥¨æ•°é‡: {len(selected_stocks)}")
            self.logger.info(f"   - é¢„æµ‹åˆ†æ•°èŒƒå›´: {predictions.min():.4f} - {predictions.max():.4f}")
            
            if len(selected_stocks) > 0:
                self.logger.info(f"   - å‰5åªè‚¡ç¥¨:")
                for i, (_, row) in enumerate(selected_stocks.head(5).iterrows()):
                    self.logger.info(f"     {row['stock_code']}: {row['prediction_score']:.4f}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ é€‰è‚¡æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def generate_report(self):
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""
        
        self.logger.info("ğŸ“ ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š...")
        
        report = f"""
# ä¼˜åŒ–Mario MLç­–ç•¥è®­ç»ƒæŠ¥å‘Š

## ğŸ¯ è®­ç»ƒæ¦‚è¦
- ç­–ç•¥åç§°: {self.strategy.config.strategy_name}
- è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ•°æ®æœŸé—´: 2020-01-01 è‡³ 2024-06-30

## ğŸ“Š æ•°æ®ç»Ÿè®¡
- è®­ç»ƒæ ·æœ¬æ•°: {len(self.training_data):,}
- åŸå§‹ç‰¹å¾æ•°: {len(self.training_data.columns)-1}
- é€‰æ‹©ç‰¹å¾æ•°: {len(self.selected_features) if self.selected_features else 0}
- æ­£æ ·æœ¬æ¯”ä¾‹: {self.training_data['label'].mean():.3f}

## ğŸ¯ æ¨¡å‹æ€§èƒ½
"""
        
        if self.model_performance:
            # ä¿®å¤ï¼šå¤„ç†ä¸åŒæ ¼å¼çš„æ€§èƒ½æ•°æ®
            if isinstance(self.model_performance, dict) and 'accuracy' in self.model_performance:
                # æ–°æ ¼å¼ï¼šç›´æ¥åŒ…å«æŒ‡æ ‡
                report += "\n### æ¨¡å‹æ€§èƒ½\n"
                for metric, value in self.model_performance.items():
                    if isinstance(value, (int, float)):
                        report += f"- {metric}: {value:.4f}\n"
            else:
                # åŸæ ¼å¼ï¼šåˆ†train/test
                for dataset, metrics in self.model_performance.items():
                    report += f"\n### {dataset}æ€§èƒ½\n"
                    for metric, value in metrics.items():
                        if isinstance(value, (int, float)):
                            report += f"- {metric}: {value:.4f}\n"
        
        report += f"""
## ğŸ”§ æ¨¡å‹é…ç½®
- ç®—æ³•: LightGBM
- å­¦ä¹ ç‡: {self.strategy.config.params['model_params']['learning_rate']}
- è¿­ä»£æ¬¡æ•°: {self.strategy.config.params['model_params']['num_boost_round']}
- ç›¸å…³æ€§é˜ˆå€¼: {self.strategy.config.params['correlation_threshold']}

## ğŸ“ˆ å› å­æ˜ å°„ç»Ÿè®¡
- ç›´æ¥å¯ç”¨å› å­: {len(self.strategy.config.direct_factors)} ä¸ª
- è®¡ç®—è·å¾—å› å­: {len(self.strategy.config.calculated_factors)} ä¸ª
- è´¢åŠ¡è¡¨è¡¥å……å› å­: {len(self.strategy.config.financial_factors)} ä¸ª
- **æ€»è®¡è¦†ç›–ç‡**: {len(self.strategy.config.all_available_factors)/82*100:.1f}%

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(__file__).parent.parent / 'train_results' / 'optimized_mario_ml_training_report.md'
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        self.logger.info(f"âœ… è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        
        return True
    
    def run_complete_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        
        self.logger.info("ğŸš€ å¼€å§‹å®Œæ•´çš„ä¼˜åŒ–Mario MLç­–ç•¥è®­ç»ƒæµç¨‹...")
        
        steps = [
            ("åˆå§‹åŒ–ç­–ç•¥", self.initialize_strategy),
            ("å‡†å¤‡è®­ç»ƒæ•°æ®", self.prepare_training_data),
            ("æ•°æ®è´¨é‡åˆ†æ", self.analyze_data_quality),
            ("ç‰¹å¾é€‰æ‹©åˆ†æ", self.feature_selection_analysis),
            ("è®­ç»ƒæ’åºæ¨¡å‹", self.train_ranking_model),  # ä½¿ç”¨æ’åºå­¦ä¹ æ¨¡å‹
            ("æµ‹è¯•é€‰è‚¡åŠŸèƒ½", self.test_stock_selection),
            ("ç”Ÿæˆå¯è§†åŒ–", self.visualize_results),
            ("ç”ŸæˆæŠ¥å‘Š", self.generate_report)
        ]
        
        success_steps = 0
        for step_name, step_func in steps:
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤: {step_name}")
            self.logger.info(f"{'='*60}")
            
            try:
                if step_func():
                    success_steps += 1
                    self.logger.info(f"âœ… {step_name} å®Œæˆ")
                else:
                    self.logger.error(f"âŒ {step_name} å¤±è´¥")
                    break
            except Exception as e:
                self.logger.error(f"âŒ {step_name} æ‰§è¡Œå¼‚å¸¸: {e}")
                break
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ‰ è®­ç»ƒæµç¨‹å®Œæˆ")
        self.logger.info(f"âœ… æˆåŠŸæ­¥éª¤: {success_steps}/{len(steps)}")
        self.logger.info(f"{'='*60}")
        
        if success_steps == len(steps):
            self.logger.info("ğŸ‰ æ‰€æœ‰æ­¥éª¤æˆåŠŸå®Œæˆï¼æ¨¡å‹å·²å‡†å¤‡å°±ç»ªã€‚")
            return True
        else:
            self.logger.warning(f"âš ï¸  éƒ¨åˆ†æ­¥éª¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚")
            return False


if __name__ == "__main__":
    # è¿è¡Œå®Œæ•´è®­ç»ƒ
    trainer = OptimizedMarioMLTrainer()
    success = trainer.run_complete_training()
    
    if success:
        print("\nğŸ‰ ä¼˜åŒ–Mario MLç­–ç•¥è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“ ç›¸å…³æ–‡ä»¶å·²ä¿å­˜è‡³:")
        print("   - æ¨¡å‹æ–‡ä»¶: quantitative/models/")
        print("   - è®­ç»ƒæŠ¥å‘Š: quantitative/reports/")
        print("   - æ—¥å¿—æ–‡ä»¶: optimized_mario_ml_training.log")
    else:
        print("\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶ã€‚")
    def _experimental_ensemble(self, X_train, y_train, X_test, y_test, lgb_proba):
        """é«˜ç²¾ç¡®ç‡å¯¼å‘çš„å¤šæ¨¡å‹é›†æˆ"""
        try:
            from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
            from sklearn.linear_model import LogisticRegression
            from sklearn.svm import SVC
            from sklearn.metrics import roc_auc_score, precision_score, recall_score
            import xgboost as xgb
            
            self.logger.info("ğŸ”¬ å¼€å§‹é«˜ç²¾ç¡®ç‡å¤šæ¨¡å‹é›†æˆ...")
            
            models = {}
            probas = {}
            
            # 1. é«˜ç²¾ç¡®ç‡RandomForest - ä¿å®ˆé…ç½®
            rf_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=6,  # é™åˆ¶æ·±åº¦æé«˜ç²¾ç¡®ç‡
                min_samples_split=20,  # å¢åŠ åˆ†å‰²æ ·æœ¬æ•°
                min_samples_leaf=10,   # å¢åŠ å¶å­èŠ‚ç‚¹æ ·æœ¬æ•°
                max_features=0.7,      # å‡å°‘ç‰¹å¾é‡‡æ ·
                random_state=42,
                n_jobs=2,
                class_weight='balanced'
            )
            rf_model.fit(X_train, y_train)
            models['RandomForest'] = rf_model
            probas['RandomForest'] = rf_model.predict_proba(X_test)[:, 1]
            
            # 2. ExtraTrees - æ›´å¼ºçš„æ­£åˆ™åŒ–
            et_model = ExtraTreesClassifier(
                n_estimators=100,
                max_depth=5,
                min_samples_split=25,
                min_samples_leaf=15,
                max_features=0.6,
                random_state=42,
                n_jobs=2,
                class_weight='balanced'
            )
            et_model.fit(X_train, y_train)
            models['ExtraTrees'] = et_model
            probas['ExtraTrees'] = et_model.predict_proba(X_test)[:, 1]
            
            # 3. XGBoost - é«˜ç²¾ç¡®ç‡é…ç½®
            try:
                xgb_model = xgb.XGBClassifier(
                    n_estimators=100,
                    max_depth=4,
                    learning_rate=0.05,
                    subsample=0.8,
                    colsample_bytree=0.7,
                    reg_alpha=2.0,
                    reg_lambda=2.0,
                    scale_pos_weight=1.5,
                    random_state=42,
                    n_jobs=2
                )
                xgb_model.fit(X_train, y_train)
                models['XGBoost'] = xgb_model
                probas['XGBoost'] = xgb_model.predict_proba(X_test)[:, 1]
            except Exception as e:
                self.logger.warning(f"XGBoostè®­ç»ƒå¤±è´¥: {e}")
            
            # 4. ä¿å®ˆçš„LogisticRegression
            lr_model = LogisticRegression(
                C=0.1,  # å¼ºæ­£åˆ™åŒ–
                random_state=42,
                max_iter=1000,
                solver='liblinear',
                class_weight='balanced'
            )
            lr_model.fit(X_train, y_train)
            models['LogisticRegression'] = lr_model
            probas['LogisticRegression'] = lr_model.predict_proba(X_test)[:, 1]
            
            # 5. GradientBoosting - ä¿å®ˆé…ç½®
            gb_model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=4,
                learning_rate=0.05,
                subsample=0.8,
                max_features=0.7,
                random_state=42
            )
            gb_model.fit(X_train, y_train)
            models['GradientBoosting'] = gb_model
            probas['GradientBoosting'] = gb_model.predict_proba(X_test)[:, 1]
            
            # è®¡ç®—å„æ¨¡å‹çš„ç²¾ç¡®ç‡å’ŒAUC
            lgb_auc = roc_auc_score(y_test, lgb_proba)
            lgb_precision = precision_score(y_test, (lgb_proba >= 0.5).astype(int), zero_division=0)
            
            self.logger.info(f"ğŸ¯ å„æ¨¡å‹æ€§èƒ½å¯¹æ¯”:")
            self.logger.info(f"   LightGBM: AUC={lgb_auc:.4f}, ç²¾ç¡®ç‡={lgb_precision:.4f}")
            
            model_scores = {}
            for name, proba in probas.items():
                auc = roc_auc_score(y_test, proba)
                precision = precision_score(y_test, (proba >= 0.5).astype(int), zero_division=0)
                recall = recall_score(y_test, (proba >= 0.5).astype(int), zero_division=0)
                
                # ç»¼åˆè¯„åˆ†ï¼šé‡è§†ç²¾ç¡®ç‡
                score = 0.5 * precision + 0.3 * auc + 0.2 * recall
                model_scores[name] = score
                
                self.logger.info(f"   {name}: AUC={auc:.4f}, ç²¾ç¡®ç‡={precision:.4f}, å¬å›ç‡={recall:.4f}, ç»¼åˆåˆ†={score:.4f}")
            
            # æ™ºèƒ½åŠ æƒï¼šæ ¹æ®ç²¾ç¡®ç‡è¡¨ç°åˆ†é…æƒé‡
            total_models = len(probas) + 1  # +1 for LightGBM
            lgb_weight = 0.4  # LightGBMåŸºç¡€æƒé‡
            
            # ä¸ºé«˜ç²¾ç¡®ç‡æ¨¡å‹åˆ†é…æ›´é«˜æƒé‡
            weights = {}
            remaining_weight = 1.0 - lgb_weight
            
            # æ ¹æ®ç²¾ç¡®ç‡æ’åºåˆ†é…æƒé‡
            sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)
            weight_sum = 0
            for i, (name, score) in enumerate(sorted_models):
                # æŒ‡æ•°é€’å‡æƒé‡åˆ†é…
                weight = remaining_weight * (0.5 ** i)
                weights[name] = weight
                weight_sum += weight
            
            # å½’ä¸€åŒ–æƒé‡
            if weight_sum > 0:
                for name in weights:
                    weights[name] = weights[name] / weight_sum * remaining_weight
            
            # é›†æˆé¢„æµ‹
            ensemble_proba = lgb_weight * lgb_proba
            for name, proba in probas.items():
                ensemble_proba += weights.get(name, 0) * proba
            
            # è¯„ä¼°é›†æˆæ•ˆæœ
            ensemble_auc = roc_auc_score(y_test, ensemble_proba)
            ensemble_precision = precision_score(y_test, (ensemble_proba >= 0.5).astype(int), zero_division=0)
            ensemble_recall = recall_score(y_test, (ensemble_proba >= 0.5).astype(int), zero_division=0)
            
            self.logger.info(f"ğŸ† é›†æˆæ¨¡å‹ç»“æœ:")
            self.logger.info(f"   é›†æˆAUC: {ensemble_auc:.4f}")
            self.logger.info(f"   é›†æˆç²¾ç¡®ç‡: {ensemble_precision:.4f}")
            self.logger.info(f"   é›†æˆå¬å›ç‡: {ensemble_recall:.4f}")
            self.logger.info(f"   æƒé‡åˆ†é…: LGB={lgb_weight:.3f}, " + ", ".join([f"{k}={v:.3f}" for k, v in weights.items()]))
            
            # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨é›†æˆæ¨¡å‹
            if ensemble_precision > lgb_precision or (ensemble_auc > lgb_auc and ensemble_precision >= lgb_precision * 0.95):
                improvement = (ensemble_precision - lgb_precision) / max(lgb_precision, 0.01) * 100
                self.logger.info(f"âœ… é›†æˆæ¨¡å‹ç²¾ç¡®ç‡æå‡: +{improvement:.2f}%")
                return ensemble_proba
            else:
                self.logger.info("âš ï¸ é›†æˆæ¨¡å‹æœªæ˜¾è‘—æå‡ç²¾ç¡®ç‡ï¼Œä¿æŒåŸæ¨¡å‹")
                return lgb_proba
                
        except Exception as e:
            self.logger.error(f"âŒ æ¨¡å‹é›†æˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return lgb_proba


    def _analyze_feature_importance(self, model, feature_names):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        try:
            # è·å–ç‰¹å¾é‡è¦æ€§
            importance = model.feature_importance(importance_type="gain")
            
            # åˆ›å»ºç‰¹å¾é‡è¦æ€§DataFrame
            feature_importance_df = pd.DataFrame({
                "feature": feature_names,
                "importance": importance
            }).sort_values("importance", ascending=False)
            
            # è®°å½•å‰10ä¸ªæœ€é‡è¦ç‰¹å¾
            self.logger.info("ğŸ” å‰10ä¸ªæœ€é‡è¦ç‰¹å¾:")
            for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):
                self.logger.info(f"   {i:2d}. {row['feature']}: {row['importance']:.0f}")
            
            # è®¡ç®—ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
            total_importance = importance.sum()
            top5_importance = feature_importance_df.head(5)["importance"].sum()
            top10_importance = feature_importance_df.head(10)["importance"].sum()
            
            self.logger.info(f"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§åˆ†å¸ƒ:")
            self.logger.info(f"   å‰5ä¸ªç‰¹å¾è´¡çŒ®: {top5_importance/total_importance*100:.1f}%")
            self.logger.info(f"   å‰10ä¸ªç‰¹å¾è´¡çŒ®: {top10_importance/total_importance*100:.1f}%")
            
            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            self.feature_importance = feature_importance_df
            
        except Exception as e:
            self.logger.error(f"âŒ ç‰¹å¾é‡è¦æ€§åˆ†æå¤±è´¥: {e}")

